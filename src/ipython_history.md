 1/1:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
 1/2:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
 3/1:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
 7/1:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
13/1:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
13/2:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
15/1:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
15/2:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
17/1:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
21/1:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
21/2:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
21/3:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
21/4:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='b', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
25/1:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='b', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
26/1:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='b', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
28/1: print('Hello World')
28/2:
i = 1
while i <= 10:
    print(i)
    i = i + 1
28/3:
i = 1
while i <= 10:
    print(i)
    i = i + 1
28/4: $\sum_{i=0}^{i=n} 5^i = \frac{(5^{n+1} -1)}{4}$
28/5:
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='b', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
30/1: 2 * 4
30/2: 2 ** 4
30/3: 7 % 2
30/4: 5 % 2
30/5:
t2 = list(t)
t2[0] = 'NEW'
print(t2)
t = t2
t[0]
30/6:
t2 = list(t)
t2[0] = 'NEW'
t2
t = t2
t[0]
30/7:
t2 = list(t)
t2[0] = 'NEW'
t2[0]
t = t2
t[0]
30/8:
t2 = list(t)
t2[0] = 'NEW'
print(t2[0])
t = t2
t[0]
30/9:
t2 = list(t)
t2[0] = 'NEW'
for i in range(0,3):
    print(t2[i])
t = t2
t[0]
30/10:
t2 = list(t)
t2[0] = 'NEW'
for i in range(0,3):
    print(i)
t = t2
t[0]
30/11:
t2 = list(t)
t2[0] = 'NEW'
for i in range(0,3):
    print(i)
t = t2
30/12:
t2 = list(t)
t2[0] = 'NEW'
for i in range(0,3):
    print(i)
30/13:
t2 = list(t)
t2[0] = 'NEW'
t = t2
t[0]
30/14: {1,2,3}
30/15:
t2 = list(t)
t2[0] = 'NEW'
t = t2
t[0]
30/16: t = (1,2,3)
30/17: t[0]
30/18:
t2 = list(t)
t2[0] = 'NEW'
t = t2
t[0]
30/19:
t2 = list(t)
t2[0] = 'NEW'
print(1)
t = t2
t[0]
30/20:
t2 = list(t)
t2[0] = 'NEW'
print(t2)
t = t2
t[0]
30/21:
t2 = list(t)
t2[0] = 'NEW'
t = t2
print(t)
t[0]
30/22:
t2 = list(t)
t2[0] = 'NEW'
t = t2
t[0]
31/1: np.array(minha_matriz)
31/2:
minha_matriz = [[1,2,3],
                [4,5,6],
                [7,8,9]]
minha_matriz
31/3: np.array(minha_matriz)
31/4: import numpy as np
31/5: np.array(minha_matriz)
31/6:
arr = np.arange(25)
ranarr = np.random.randint(0,50,10)
31/7:
arr = np.arange(25)
arr
ranarr = np.random.randint(0,50,10)
31/8:
arr = np.arange(25)
arr
ranarr = np.random.randint(0,50,10)
31/9:
arr = np.arange(25)
print(arr)
ranarr = np.random.randint(0,50,10)
31/10:
arr = np.arange(25)
ranarr = np.random.randint(0,50,10)
31/11: arr
31/12:
arr.reshape(5,5)
arr
m = np.random.randint(1,100,(4,3,5))
m
31/13:
arr.reshape(5,5)
print(arr)
m = np.random.randint(1,100,(4,3,5))
m
31/14:
arr.reshape(5,5)
print(arr)
m = np.random.randint(1,100,(4,3,5))
m
31/15:
arr.reshape(5,5)
m = np.random.randint(1,100,(4,3,5))
m
31/16:
arr.reshape(5,5)
m = np.random.randint(1,100,(4,3,5))
31/17:
arr = np.arange(25)
ranarr = np.random.randint(0,50,10)
31/18: arr
31/19: ranarr
31/20:
arr.reshape(5,5)
m = np.random.randint(1,100,(4,3,5))
31/21: m.reshape(3,4)
31/22:
arr.reshape(5,5)
m = np.random.randint(1,100,(4,3,5))
m
31/23: m.reshape(3,4)
31/24: m.reshape(3,4)
31/25:
arr.reshape(5,5)
m = np.random.randint(1,100,(4,3,5))
m
32/1:
import numpy as np
arr = np.arange(0,10)
32/2: arr + arr
32/3: arr * arr
32/4: arr - arr
32/5:
# Aviso na divisão por zero, mas não um erro!
# Apenas substituído por nan
arr/arr
32/6:
# Aviso na divisão por zero, mas não um erro!
# Apenas substituído por nan
arr/arr
32/7:
# Também aviso, mas não um erro, apenas infinito
1/arr
32/8: arr**3
34/1:
def f(x):
    return x**3

# Making two lists for x and y
n=5            #No. of points
dx=1.0/(n-1)   #x spacing in [0,1]
xlist = [i*dx for i in range(n)]
ylist = [f(x) for x in xlist]
print ylist
34/2:
def f(x):
    return x**3

# Making two lists for x and y
n=5            #No. of points
dx=1.0/(n-1)   #x spacing in [0,1]
xlist = [i*dx for i in range(n)]
ylist = [f(x) for x in xlist]
print xlist
34/3:
def f(x):
    return x**3

# Making two lists for x and y
n=5            #No. of points
dx=1.0/(n-1)   #x spacing in [0,1]
xlist = [i*dx for i in range(n)]
ylist = [f(x) for x in xlist]
print(xlist)
34/4:
def f(x):
    return x**3

# Making two lists for x and y
n=5            #No. of points
dx=1.0/(n-1)   #x spacing in [0,1]
xlist = [i*dx for i in range(n)]
ylist = [f(x) for x in xlist]
print(xlist)
print(ylist)
34/5:
# Transform the lists in numerical arrays
import numpy as np
x = np.array(xlist)
y = np.array(ylist)
np.set_printoptions(formatter={'float': '{: 0.6f}'.format})
print x
print y
34/6:
# Transform the lists in numerical arrays
import numpy as np
x = np.array(xlist)
y = np.array(ylist)
np.set_printoptions(formatter={'float': '{: 0.6f}'.format})
print(x)
print(y)
34/7:
y = np.sin(x)
print(y)
34/8:
from numpy import sin, exp, linspace

def f(x):
    return x**3 + sin(x)*exp(-3*x)
  
x = 1.2   # float object
y = f(x)  # y is float
print(y)

x = linspace(0, 3, 10001)    # 10000 intervals in [0,3]
y = f(x)   # y is array
print(y)
34/9:
import numpy as np
import matplotlib.pyplot as plt

def f1(t):
    return t**2*np.exp(-t**2)

def f2(t):
    return t**2*f1(t)

t = np.linspace(0, 3, 51)
y1 = f1(t)
y2 = f2(t)

plt.plot(t,y1,"-b", label="t**2*exp(-t**2)", marker="o")
plt.plot(t,y2,"-r", label="t**4*exp(-t**2)", marker="s")
plt.xlabel('t')
plt.ylabel('y')
plt.legend(loc="upper right")
plt.xlim(0,3)
plt.ylim(0,0.6)
plt.savefig("tmp.pdf")
plt.show()
34/10:
import numpy as np
import matplotlib.pyplot as plt
import time, glob, os

plt.ion()

# Clean up old frames
for name in glob.glob('tmp_*.pdf'):
    os.remove(name)

def f(x, m, s):
    return (1.0/(np.sqrt(2*np.pi)*s))*np.exp(-0.5*((x-m)/s)**2)

m = 0
s_max = 2
s_min = 0.2
x = np.linspace(m -3*s_max, m + 3*s_max, 1000)
print(x)
s_values = np.linspace(s_max, s_min, 30)
# f is max for x=m; smaller s gives larger max value
max_f = f(m, m, s_min)

# Make a first plot (here empty)
plt.ion()
y = f(x, m, s_max)
lines = plt.plot(x, y)
plt.axis([x[0], x[-1], -0.1, max_f])
plt.xlabel('x')
plt.ylabel('f')
plt.legend(['s=%4.2f' % s_max])

# Make hardcopies of all frames
counter = 0
for s in s_values:
    y = f(x, m, s)
    lines[0].set_ydata(y)
    plt.legend(['s=%4.2f' % s])
    plt.draw()
    plt.savefig('tmp_%04d.png' % counter)
    counter += 1
34/11:
import numpy as np
import matplotlib.pyplot as plt
import time, glob, os

plt.ion()

# Clean up old frames
for name in glob.glob('tmp_*.pdf'):
    os.remove(name)

def f(x, m, s):
    return (1.0/(np.sqrt(2*np.pi)*s))*np.exp(-0.5*((x-m)/s)**2)

m = 0
s_max = 2
s_min = 0.2
x = np.linspace(m -3*s_max, m + 3*s_max, 1000)
s_values = np.linspace(s_max, s_min, 30)
print(s)
# f is max for x=m; smaller s gives larger max value
max_f = f(m, m, s_min)

# Make a first plot (here empty)
plt.ion()
y = f(x, m, s_max)
lines = plt.plot(x, y)
plt.axis([x[0], x[-1], -0.1, max_f])
plt.xlabel('x')
plt.ylabel('f')
plt.legend(['s=%4.2f' % s_max])

# Make hardcopies of all frames
counter = 0
for s in s_values:
    y = f(x, m, s)
    lines[0].set_ydata(y)
    plt.legend(['s=%4.2f' % s])
    plt.draw()
    plt.savefig('tmp_%04d.png' % counter)
    counter += 1
34/12:
import numpy as np
import matplotlib.pyplot as plt
import time, glob, os

plt.ion()

# Clean up old frames
for name in glob.glob('tmp_*.pdf'):
    os.remove(name)

def f(x, m, s):
    return (1.0/(np.sqrt(2*np.pi)*s))*np.exp(-0.5*((x-m)/s)**2)

m = 0
s_max = 2
s_min = 0.2
x = np.linspace(m -3*s_max, m + 3*s_max, 1000)
s_values = np.linspace(s_max, s_min, 30)
print(s_values)
# f is max for x=m; smaller s gives larger max value
max_f = f(m, m, s_min)

# Make a first plot (here empty)
plt.ion()
y = f(x, m, s_max)
lines = plt.plot(x, y)
plt.axis([x[0], x[-1], -0.1, max_f])
plt.xlabel('x')
plt.ylabel('f')
plt.legend(['s=%4.2f' % s_max])

# Make hardcopies of all frames
counter = 0
for s in s_values:
    y = f(x, m, s)
    lines[0].set_ydata(y)
    plt.legend(['s=%4.2f' % s])
    plt.draw()
    plt.savefig('tmp_%04d.png' % counter)
    counter += 1
34/13:
import numpy as np
import matplotlib.pyplot as plt
34/14:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')


def argand(complex_number):
    '''
    This function takes a complex number.
    '''
    y = complex_number
    x1,y1 = [0,real(y)], [0, imag(y)]
    x2,y2 = [real(y), real(y)], [0, imag(y)]


    plt.plot(x1,y1, 'r') # Draw the hypotenuse
    plt.plot(x2,y2, 'r') # Draw the projection on real-axis

    plt.plot(real(y), imag(y), 'bo')

[argand(r*exp(1j*theta)) for theta in linspace(0,2*pi,100)]
plt.show()
34/15:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')


def argand(complex_number):
    '''
    This function takes a complex number.
    '''
    y = complex_number
    x1,y1 = [0,real(y)], [0, imag(y)]
    x2,y2 = [real(y), real(y)], [0, imag(y)]


    #plt.plot(x1,y1, 'r') # Draw the hypotenuse
    plt.plot(x2,y2, 'r') # Draw the projection on real-axis

    plt.plot(real(y), imag(y), 'bo')

[argand(r*exp(1j*theta)) for theta in linspace(0,2*pi,100)]
plt.show()
34/16:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')


def argand(complex_number):
    '''
    This function takes a complex number.
    '''
    y = complex_number
    x1,y1 = [0,real(y)], [0, imag(y)]
    x2,y2 = [real(y), real(y)], [0, imag(y)]


    #plt.plot(x1,y1, 'r') # Draw the hypotenuse
    #plt.plot(x2,y2, 'r') # Draw the projection on real-axis

    plt.plot(real(y), imag(y), 'bo')

[argand(r*exp(1j*theta)) for theta in linspace(0,2*pi,100)]
plt.show()
34/17:
import matplotlib.pyplot as plt
from numpy import * as np


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')


def argand(complex_number):
    
    y = complex_number

    plt.plot(real(y), imag(y), 'bo')

[argand(r*exp(1j*theta)) for theta in linspace(0,2*pi,100)]
plt.show()
34/18:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')


def argand(complex_number):
    
    y = complex_number

    plt.plot(real(y), imag(y), 'bo')

[argand(r*exp(1j*theta)) for theta in linspace(0,2*pi,100)]
plt.show()
34/19:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = np.array(Y)
print(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')


def argand(complex_number):
    
    y = complex_number

    plt.plot(real(y), imag(y), 'bo')

[argand(r*exp(1j*theta)) for theta in linspace(0,2*pi,100)]
plt.show()
34/20:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')


def argand(complex_number):
    
    y = complex_number

    plt.plot(real(y), imag(y), 'bo')

#[argand(r*exp(1j*theta)) for theta in linspace(0,2*pi,100)]
plt.show()
34/21:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')


def argand(complex_number):
    
    y = complex_number

    plt.plot(real(y), imag(y), 'bo')

[argand(r*exp(1j*theta)) for theta in linspace(0,2*pi,100)]
plt.show()
34/22:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')


def argand(complex_number):
    
    y = complex_number

    plt.plot(real(y), imag(y), 'bo')

[argand(r*exp(1j*theta)) for theta in linspace(0,2*pi,200)]
plt.show()
34/23:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
Y = [r*exp(1j*theta) for theta in linspace(0,2*pi, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/24:
import matplotlib.pyplot as plt
from numpy import *


'''
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
This draws the axis for argand diagram
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
'''
r = 1
w = 4
Y = [r*exp(1j*theta*4) for theta in linspace(0,2*pi, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/25:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp(1j*theta*4) for theta in linspace(0,pi/2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/26:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp(1j*theta*w) for theta in linspace(0,pi/2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/27:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp(1j*theta) for theta in linspace(0,pi/2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/28:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp(1j*theta*w) for theta in linspace(0,pi/2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/29:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp(1j*theta*w) for theta in linspace(0,pi/4, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/30:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp(1j*theta*w) for theta in linspace(0,pi/2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/31:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp((1j*t*w)) for t in linspace(0,pi/2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/32:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp((1j*t*w)) for t in linspace(0,pi/2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/33:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp((1j*t*w)) for t in linspace(0,pi/8, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/34:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
Y = [r*exp((1j*t*w)) for t in linspace(0,pi/2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/35:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/36:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/37:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi*2, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/38:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi*5, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/39:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/8, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/40:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/32, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/41:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/16, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/42:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/8, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/43:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/8, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r').margins(2,2)
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/44:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/8, 200)]
Y = np.array(Y)
plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/45:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/8, 200)]
Y = np.array(Y)

ax2 = plt.subplot(221)
ax2.margins(2, 2)           # Values >0.0 zoom out
ax2.plot(t1, f(t1))

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
a.ylabel('Imaginary')
a.xlabel('Real')
a.axhline(y=0,color='black')
a.axvline(x=0, color='black')
plt.show()
34/46:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/8, 200)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
a.ylabel('Imaginary')
a.xlabel('Real')
a.axhline(y=0,color='black')
a.axvline(x=0, color='black')
plt.show()
34/47:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/8, 200)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/48:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/4, 200)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/49:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 200)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/50:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 100)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/51:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/52:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi*6, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/53:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 6
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/2, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/54:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 20
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/2, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/55:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/2, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/56:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/57:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,10000, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/58:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/59:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi**pi, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/60:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi**pi**pi, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/61:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi**pi*pi, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/62:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 4
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/63:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 40
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

a = plt.subplot(221)
a.margins(2,2)
a.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/64:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 40
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/65:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 400
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/66:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 60
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/67:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 10
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/68:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 30
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/69:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 40
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/70:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 40
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/2, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/71:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 40
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/8, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/72:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 40
s = 2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/2, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
34/73:
import matplotlib.pyplot as plt
from numpy import *

r = 1
w = 40
s = -2
Y = [r*exp(((1j*w) + s)*t) for t in linspace(0,pi/2, 1000)]
Y = np.array(Y)

plt.plot(real(Y), imag(Y), 'r')
plt.ylabel('Imaginary')
plt.xlabel('Real')
plt.axhline(y=0,color='black')
plt.axvline(x=0, color='black')
plt.show()
35/1:
# Initial conditions vector
y0 = S0, I0, R0
# Integrate the SIR equations over the time grid, t.
ret = odeint(deriv, y0, t, args=(N, beta, gamma))
S, I, R = ret.T
35/2:
# The SIR model differential equations.
def deriv(y, t, N, beta, gamma):
    S, I, R = y
    dSdt = -beta * S * I 
    dIdt = beta * S * I - gamma * I
    dRdt = gamma * I
    return dSdt, dIdt, dRdt
35/3:
# The SIR model differential equations.
def deriv(y, t, N, beta, gamma):
    S, I, R = y
    dSdt = -beta * S * I 
    dIdt = beta * S * I - gamma * I
    dRdt = gamma * I
    return dSdt, dIdt, dRdt
35/4:
# Initial conditions vector
y0 = S0, I0, R0
# Integrate the SIR equations over the time grid, t.
ret = odeint(deriv, y0, t, args=(N, beta, gamma))
S, I, R = ret.T
35/5:
# Total population, N.
N = 1000
# Initial number of infected and recovered individuals, I0 and R0.
I0, R0 = 1, 0
# Everyone else, S0, is susceptible to infection initially.
S0 = N - I0 - R0
# Contact rate, beta (in 1/days/#Individuals), and mean recovery rate, gamma, (in 1/days).
beta, gamma = 0.2/N, 1./10 
# A grid of time points (in days)
t = np.linspace(0, 160, 160)
35/6:
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
35/7:
# Total population, N.
N = 1000
# Initial number of infected and recovered individuals, I0 and R0.
I0, R0 = 1, 0
# Everyone else, S0, is susceptible to infection initially.
S0 = N - I0 - R0
# Contact rate, beta (in 1/days/#Individuals), and mean recovery rate, gamma, (in 1/days).
beta, gamma = 0.2/N, 1./10 
# A grid of time points (in days)
t = np.linspace(0, 160, 160)
35/8:
# The SIR model differential equations.
def deriv(y, t, N, beta, gamma):
    S, I, R = y
    dSdt = -beta * S * I 
    dIdt = beta * S * I - gamma * I
    dRdt = gamma * I
    return dSdt, dIdt, dRdt
35/9:
# Initial conditions vector
y0 = S0, I0, R0
# Integrate the SIR equations over the time grid, t.
ret = odeint(deriv, y0, t, args=(N, beta, gamma))
S, I, R = ret.T
36/1: Questão 1
36/2:
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
36/3:
# Definindo a grade de pontos no tempo (em segundos)
t = np.linspace(0, 160, 160)
36/4:
def deriv(y, t):
    dydt = y
    return dydt
36/5:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
ret = odeint(deriv, y0, t)
36/6:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.plot(t, Y, 'r', alpha=0.5, lw=2, label='y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/7:
# Definindo a equação diferencial dada.
def deriv(y, t):
    Y = y
    dYdt = Y
    return dYdt
36/8:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
ret = odeint(deriv, y0, t)
36/9:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.plot(t, Y, 'r', alpha=0.5, lw=2, label='y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/10:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.plot(t, Y, 'r', alpha=0.5, lw=2, label='y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/11:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
ret = odeint(deriv, y0, t)
Y = ret.T
36/12:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.plot(t, Y, 'r', alpha=0.5, lw=2, label='y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/13:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.plot(t, Y/100, 'r', alpha=0.5, lw=2, label='y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/14:
# Definindo a equação diferencial dada.
def deriv(y, t):
    Y = y
    dYdt = Y
    return dYdt
36/15:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
ret = odeint(deriv, y0, t), args=(100))
Y = ret.T
36/16:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
ret = odeint(deriv, y0, t, args=(100))
Y = ret.T
36/17:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/18:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/19:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.plot(t, Y/100, 'r', alpha=0.5, lw=2, label='y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/20:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2, label='y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/21:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2, label='y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/22:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.set_xlabel('time (s)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/23:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('time (s)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/24:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('time (s)', fontsize=12)
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/25:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('time (s)', fontsize=14)
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/26:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('time (s)', fontsize=14)
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/27:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t em segundos', fontsize=14)
plt.ylabel('y()')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/28:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/29:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/30:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/31:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t em segundos', fontsize=14)
plt.ylabel('y()')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/32:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('t(s)', fontsize=14)
plt.ylabel('y()')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/33:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('t(s)', fontsize=14)
plt.ylabel('y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/34:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)')
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/35:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/36:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)
ax.set_xlabel('time (s)')
ax.set_ylabel('y')
ax.set_ylim(0,2.0)
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/37:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)

legend = ax.legend()
legend.get_frame().set_alpha(0.5)
for spine in ('top', 'right', 'bottom', 'left'):
    ax.spines[spine].set_visible(False)
plt.savefig('sir.pdf')
plt.show()
36/38:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)
plt.show()
36/39:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)
plt.legend('a')
plt.show()
36/40:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)
plt.legend('solução')
plt.show()
36/41:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)
plt.legend('solucao')
plt.show()
36/42:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)

fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/43:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)

fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#E6E6E6', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/44:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)

fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/45:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)

fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
ax.title('aaa')
plt.show()
36/46:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)

plt.title('aaa')
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
ax.title('aaa')
plt.show()
36/47:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)

plt.title('aaa')
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/48:
# Plotar os dados da curva y(t)
plt.plot(t, y, 'r')
plt.xlabel('tempo t(s)', fontsize=14)
plt.ylabel('curva y(t)', fontsize=14)

plt.title('aaa')
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/49:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/50:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
#ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/51:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'b', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/52:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/53:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='b')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/54:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2, label='solution')
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/55:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/56:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
#ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/57:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/58:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='b', lw=2, ls='-')
plt.show()
36/59:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='black', lw=2, ls='-')
plt.show()
36/60:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('aaaa')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/61:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('approximated solution')
ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/62:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('approximated solution')
#ax.yaxis.set_tick_params(length=0)
#ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/63:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('approximated solution')
#ax.yaxis.set_tick_params(length=0)
ax.xaxis.set_tick_params(length=0)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/64:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('approximated solution')
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/65:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)')
ax.set_ylabel('curva y(t)')
ax.set_title('approximated solution', fontsize=16)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/66:
# Plotar os dados da curva y(t)
fig = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)', fontsize=13)
ax.set_ylabel('curva y(t)', fontsize=13)
ax.set_title('approximated solution', fontsize=16)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')
plt.show()
36/67:
# Plotando os dados aproximados utilizando odeint
appx = plt.figure(facecolor='w')
ax = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, y, 'r', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)', fontsize=13)
ax.set_ylabel('curva y(t)', fontsize=13)
ax.set_title('approximated solution', fontsize=16)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expt = plt.figure(facecolor='w')
ax = fid.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
ax.plot(t, ya, 'b', alpha=0.5, lw=2)
ax.set_xlabel('tempo t(s)', fontsize=13)
ax.set_ylabel('curva y(t)', fontsize=13)
ax.set_title('expected solution', fontsize=16)
ax.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/68:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fid.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/69:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/70:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/71:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/72:
fig = plt.figure(facecolor='w')
sub = plt.subplots(nrows=1, ncol=2)

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.subplot(1,2,1)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')
approximated.plot(t, y, 'r', alpha=0.5, lw=2)

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/73:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(1111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/74:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(112, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/75:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(11, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/76:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(110, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/77:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/78:
fig = plt.figure(facecolor='w')
tiledlayout(1,2)
# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

nexttile

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/79:
fig = plt.figure(facecolor='w')
t = tiledlayout(1,2)
# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

nexttile

# Plotando os dados esperados pela solução analítica
expected = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/80:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

figg = plt.figure(facecolor='w')
# Plotando os dados esperados pela solução analítica
expected = figg.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/81:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

fig2 = plt.figure(facecolor='w')
# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/82:
fig, fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')



# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/83:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/84:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/85:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/86:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/87:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/88:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/89:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/90:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(18.5, 10.5)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/91:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(10.5, 15.5)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/92:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(10.5, 5.5)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/93:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(14.5, 5.5)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/94:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.5, 5.5)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/95:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.5, 3.5)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/96:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/97:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(4.0, 2.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/98:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/99:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/100:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/101:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/102:
# Definindo a equação diferencial dada.
def deriv2(y, t):
    dydt = 1
    return dydt
36/103:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
36/104:
# Condição inicial
y0 = 4
# Solução analítica é dada por exp(t)
ya = t + 4
36/105:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/106:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(4, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/107:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/108:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/109:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/110:
# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
36/111:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/112:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/113:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
36/114:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6t))) + (((np.exp(6t))-(np.exp(-t)))/(1/7))
36/115:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/(1/7))
36/116:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/117:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.set_xlim(0, 1.0)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/118:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.set_xlim(0, 10)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/119:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.set_xlim(1, 10)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/120:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/121:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 1)
36/122:
# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
36/123:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
36/124:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/(1/7))
36/125:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/126:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 1000)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/127:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 100)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/128:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 500)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/129:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 500)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 500)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/130:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) - ((np.exp(-t))/(1/7))
36/131:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 500)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 500)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/132:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 500)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 500)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/133:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 500)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 500)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/134:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/135:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/136:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/137:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/138:
# Definindo a equação diferencial dada.
def deriv2(y, t):
    dydt = 1
    return dydt
36/139:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
36/140:
# Condição inicial
y0 = 4
# Solução analítica é dada por exp(t)
ya = t + 4
36/141:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/142:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/143:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2, label='aaa')
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/144:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2, label='aaa')
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = approximated.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/145:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = approximated.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/146:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = approximated.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/147:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = approximated.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/148:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/149:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/150:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/151:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/152:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/153:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/154:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/155:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 10)
36/156:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/157:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/158:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/159:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/160:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/161:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/162:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/163:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/164:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/165:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/166:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 200)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/167:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/168:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 1)
36/169:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/170:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/171:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/172:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/173:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/174:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/175:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/176:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/177:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/178:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/179:
# Definindo a equação diferencial dada.
def deriv2(y, t):
    dydt = 1
    return dydt
36/180:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
36/181:
# Condição inicial
y0 = 4
# Solução analítica é dada por exp(t)
ya = t + 4
36/182:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/183:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 250)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/184:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/185:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/186:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/187:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/188:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 250)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/189:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 10)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/190:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 40)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/191:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 20)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/192:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 800)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/193:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 250)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/194:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 10)
36/195:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/196:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/197:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/198:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 250)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/199:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/200:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/201:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/202:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/203:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 250)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/204:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 500)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/205:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 50)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/206:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 500)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 500)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/207:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/208:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/209:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/210:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/211:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 500)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 500)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/212:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 500)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/213:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 25)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/214:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 500)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/215:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 1000)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 25)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/216:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 1000)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 1000)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/217:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/218:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/219:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/220:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/221:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 1000)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 1000)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/222:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 140)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 1000)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/223:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 200)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 1000)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/224:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 100)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 1000)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/225:
fig = plt.figure(facecolor='w')
fig2 = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
approximated = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
approximated.plot(t, y, 'r', alpha=0.5, lw=2)
approximated.plot(t, ya, 'b--', alpha=0.5, lw=2)
approximated.set_xlabel('tempo t(s)', fontsize=13)
approximated.set_ylabel('curva y(t)', fontsize=13)
approximated.set_title('approximated solution', fontsize=16)
approximated.set_ylim(0, 100)
approximated.grid(b=True, which='major', c='w', lw=2, ls='-')

# Plotando os dados esperados pela solução analítica
fig2.set_size_inches(5.0, 3.0)
expected = fig2.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
expected.plot(t, ya, 'b', alpha=0.5, lw=2)
expected.set_xlabel('tempo t(s)', fontsize=13)
expected.set_ylabel('curva y(t)', fontsize=13)
expected.set_title('expected solution', fontsize=16)
expected.set_ylim(0, 100)
expected.grid(b=True, which='major', c='w', lw=2, ls='-')

plt.show()
36/226:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/227:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/228:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/229:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/230:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/231:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/232:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/233:
# Definindo a equação diferencial dada.
def deriv2(y, t):
    dydt = 1
    return dydt
36/234:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
36/235:
# Condição inicial
y0 = 4
# Solução analítica é dada por exp(t)
ya = t + 4
36/236:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/237:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 1)
36/238:
# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
36/239:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
36/240:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) - ((np.exp(-t))/(1/7))
36/241:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/242:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) - ((np.exp(-t))/7)
36/243:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/244:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
36/245:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/246:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/247:
# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
36/248:
# Definindo a equação diferencial dada.
def deriv4(y, t):
    y1 = y[0]
    y2 = y[1]
    dydt = y2
    d2ydt2 = 1 - (9*y1)
    diff_vet = [dydt, d2ydt2]
    return diff_vet
36/249:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(y, t):
    y1 = y[0]
    y2 = y[1]
    dydt = y2
    d2ydt2 = 1 - (9*y1)
    diff_vet = [dydt, d2ydt2]
    return diff_vet
36/250: #### Resolvendo com odeint
36/251:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/252:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/253:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 1)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/254:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 0.1)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/255:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(0, 0.5)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/256:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(y, t):
    diff_vet = [y[1], 1-(9*y[0])]
    return diff_vet
36/257:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/258:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(0, 0.5)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/259:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/260:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(y, t):
    diff_vet = [y[1], 1-(9*y[0])]
    return diff_vet
36/261:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/262:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(0, 0.5)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/263:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.5, 0.5)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/264:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/265:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(y, t):
    diff_vet = [y[1], 1-(9*y[0])]
    return diff_vet
36/266:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/267:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.5, 0.5)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/268:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.5, 0.5)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/269:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
##graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.5, 0.5)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/270:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/271:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(y, t):
    diff_vet = [y[1], 1-(9*y[0])]
    return diff_vet
36/272:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/273:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
##graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.5, 0.5)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/274:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
##graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.3, 0.3)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/275:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
##graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.4, 0.4)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/276:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(y, t):
    y = y[0]
    dy = y[1]
    diff_vet = [dy, 1-(9*y)]
    return diff_vet
36/277:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/278:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(y, t):
    y = y[0]
    dy = y[1]
    diff_vet = [dy, 1-(9*y)]
    return diff_vet
36/279:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/280:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 1-(9*y)]
    return diff_vet
36/281:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/282:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.4, 0.4)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/283:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy 1-(9*y)]
    return diff_vet
36/284:
# Definindo a equação diferencial dada.
# y é um vetor onde y[0] é a posição e y[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy + 1-(9*y)]
    return diff_vet
36/285:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/286:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.4, 0.4)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/287:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.4, 0.4)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/288:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 1)
36/289:
# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
36/290:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
36/291:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
36/292:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/293:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 250)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/294:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 500)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/295:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 0;5)
36/296:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 0.5)
36/297:
# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
36/298:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
36/299:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
36/300:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 500)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/301:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 50)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/302:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 30)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/303:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('approximated solution', fontsize=16)
graph.set_ylim(0, 50)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/304:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
36/305:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/306:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.4, 0.4)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/307:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/308:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
36/309:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/310:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.4, 0.4)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/311:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.4, 0.4)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/312:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.4, 0.4)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/313:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.3, 0.3)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/314:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/315:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
36/316:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/317:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.3, 0.3)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/318:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(0, 0.3)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/319:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(0, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/320:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.25, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/321:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.15, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/322:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/323:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 10)
36/324:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
36/325:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/326:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/327:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 2.5)
36/328:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
36/329:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/330:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/331:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 4)
36/332:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
36/333:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/334:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/335:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 4.5)
36/336:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
36/337:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/338:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/339:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/340:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
36/341:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/342:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
#graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/343:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/344:
# Condição inicial
y0 = [0,0]
# Solução analítica é dada por exp(t)
ya = (1/9) - ((np.cos(3*t))/9)
36/345:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('aproximated solution', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/346:
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
36/347:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/348:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
36/349:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
36/350:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
36/351:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/352:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
#fig.set_size_inches(5.0, 3.0)
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/353:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/354:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
36/355:
# Definindo a equação diferencial dada.
def deriv2(y, t):
    dydt = 1
    return dydt
36/356:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
36/357:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
36/358:
# Condição inicial
y0 = 4
# Solução analítica é dada por exp(t)
ya = t + 4
36/359:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/360:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 0.5)
36/361:
# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
36/362:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
36/363:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
36/364:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 50)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/365:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
36/366:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
36/367:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
36/368:
# Condição inicial
y0 = [0,0]
# Solução analítica é dada por exp(t)
ya = (1/9) - ((np.cos(3*t))/9)
36/369:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/370:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, y[:,1], 'y', alpha=0.5, lw=2, label='speed')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
36/371:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
38/1:
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
38/2:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
38/3:
# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
38/4:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
38/5:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
38/6:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
38/7:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
38/8:
# Definindo a equação diferencial dada.
def deriv2(y, t):
    dydt = 1
    return dydt
38/9:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
38/10:
# Condição inicial
y0 = 4
# Solução analítica é dada por exp(t)
ya = t + 4
38/11:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
38/12:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 0.5)
38/13:
# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
38/14:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
38/15:
# Condição inicial
y0 = 2
# Solução analítica é dada por exp(t)
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
38/16:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 50)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
38/17:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)
38/18:
# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
38/19:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
38/20:
# Condição inicial
y0 = [0,0]
# Solução analítica é dada por exp(t)
ya = (1/9) - ((np.cos(3*t))/9)
38/21:
fig = plt.figure(facecolor='w')

# Plotando os dados aproximados utilizando odeint
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
39/1:
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
39/2:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
39/3:
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
39/4:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)

# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
39/5:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
39/6:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
39/7:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated') # curva aproximada dada por y
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected') # curva esperada dada por ya
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
39/8:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)
39/9:
# Condição inicial
y0 = 4
# Solução analítica é dada por t + 4
ya = t + 4
39/10:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated') # curva aproximada dada por y
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected') # curva esperada dada por ya
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
39/11:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)

# Definindo a equação diferencial dada.
def deriv2(y, t):
    dydt = 1
    return dydt
39/12:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
39/13:
# Condição inicial
y0 = 4
# Solução analítica é dada por t + 4
ya = t + 4
39/14:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated') # curva aproximada dada por y
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected') # curva esperada dada por ya
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
39/15:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 0.5)

# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
39/16:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
39/17:
# Condição inicial
y0 = 2
# Solução analítica é dada por $y(t) = 2e^{6t} + \frac{e^{6t} - e^{-t}}{7}$
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
39/18:
# Condição inicial
y0 = 2
# Solução analítica é dada por $y(t) = 2e^{6t} + \frac{e^{6t} - e^{-t}}{7}$
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
39/19:
# Condição inicial
y0 = 2
# Solução analítica é dada por 2e^{6t} + ({e^{6t} - e^{-t}})/{7}
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
39/20:
# Condição inicial
y0 = 2
# Solução analítica é dada por 2e^{6t} + {e^{6t} - e^{-t}}/{7}
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
39/21:
# Condição inicial
y0 = 2
# Solução analítica é dada por 2e^{6t} + {e^{6t} - e^{-t}}/{7}
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
39/22:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated') # a curva aproximada dada por y
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected') # a curva esperada dada por ya
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 50)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
39/23:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)

# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
39/24:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
39/25:
# Condição inicial
y0 = [0,0]
# Solução analítica é dada por 1/9 - (cos(3t))/9
ya = (1/9) - ((np.cos(3*t))/9)
39/26:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated') # a curva aproximada é dada por y[:,0]
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected') # a curva esperada é dada por ya
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
40/1:
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
40/2:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)

# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
40/3:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
40/4:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
40/5:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated') # curva aproximada dada por y
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected') # curva esperada dada por ya
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
40/6:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)

# Definindo a equação diferencial dada.
def deriv2(y, t):
    dydt = 1
    return dydt
40/7:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
40/8:
# Condição inicial
y0 = 4
# Solução analítica é dada por t + 4
ya = t + 4
40/9:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated') # curva aproximada dada por y
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected') # curva esperada dada por ya
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
40/10:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 0.5)

# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
40/11:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
40/12:
# Condição inicial
y0 = 2
# Solução analítica é dada por 2e^{6t} + {e^{6t} - e^{-t}}/{7}
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
40/13:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated') # a curva aproximada dada por y
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected') # a curva esperada dada por ya
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 50)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
40/14:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)

# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
40/15:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
40/16:
# Condição inicial
y0 = [0,0]
# Solução analítica é dada por 1/9 - (cos(3t))/9
ya = (1/9) - ((np.cos(3*t))/9)
40/17:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated') # a curva aproximada é dada por y[:,0]
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected') # a curva esperada é dada por ya
graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
42/1:
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt
42/2:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)

# Definindo a equação diferencial dada.
def deriv(y, t):
    dydt = y
    return dydt
42/3:
# Condição inicial
y0 = 1
# Integrar a equação de acordo com t.
y = odeint(deriv, y0, t)
42/4:
# Condição inicial
y0 = 1
# Solução analítica é dada por exp(t)
ya = np.exp(t)
42/5:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)

# curva aproximada dada é por y
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')

# curva esperada dada é por ya
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')

graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 100)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
42/6:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 20)

# Definindo a equação diferencial dada.
def deriv2(y, t):
    dydt = 1
    return dydt
42/7:
# Condição inicial
y0 = 4
# Integrar a equação de acordo com t.
y = odeint(deriv2, y0, t)
42/8:
# Condição inicial
y0 = 4
# Solução analítica é dada por t + 4
ya = t + 4
42/9:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)

# curva aproximada é dada por y
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')

# curva esperada é dada por ya
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')

graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
42/10:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 0.5)

# Definindo a equação diferencial dada.
def deriv3(y, t):
    dydt = (6*y) + (np.exp(-t))
    return dydt
42/11:
# Condição inicial
y0 = 2
# Integrar a equação de acordo com t.
y = odeint(deriv3, y0, t)
42/12:
# Condição inicial
y0 = 2
# Solução analítica é dada por 2e^{6t} + {e^{6t} - e^{-t}}/{7}
ya = (2*(np.exp(6*t))) + (((np.exp(6*t))-(np.exp(-t)))/7)
42/13:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)

# a curva aproximada é dada por y
graph.plot(t, y, 'r', alpha=0.5, lw=2, label='approximated')

# curva esperada é dada por ya
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')

graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(0, 50)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
42/14:
# Definindo a grade de pontos no tempo (em segundos).
t = np.linspace(0, 5)

# Definindo a equação diferencial dada.
# k é um vetor onde k[0] é a posição e k[1] é a velocidade
def deriv4(k, t):
    y = k[0]
    dy = k[1]
    diff_vet = [dy, 0*dy - (9*y) + 1]
    return diff_vet
42/15:
# Vetor com as condições iniciais [y(0), y'(0)]
y0 = [0,0]
# Integrar a equação de acordo com t.
y = odeint(deriv4, y0, t)
42/16:
# Condição inicial
y0 = [0,0]
# Solução analítica é dada por 1/9 - (cos(3t))/9
ya = (1/9) - ((np.cos(3*t))/9)
42/17:
fig = plt.figure(facecolor='w')

# Plotando os dados
graph = fig.add_subplot(111, facecolor='#F2F2F2', axisbelow=True)

# a curva aproximada é dada por y[:,0]
graph.plot(t, y[:,0], 'r', alpha=0.5, lw=2, label='approximated')

# a curva esperada é dada por ya
graph.plot(t, ya, 'b--', alpha=0.5, lw=2, label='expected')

graph.set_xlabel('tempo t(s)', fontsize=13)
graph.set_ylabel('curva y(t)', fontsize=13)
graph.set_title('solutions', fontsize=16)
graph.set_ylim(-0.05, 0.25)
graph.grid(b=True, which='major', c='w', lw=2, ls='-')
legend = graph.legend()
legend.get_frame().set_alpha(0.5)

plt.show()
47/1:
import numpy as np
import matplotlib.pyplot as plt
import math
47/2:
def delta(L, N):
    return L/N
47/3:
def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 1):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
47/4: def impedance():
47/5:
def impedance():
    return
47/6:
def tension(N):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        tensions.append()
47/7:
def tension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
47/8:
def impedance(N, Delta):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                impedances.append((Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2))))
    
    
    return
47/9:
def impedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    
    return
47/10:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def impedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
47/11:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def tension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
47/12:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/1: Delta = delta(0.1, 15)
49/2:
import numpy as np
import matplotlib.pyplot as plt
import math
49/3:
def delta(L, N):
    return L/N
49/4:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 1):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/5:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def impedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/6:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def tension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/7:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/8: Delta = delta(0.1, 15)
49/9:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
49/10:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
49/11:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 1):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/12:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/13:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/14:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/15:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
49/16:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/17:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/18:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/19:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
49/20:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/21:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/22:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/23:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers)
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/24:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(len(centers))
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/25:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/26:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers)
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/27:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 1):
        print(k)
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/28:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/29:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/30:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/31:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers)
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/32:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/33:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/34:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/35:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/36:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/37:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/38:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 3):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/39:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 3):
        print(k)
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/40:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/41:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/42:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/43:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/44:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/45:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/46:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/47:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/48:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/49:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        print(k)
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/50:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/51:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/52:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/53:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/54:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(len(centers))
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/55:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 1):
        print(k)
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/56:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/57:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/58:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/59:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(len(centers))
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/60:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []

        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/61:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/62:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    for i in range(1, N+1):
        for j in range(1, N+1):
            centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/63:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/64:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/65:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/66:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(len(centers))
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/67:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    for i in range(1, N+1):
        for j in range(1, N+1):
            centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, d])
    
    return centers
49/68:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/69:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/70:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/71:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(len(centers))
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/72:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        print(k)
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/73:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/74:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/75:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/76:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(len(centers))
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/77:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/78:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/79:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = true
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = false
        tensions.append(0 if flag else V0)
49/80:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/81:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(len(centers))
print(centers[220])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/82:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(len(centers))
print(centers[400])
#impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/83:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
#tensions = setTension(15, 1)
49/84:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
49/85:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
49/86:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/87:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
49/88:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
print(tensions)
49/89:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/90:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/91:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
print(tensions)
49/92:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
print(len(tensions))
49/93:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
print(impedances)
49/94:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
print(len(impedances))
49/95:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
49/96:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/97:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/98:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
print(len(coefficients))
49/99:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/100:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(n, n))
    Y = np.reshape(Y,(n, n))
    Z = np.reshape(Z,(n, n))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/101: superiorPlot(centers, coefficients, 15)
49/102:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/103: superiorPlot(centers, coefficients, 15)
49/104:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(round(centers, 3))
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/105:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
for it in centers:
    print(round(it), 3)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/106:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
for i in range(2 * N**2):
    for j in range(3):
        print(round(centers[i][j], 3), end = ' ')
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/107:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
for i in range(2 * 15**2):
    for j in range(3):
        print(round(centers[i][j], 3), end = ' ')
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/108:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
for i in range(2 * 15**2):
    for j in range(3):
        print(round(centers[i][j], 3), end = ' ')
    print()
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/109:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
for i in range(2 * 10**2):
    for j in range(3):
        print(round(centers[i][j], 3), end = ' ')
    print()
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/110:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(j+(1/2))*Delta, (i+(1/2))*Delta, k*d])
    
    return centers
49/111:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/112:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/113:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/114:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
for i in range(2 * 10**2):
    for j in range(3):
        print(round(centers[i][j], 3), end = ' ')
    print()
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/115:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/116: superiorPlot(centers, coefficients, 15)
49/117: superiorPlot(centers, coefficients, 10)
49/118:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/119:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/120: superiorPlot(centers, coefficients, 15)
49/121:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (pow(Delta, 2))/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/122:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/123:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/124:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/125:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/126: superiorPlot(centers, coefficients, 15)
49/127:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/128:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/129:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/130:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/131:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/132:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/133:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/134: superiorPlot(centers, coefficients, 15)
49/135: superiorPlot(centers, coefficients, 10)
49/136:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    figr = plt.figure(figsize=plt.figaspect(0.5))
    ax = figr.add_subplot(1, 2, 1, projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title(texto + ' (z = 0)')
    plt.show()
49/137: superiorPlot(centers, coefficients, 10)
49/138:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    figr = plt.figure(figsize=plt.figaspect(0.5))
    ax = figr.add_subplot(1, 2, 1, projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title(' (z = 0)')
    plt.show()
49/139: superiorPlot(centers, coefficients, 10)
49/140:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/141: superiorPlot(centers, coefficients, 10)
49/142:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    def aux(m,n):
        if m == n:
            return (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2))) 
        
        return 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
    
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            impedances[m].append(aux(m,n))
    
    return impedances
49/143:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/144:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/145:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/146:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/147: superiorPlot(centers, coefficients, 10)
49/148:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/149:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/150:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/151:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/152:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/153:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/154:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/155:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/156:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/157: superiorPlot(centers, coefficients, 10)
49/158:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(j-(1/2))*Delta, (i-(1/2))*Delta, k*d])
    
    return centers
49/159:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/160:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/161:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/162:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/163:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/164: superiorPlot(centers, coefficients, 10)
49/165:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(2*N*N):
        impedances.append([])
        for n in range(2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/166:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/167:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/168:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/169:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/170: superiorPlot(centers, coefficients, 10)
49/171:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = Delta/(math.pi * epsilon0) * math.log(1 + math.sqrt(2))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/172:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/173:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/174:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/175:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/176: superiorPlot(centers, coefficients, 10)
49/177:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
print(tensions)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
49/178:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
print(tensions)
coefficients = linear_solution(impedances, tensions)
49/179:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/180:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([(i-(1/2))*Delta, (j-(1/2))*Delta, k*d])
    
    return centers
49/181:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/182:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/183:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/184:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/185:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/186: superiorPlot(centers, coefficients, 10)
49/187:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([i*Delta - Delta/2, j*Delta - Delta/2, k*d])
                
    
    return centers
49/188:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/189:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/190:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/191:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/192:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/193: superiorPlot(centers, coefficients, 10)
49/194:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    print(X)
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/195: superiorPlot(centers, coefficients, 10)
49/196:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    print(X)
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    print(X)
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/197: superiorPlot(centers, coefficients, 10)
49/198:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([i*Delta - Delta/2, j*Delta - Delta/2, k*d])
                
    
    return centers

def func_R(n):
  R = []
  for i in range(1, n + 1):
    for j in range(1, n + 1):
      r = [j * delta - delta/2, i * delta - delta/2, 0]
      R.append(r)

  for i in range(1, n + 1):
    for j in range(1, n + 1):
      r = [j * delta - delta/2, i * delta - delta/2, d]
      R.append(r)

  return R
49/199:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([i*Delta - Delta/2, j*Delta - Delta/2, k*d])
                
    
    return centers

def func_R(n):
  R = []
  for i in range(1, n + 1):
    for j in range(1, n + 1):
      r = [j * delta - delta/2, i * delta - delta/2, 0]
      R.append(r)

  for i in range(1, n + 1):
    for j in range(1, n + 1):
      r = [j * delta - delta/2, i * delta - delta/2, d]
      R.append(r)

  return R
49/200:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)

n = 15
delta = lado/n
R = func_R(n)
print(R)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/201:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)

n = 15
delta = 0.1/n
R = func_R(n)
print(R)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/202:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([i*Delta - Delta/2, j*Delta - Delta/2, k*d])
                
    
    return centers

def func_R(n):
  R = []
  for i in range(1, n + 1):
    for j in range(1, n + 1):
      r = [j * delta - delta/2, i * delta - delta/2, 0]
      R.append(r)

  for i in range(1, n + 1):
    for j in range(1, n + 1):
      r = [j * delta - delta/2, i * delta - delta/2, 0.001]
      R.append(r)

  return R
49/203:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/204:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/205:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/206:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)

n = 15
delta = 0.1/n
R = func_R(n)
print(R)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/207:

n = 15
delta = 0.1/n
R = func_R(n)
print(R)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/208:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/209:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([i*Delta - Delta/2, j*Delta - Delta/2, k*d])
                
    
    return centers
49/210:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/211:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/212:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/213:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/214:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/215:
def delta(L, N):
    return L/N
49/216:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([i*Delta - Delta/2, j*Delta - Delta/2, k*d])
                
    
    return centers
49/217:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/218:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/219:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/220:
Delta = delta(0.1, 10)
centers = setCenters(10, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/221:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    print(X)
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    print(X)
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/222: superiorPlot(centers, coefficients, 10)
49/223:
Delta = delta(0.1, 15)
centers = setCenters(10, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/224:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/225:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    print(X)
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    print(X)
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/226: superiorPlot(centers, coefficients, 10)
49/227:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/228:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/229:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(10, Delta, centers)
tensions = setTension(10, 1)
coefficients = linear_solution(impedances, tensions)
49/230:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    print(X)
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    print(X)
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/231: superiorPlot(centers, coefficients, 10)
49/232: superiorPlot(centers, coefficients, 15)
49/233:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    print(X)
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    print(X)
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/234: superiorPlot(centers, coefficients, 15)
49/235:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/236:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    print(X)
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/237: superiorPlot(centers, coefficients, 15)
49/238:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/239: superiorPlot(centers, coefficients, 15)
49/240:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

'''def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances'''

def func_impedancia(n, centers, delta):
  def z(m, n):
    if (m == n):
      # Zmn = delta/piE0 * ln[1 + raiz(2)]
      return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
    else:
      # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
      return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))

  Z = []

  for i in range(0, 2 * n * n):
    Z.append([])
    for j in range(0, 2 * n * n):
      Z[i].append(z(i, j))

  return Z
49/241:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

'''def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances'''

def func_impedancia(n, centers, delta):
  def z(m, n):
    if (m == n):
      # Zmn = delta/piE0 * ln[1 + raiz(2)]
      return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
    else:
      # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
      return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))

  Z = []

  for i in range(0, 2 * n * n):
    Z.append([])
    for j in range(0, 2 * n * n):
      Z[i].append(z(i, j))

  return Z
49/242:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/243:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/244:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/245:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/246: superiorPlot(centers, coefficients, 15)
49/247:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def func_impedancia(n, centers, delta):
  def z(m, n):
    if (m == n):
      # Zmn = delta/piE0 * ln[1 + raiz(2)]
      return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
    else:
      # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
      return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))

  Z = []

  for i in range(0, 2 * n * n):
    Z.append([])
    for j in range(0, 2 * n * n):
      Z[i].append(z(i, j))

  return Z
49/248:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/249:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def func_impedancia(n, centers, delta):
    e0 = 8.85e-12
    def z(m, n):
        if (m == n):
          # Zmn = delta/piE0 * ln[1 + raiz(2)]
          return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
        else:
          # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
          return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))

    Z = []

    for i in range(0, 2 * n * n):
        Z.append([])
        for j in range(0, 2 * n * n):
            Z[i].append(z(i, j))

    return Z 


'''def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances'''
49/250:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/251:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/252:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/253:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/254: superiorPlot(centers, coefficients, 15)
49/255:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])
    print(Z)

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/256:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])
    print(Z)

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/257:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])
    print(Z)

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/258: superiorPlot(centers, coefficients, 15)
49/259:
def superiorPlot(centers, coefficients, N):
    C = np.array(centers)
    a = np.array(coefficients)

    x1 = []
    y1 = []
    z1 = []
    x2 = []
    y2 = []
    z2 = []

    for i in range(2 * (N**2)):
        x = C[i][0]
        y = C[i][1]
        z = a[i]

        x1.append(x) if i < (N**2) else x2.append(x)
        y1.append(y) if i < (N**2) else y2.append(y)
        z1.append(z) if i < (N**2) else z2.append(z)

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x1,(N, N)), np.reshape(y1,(N, N)), np.reshape(z1,(N, N)))
    ax.set_title('Placa em z = 0 para N = ' + str(N))
    plt.show()

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x2,(N, N)), np.reshape(y2,(N, N)), np.reshape(z2,(N, N)))
    ax.set_title('Placa em z = d para N = ' + str(N))
    plt.show()
49/260: superiorPlot(centers, coefficients, 15)
49/261:
def superiorPlot(centers, coefficients, N):
    C = np.array(centers)
    a = np.array(coefficients)

    x1 = []
    y1 = []
    z1 = []
    x2 = []
    y2 = []
    z2 = []

    for i in range(2 * (N**2)):
        x = C[i][1]
        y = C[i][0]
        z = a[i]

        x1.append(x) if i < (N**2) else x2.append(x)
        y1.append(y) if i < (N**2) else y2.append(y)
        z1.append(z) if i < (N**2) else z2.append(z)

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x1,(N, N)), np.reshape(y1,(N, N)), np.reshape(z1,(N, N)))
    ax.set_title('Placa em z = 0 para N = ' + str(N))
    plt.show()

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x2,(N, N)), np.reshape(y2,(N, N)), np.reshape(z2,(N, N)))
    ax.set_title('Placa em z = d para N = ' + str(N))
    plt.show()
49/262: superiorPlot(centers, coefficients, 15)
49/263:
def superiorPlot(centers, coefficients, N):
    C = np.array(centers)
    a = np.array(coefficients)

    x1 = []
    y1 = []
    z1 = []
    x2 = []
    y2 = []
    z2 = []

    for i in range(2 * (N**2)):
        x = C[i][0]
        y = C[i][1]
        z = a[i]

        x1.append(x) if i < (N**2) else x2.append(x)
        y1.append(y) if i < (N**2) else y2.append(y)
        z1.append(z) if i < (N**2) else z2.append(z)

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x1,(N, N)), np.reshape(y1,(N, N)), np.reshape(z1,(N, N)))
    ax.set_title('Placa em z = 0 para N = ' + str(N))
    plt.show()

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x2,(N, N)), np.reshape(y2,(N, N)), np.reshape(z2,(N, N)))
    ax.set_title('Placa em z = d para N = ' + str(N))
    plt.show()
49/264: superiorPlot(centers, coefficients, 15)
49/265:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(N):
            for j in range(N):
                centers.append([i*Delta - Delta/2, j*Delta - Delta/2, k*d])
                
    
    return centers
49/266:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def func_impedancia(n, centers, delta):
    e0 = 8.85e-12
    def z(m, n):
        if (m == n):
          # Zmn = delta/piE0 * ln[1 + raiz(2)]
          return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
        else:
          # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
          return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))

    Z = []

    for i in range(0, 2 * n * n):
        Z.append([])
        for j in range(0, 2 * n * n):
            Z[i].append(z(i, j))

    return Z 


'''def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances'''
49/267:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/268:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/269:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/270:
def superiorPlot(centers, coefficients, N):
    C = np.array(centers)
    a = np.array(coefficients)

    x1 = []
    y1 = []
    z1 = []
    x2 = []
    y2 = []
    z2 = []

    for i in range(2 * (N**2)):
        x = C[i][0]
        y = C[i][1]
        z = a[i]

        x1.append(x) if i < (N**2) else x2.append(x)
        y1.append(y) if i < (N**2) else y2.append(y)
        z1.append(z) if i < (N**2) else z2.append(z)

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x1,(N, N)), np.reshape(y1,(N, N)), np.reshape(z1,(N, N)))
    ax.set_title('Placa em z = 0 para N = ' + str(N))
    plt.show()

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x2,(N, N)), np.reshape(y2,(N, N)), np.reshape(z2,(N, N)))
    ax.set_title('Placa em z = d para N = ' + str(N))
    plt.show()
49/271: superiorPlot(centers, coefficients, 15)
49/272:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([i*Delta - Delta/2, j*Delta - Delta/2, k*d])
                
    
    return centers
49/273:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)


def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/274:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/275:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/276:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/277:
def superiorPlot(centers, coefficients, N):
    C = np.array(centers)
    a = np.array(coefficients)

    x1 = []
    y1 = []
    z1 = []
    x2 = []
    y2 = []
    z2 = []

    for i in range(2 * (N**2)):
        x = C[i][0]
        y = C[i][1]
        z = a[i]

        x1.append(x) if i < (N**2) else x2.append(x)
        y1.append(y) if i < (N**2) else y2.append(y)
        z1.append(z) if i < (N**2) else z2.append(z)

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x1,(N, N)), np.reshape(y1,(N, N)), np.reshape(z1,(N, N)))
    ax.set_title('Placa em z = 0 para N = ' + str(N))
    plt.show()

    f = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(np.reshape(x2,(N, N)), np.reshape(y2,(N, N)), np.reshape(z2,(N, N)))
    ax.set_title('Placa em z = d para N = ' + str(N))
    plt.show()
49/278: superiorPlot(centers, coefficients, 15)
49/279:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])
    print(Z)

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/280: superiorPlot(centers, coefficients, 15)
49/281:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/282: superiorPlot(centers, coefficients, 15)
49/283:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
superiorPlot(centers, coefficients, 15)
50/1:
import numpy as np
import matplotlib.pyplot as plt
import math
50/2:
v0 = 1          # potencial da placa em z = d
                # potencial da outra placa eh igual a 0
e0 = 8.85e-12   # permissividade no vacuo
lado = 0.1      # tamanho do lado L da placa em metros
d = 0.001       # distancia entre as placas condutoras
n = 15           # numero de elementos

delta = lado/n  # tamanho do lado de cada elemento
50/3:
# criando uma matriz contendo as coordenadas para o ponto no centro de cada elemento
# rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)
# vai ter 2*n*n elementos, então 2*n*n centros
def func_R(n):
  R = []
  for i in range(1, n + 1):
    for j in range(1, n + 1):
      r = [j * delta - delta/2, i * delta - delta/2, 0]
      R.append(r)

  for i in range(1, n + 1):
    for j in range(1, n + 1):
      r = [j * delta - delta/2, i * delta - delta/2, d]
      R.append(r)

  return R
50/4:
# criando a matriz de impedancia
# r_m = (xp, yq, zs)
# r_n = (xi, yj, zk)
def func_impedancia(n):
  def z(m, n):
    if (m == n):
      # Zmn = delta/piE0 * ln[1 + raiz(2)]
      return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
    else:
      # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
      return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(R[m][0] - R[n][0], 2) + pow(R[m][1] - R[n][1], 2) + pow(R[m][2] - R[n][2], 2))

  Z = []

  for i in range(0, 2 * n * n):
    Z.append([])
    for j in range(0, 2 * n * n):
      Z[i].append(z(i, j))

  return Z
50/5:
# criando a matriz de potencial
def func_potencial(n):
  V = []
  for i in range(0, n * n):   # ate a linha n² o potencial eh 0
    V.append(0)

  for i in range(n * n, 2 * n * n): # a partir dela eh v0
    V.append(v0)

  return V
50/6:
# resolvendo o sistema
# Zmn * am = Vm

def solve(Z, V):
    A = np.linalg.solve(Z, V)
    return A
50/7:
# aproximacao pra distribuicao de carga superficial = soma dos elementos de A
def plot1(A):
    plt.plot(A)
    plt.xlabel('Elemento')
    plt.ylabel('Solucao')

    distribuicao = sum(A)
    print("rho =", distribuicao)
50/8:
# Testando o resultado do sistema
#for i in range(0, 2 * n * n):
#    result = 0
#    for j in range(0, 2 * n * n):
#        result += Z[i][j] * A[j]
#    print("Resultado:", result, "Valor esperado: ", V[i])
50/9:
def plot2(R, A, n):
    R = np.array(R)

    X = R[:len(R)//2, 0]
    Y = R[:len(R)//2, 1]
    Z = np.array(A[:len(A)//2])

    X = np.reshape(X,(n, n))
    Y = np.reshape(Y,(n, n))
    Z = np.reshape(Z,(n, n))

    fig = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
50/10:
def plot3(R, A, n):
    R = np.array(R)

    X = R[len(R)//2:len(R), 0]
    Y = R[len(R)//2:len(R), 1]
    Z = np.array(A[len(A)//2:len(A)])

    X = np.reshape(X,(n,n))
    Y = np.reshape(Y,(n,n))
    Z = np.reshape(Z,(n,n))

    fig = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = d')
    plt.show()
50/11:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
50/12:
n = 10
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
50/13:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
50/14:
n = 20
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
50/15:
n = 10
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
C = sum(A[:n * n]) * delta**2 / v0
print(C)
50/16:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
C = sum(A[:n * n]) * delta**2 / v0
print(C)
50/17:
n = 20
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
C = sum(A[:n * n]) * delta**2 / v0
print(C)
50/18:
n = 25
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
C = sum(A[:n * n]) * delta**2 / v0
print(C)
50/19:
# capacitancia teoria
print(e0*lado**2/d)
50/20:
n = 15
delta = lado/n
R = func_R(n)
print(R)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
49/284:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
print(centers)
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/285:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
for it in centers:
    print(it)
    print()
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
50/21:
n = 15
delta = lado/n
R = func_R(n)
for it in R:
    print(it)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
49/286:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
for it in centers:
    print(it)
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/287:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([j*Delta - Delta/2, i*Delta - Delta/2, k*d])
                
    
    return centers
49/288:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)


def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/289:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/290:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/291:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
for it in centers:
    print(it)
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/292:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
for it in centers:
    print(it)
print(len(centers))
impedances = func_impedancia(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
50/22:
n = 15
delta = lado/n
R = func_R(n)
for it in R:
    print(it)
print(len(R))
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
49/293:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedances(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/294:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)


def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/295:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/296:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/297:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedances(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/298:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/299:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/300:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/301:
import numpy as np
import matplotlib.pyplot as plt
import math
49/302:
def delta(L, N):
    return L/N
49/303:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([j*Delta - Delta/2, i*Delta - Delta/2, k*d])
                
    
    return centers
49/304:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)


def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/305:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/306:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/307:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/308:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/309:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/310:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/311:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/312:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/313:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/314:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/315:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/316:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/317:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0) # * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/318:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/319:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/320:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, centers, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/321:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/322:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/323:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/324:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/325:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
for it in impedances:
    print(it)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/326:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
for it in impedances:
    print(it[0])
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
50/23:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
for it in Z:
    print(it[0])
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
49/327:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients)
50/24:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
print(A)
plot2(R, A, n)
plot3(R, A, n)
49/328:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
print(tensions)
coefficients = linear_solution(impedances, tensions)
50/25:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
print(V)
A = solve(Z, V)
print(A)
plot2(R, A, n)
plot3(R, A, n)
50/26:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
print(V)
plot2(R, A, n)
plot3(R, A, n)
49/329:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
print(tensions)
print(len(tensions))
coefficients = linear_solution(impedances, tensions)
50/27:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
print(V)
print(len(V))
plot2(R, A, n)
plot3(R, A, n)
50/28:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
for it in Z:
    print(it[4])
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
49/330:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
for it in impedances:
    print(it[4])
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/331:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
for it in impedances:
    print(it[50])
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
50/29:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
for it in Z:
    print(it[50])
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
49/332:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(tensions, impedances)
49/333:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/334:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 0]
    Y = centers[:len(centers)//2, 1]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/335:
def plot2(R, A, n):
    R = np.array(R)

    X = R[:len(R)//2, 0]
    Y = R[:len(R)//2, 1]
    Z = np.array(A[:len(A)//2])

    X = np.reshape(X,(n, n))
    Y = np.reshape(Y,(n, n))
    Z = np.reshape(Z,(n, n))

    fig = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/336: plot2(centers, coefficients, 15)
49/337:
def plot2(R, A, n):
    R = np.array(R)

    X = R[:len(R)//2, 0]
    Y = R[:len(R)//2, 1]
    Z = np.array(A[:len(A)//2])

    X = np.reshape(X,(n, n))
    Y = np.reshape(Y,(n, n))
    Z = np.reshape(Z,(n, n))

    fig = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='black')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/338:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
print(impedances[65][6])
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
50/30:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
print(Z[65][6])
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
49/339:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
print(impedances[67][6])
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
50/31:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
print(Z[67][6])
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
49/340:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
print(impedances[67][87])
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
50/32:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
print(Z[67][87])
V = func_potencial(n)
A = solve(Z, V)
plot2(R, A, n)
plot3(R, A, n)
49/341:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
print(impedances[67][87])
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients[5])
49/342:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients[5])
50/33:
n = 15
delta = lado/n
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
print(A[5])
plot2(R, A, n)
plot3(R, A, n)
49/343:
Delta = delta(0.1, 15)
print(Delta)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
print(coefficients[5])
50/34:
n = 15
delta = lado/n
print(delta)
R = func_R(n)
Z = func_impedancia(n)
V = func_potencial(n)
A = solve(Z, V)
print(A[5])
plot2(R, A, n)
plot3(R, A, n)
49/344:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/345: superiorPlot(centers, coefficients, 15)
49/346:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([j*Delta - Delta/2, i*Delta - Delta/2, k*d])
                
    
    return centers

def func_R(n):
    R = []
    for i in range(1, n + 1):
        for j in range(1, n + 1):
            r = [j * delta - delta/2, i * delta - delta/2, 0]
            R.append(r)

    for i in range(1, n + 1):
        for j in range(1, n + 1):
            r = [j * delta - delta/2, i * delta - delta/2, d]
            R.append(r)

    return R
49/347:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/348:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/349:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([j*Delta - Delta/2, i*Delta - Delta/2, k*d])
                
    
    return centers

def func_R(n, delta, d):
    R = []
    for i in range(1, n + 1):
        for j in range(1, n + 1):
            r = [j * delta - delta/2, i * delta - delta/2, 0]
            R.append(r)

    for i in range(1, n + 1):
        for j in range(1, n + 1):
            r = [j * delta - delta/2, i * delta - delta/2, d]
            R.append(r)

    return R
49/350:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances
49/351:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/352:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/353:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/354:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()

impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/355:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/356:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances

def func_impedancia(n):
    def z(m, n):
        if (m == n):
          # Zmn = delta/piE0 * ln[1 + raiz(2)]
          return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
        else:
          # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
          return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(R[m][0] - R[n][0], 2) + pow(R[m][1] - R[n][1], 2) + pow(R[m][2] - R[n][2], 2))

    Z = []

    for i in range(0, 2 * n * n):
        Z.append([])
        for j in range(0, 2 * n * n):
            Z[i].append(z(i, j))

    return Z
49/357:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = (Delta * Delta)/(4 * math.pi * epsilon0 * math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2)))
                impedances[m].append(zmn)
    
    return impedances

def func_impedancia(n, R, delta):
    e0 = 8.85e-12
    def z(m, n):
        if (m == n):
          # Zmn = delta/piE0 * ln[1 + raiz(2)]
          return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
        else:
          # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
          return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(R[m][0] - R[n][0], 2) + pow(R[m][1] - R[n][1], 2) + pow(R[m][2] - R[n][2], 2))

    Z = []

    for i in range(0, 2 * n * n):
        Z.append([])
        for j in range(0, 2 * n * n):
            Z[i].append(z(i, j))

    return Z
49/358:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/359:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/360:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
Z.sort()
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/361:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
Z.sort()
if(impedances == Z):
    print("igual")
else:
    print("f")
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/362:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances

def func_impedancia(n, R, delta):
    e0 = 8.85e-12
    def z(m, n):
        if (m == n):
          # Zmn = delta/piE0 * ln[1 + raiz(2)]
          return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
        else:
          # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
          return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(R[m][0] - R[n][0], 2) + pow(R[m][1] - R[n][1], 2) + pow(R[m][2] - R[n][2], 2))

    Z = []

    for i in range(0, 2 * n * n):
        Z.append([])
        for j in range(0, 2 * n * n):
            Z[i].append(z(i, j))

    return Z
49/363:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/364:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/365:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
Z.sort()
if(impedances == Z):
    print("igual")
else:
    print("f")
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/366:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances

def func_impedancia(n, R, delta):
    e0 = 8.85e-12
    def z(m, n):
        if (m == n):
          # Zmn = delta/piE0 * ln[1 + raiz(2)]
          return delta/(math.pi * e0) * math.log(1 + math.sqrt(2))
        else:
          # Zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
          return 1/(4 * math.pi * e0) * pow(delta, 2)/math.sqrt(pow(R[m][0] - R[n][0], 2) + pow(R[m][1] - R[n][1], 2) + pow(R[m][2] - R[n][2], 2))

    Z = []

    for i in range(0, 2 * n * n):
        Z.append([])
        for j in range(0, 2 * n * n):
            Z[i].append(z(i, j))

    return Z
49/367:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions
49/368:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/369:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
Z.sort()
if(impedances == Z):
    print("igual")
else:
    print("f")
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/370:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
Z.sort()
if(impedances == Z):
    print("igual")
else:
    print("f")
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/371:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/372: superiorPlot(centers, coefficients, 15)
49/373:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions

def func_potencial(n):
    V = []
    for i in range(0, n * n):   # ate a linha n² o potencial eh 0
        V.append(0)

    for i in range(n * n, 2 * n * n): # a partir dela eh v0
        V.append(v0)

    return V
49/374:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
Z.sort()
if(impedances == Z):
    print("igual")
else:
    print("f")
tensions = setTension(15, 1)
V = func_potencial(15, 1)
coefficients = linear_solution(impedances, tensions)
49/375:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []
    
    flag = True
    for n in range(0, 2*N*N):
        if (n >= N*N + 1): 
            flag = False
        tensions.append(0 if flag else V0)
    
    return tensions

def func_potencial(n, v0):
    V = []
    for i in range(0, n * n):   # ate a linha n² o potencial eh 0
        V.append(0)

    for i in range(n * n, 2 * n * n): # a partir dela eh v0
        V.append(v0)

    return V
49/376:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/377:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
Z.sort()
if(impedances == Z):
    print("igual")
else:
    print("f")
tensions = setTension(15, 1)
V = func_potencial(15, 1)
coefficients = linear_solution(impedances, tensions)
49/378:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
Z.sort()
if(impedances == Z):
    print("igual")
else:
    print("f")
tensions = setTension(15, 1)
V = func_potencial(15, 1)
tensions.sort()
V.sort()
if(tensions == V):
    print("igual")
else:
    print("f")
coefficients = linear_solution(impedances, tensions)
49/379:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []

    for n in range(0, 2*N*N):
        tensions.append(0 if (n < N*N) else V0)
    
    return tensions

def func_potencial(n, v0):
    V = []
    for i in range(0, n * n):   # ate a linha n² o potencial eh 0
        V.append(0)

    for i in range(n * n, 2 * n * n): # a partir dela eh v0
        V.append(v0)

    return V
49/380:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/381:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
centers.sort()
R.sort()
if(centers == R):
    print("igual")
else:
    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
impedances.sort()
Z.sort()
if(impedances == Z):
    print("igual")
else:
    print("f")
tensions = setTension(15, 1)
V = func_potencial(15, 1)
tensions.sort()
V.sort()
if(tensions == V):
    print("igual")
else:
    print("f")
coefficients = linear_solution(impedances, tensions)
49/382:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/383: superiorPlot(centers, coefficients, 15)
49/384:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
#centers.sort()
#R.sort()
#if(centers == R):
#    print("igual")
#else:
3    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
#impedances.sort()
#Z.sort()
#if(impedances == Z):
    print("igual")
else:
    print("f")
tensions = setTension(15, 1)
V = func_potencial(15, 1)
tensions.sort()
V.sort()
if(tensions == V):
    print("igual")
else:
    print("f")
coefficients = linear_solution(impedances, tensions)
49/385:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
#centers.sort()
#R.sort()
#if(centers == R):
#    print("igual")
#else:
3    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
#impedances.sort()
#Z.sort()
#if(impedances == Z):
#    print("igual")
#else:
#    print("f")
tensions = setTension(15, 1)
V = func_potencial(15, 1)
#tensions.sort()
V.sort()
if(tensions == V):
    print("igual")
else:
    print("f")
coefficients = linear_solution(impedances, tensions)
49/386:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
#centers.sort()
#R.sort()
#if(centers == R):
#    print("igual")
#else:
3    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
#impedances.sort()
#Z.sort()
#if(impedances == Z):
#    print("igual")
#else:
#    print("f")
tensions = setTension(15, 1)
V = func_potencial(15, 1)
#tensions.sort()
#V.sort()
#if(tensions == V):
#    print("igual")
#else:
#    print("f")
coefficients = linear_solution(impedances, tensions)
49/387:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
R = func_R(15, Delta, 0.001)
#centers.sort()
#R.sort()
#if(centers == R):
#    print("igual")
#else:
#    print("f")
impedances = setImpedance(15, Delta, centers)
Z = func_impedancia(15, R, Delta)
#impedances.sort()
#Z.sort()
#if(impedances == Z):
#    print("igual")
#else:
#    print("f")
tensions = setTension(15, 1)
V = func_potencial(15, 1)
#tensions.sort()
#V.sort()
#if(tensions == V):
#    print("igual")
#else:
#    print("f")
coefficients = linear_solution(impedances, tensions)
49/388:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/389: superiorPlot(centers, coefficients, 15)
49/390:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([j*Delta - Delta/2, i*Delta - Delta/2, k*d])
                
    
    return centers
49/391:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/392:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []

    for n in range(0, 2*N*N):
        tensions.append(0 if (n < N*N) else V0)
    
    return tensions
49/393:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/394:
Delta = delta(0.1, 15)
centers = setCenters(15, Delta, 0.001)
impedances = setImpedance(15, Delta, centers)
tensions = setTension(15, 1)
coefficients = linear_solution(impedances, tensions)
49/395:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/396: superiorPlot(centers, coefficients, 15)
49/397: superiorPlot(centers, coefficients, 15)
49/398: superiorPlot(centers, coefficients, 15)
49/399:
Delta = delta(0.1, 12)
centers = setCenters(12, Delta, 0.001)
impedances = setImpedance(12, Delta, centers)
tensions = setTension(12, 1)
coefficients = linear_solution(impedances, tensions)
49/400:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/401: superiorPlot(centers, coefficients, 12)
49/402:
Delta = delta(0.1, 12)
centers = setCenters(12, Delta, 0.001)
impedances = setImpedance(12, Delta, centers)
tensions = setTension(12, 1)
coefficients = linear_solution(impedances, tensions)
49/403:
Delta = delta(0.1, 12)
centers = setCenters(12, Delta, 0.001)
impedances = setImpedance(12, Delta, centers)
tensions = setTension(12, 1)

coefficients = linear_solution(impedances, tensions)
49/404:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/405: superiorPlot(centers, coefficients, 12)
49/406:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/407: inferiorPlot(centers, coefficients, 12)
49/408:
Delta = delta(0.1, 20)
centers = setCenters(20, Delta, 0.001)
impedances = setImpedance(20, Delta, centers)
tensions = setTension(20, 1)

coefficients = linear_solution(impedances, tensions)
49/409:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/410: superiorPlot(centers, coefficients, 20)
49/411: inferiorPlot(centers, coefficients, 20)
49/412: superiorPlot(centers, coefficients, 50)
49/413:
Delta = delta(0.1, 100)
centers = setCenters(100, Delta, 0.001)
impedances = setImpedance(100, Delta, centers)
tensions = setTension(100, 1)

coefficients = linear_solution(impedances, tensions)
49/414:
Delta = delta(0.1, 40)
centers = setCenters(40, Delta, 0.001)
impedances = setImpedance(40, Delta, centers)
tensions = setTension(40, 1)

coefficients = linear_solution(impedances, tensions)
49/415:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/416: superiorPlot(centers, coefficients, 40)
49/417:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/418: inferiorPlot(centers, coefficients, 40)
49/419:
%%time
superiorPlot(centers, coefficients, 40)
49/420:
%%time
inferiorPlot(centers, coefficients, 40)
49/421:
Delta = delta(0.1, 20)
centers = setCenters(20, Delta, 0.001)
impedances = setImpedance(20, Delta, centers)
tensions = setTension(20, 1)

coefficients = linear_solution(impedances, tensions)
49/422:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/423:
%%time
superiorPlot(centers, coefficients, 20)
49/424:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/425:
%%time
inferiorPlot(centers, coefficients, 20)
49/426:
Delta = delta(0.1, 40)
centers = setCenters(40, Delta, 0.001)
impedances = setImpedance(40, Delta, centers)
tensions = setTension(40, 1)

coefficients = linear_solution(impedances, tensions)
49/427:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/428:
%%time
superiorPlot(centers, coefficients, 40)
49/429:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
49/430:
%%time
inferiorPlot(centers, coefficients, 40)
49/431:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/432:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
    
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
X, Y = np.meshgrid(X, Y)

# Plot the surface.
surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)

# Customize the z axis.
#ax.set_zlim(-1.01, 1.01)
ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()
49/433:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
    
    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

    # Make data.
    X, Y = np.meshgrid(X, Y)

    # Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)

    # Customize the z axis.
    #ax.set_zlim(-1.01, 1.01)
    ax.zaxis.set_major_locator(LinearLocator(10))
    # A StrMethodFormatter is used automatically
    ax.zaxis.set_major_formatter('{x:.02f}')

    # Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)
    
    plt.show()
49/434:
%%time
superiorPlot(centers, coefficients, 40)
49/435:
import numpy as np
import matplotlib.pyplot as plt
import math
from matplotlib import cm
from matplotlib.ticker import LinearLocator
49/436:
def delta(L, N):
    return L/N
49/437:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([j*Delta - Delta/2, i*Delta - Delta/2, k*d])
                
    
    return centers
49/438:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/439:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []

    for n in range(0, 2*N*N):
        tensions.append(0 if (n < N*N) else V0)
    
    return tensions
49/440:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/441:
Delta = delta(0.1, 40)
centers = setCenters(40, Delta, 0.001)
impedances = setImpedance(40, Delta, centers)
tensions = setTension(40, 1)

coefficients = linear_solution(impedances, tensions)
49/442:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
    
    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

    # Make data.
    X, Y = np.meshgrid(X, Y)

    # Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)

    # Customize the z axis.
    #ax.set_zlim(-1.01, 1.01)
    ax.zaxis.set_major_locator(LinearLocator(10))
    # A StrMethodFormatter is used automatically
    ax.zaxis.set_major_formatter('{x:.02f}')

    # Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)
    
    plt.show()
49/443:
%%time
superiorPlot(centers, coefficients, 40)
49/444:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()
    
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator
import numpy as np

fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
X = np.arange(-5, 5, 0.25)
Y = np.arange(-5, 5, 0.25)
X, Y = np.meshgrid(X, Y)
R = np.sqrt(X**2 + Y**2)
Z = np.sin(R)

# Plot the surface.
surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
ax.set_zlim(-1.01, 1.01)
ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()
49/445:
%%time
superiorPlot(centers, coefficients, 40)
49/446:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
X = np.arange(-5, 5, 0.25)
Y = np.arange(-5, 5, 0.25)
X, Y = np.meshgrid(X, Y)
R = np.sqrt(X**2 + Y**2)
Z = np.sin(R)

# Plot the surface.
surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
ax.set_zlim(-1.01, 1.01)
ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()
49/447:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
#X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
ax.set_zlim(-1.01, 1.01)
ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()
49/448:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
#X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
ax.set_zlim(-1.01, 1.01)
ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()
49/449:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
#X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
    ax.set_zlim(-1.01, 1.01)
    ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
    ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
49/450:
%%time
superiorPlot(centers, coefficients, 40)
49/451:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
    X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
    ax.set_zlim(-1.01, 1.01)
    ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
    ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
49/452:
%%time
superiorPlot(centers, coefficients, 40)
49/453:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
    #X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
    ax.set_zlim(-1.01, 1.01)
    ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
    ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
49/454:
%%time
superiorPlot(centers, coefficients, 40)
49/455:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
    #X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
    #ax.set_zlim(-1.01, 1.01)
    ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
    ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
49/456:
%%time
superiorPlot(centers, coefficients, 40)
49/457:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
    #X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
    #ax.set_zlim(-1.01, 1.01)
    #ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
    ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
49/458:
%%time
superiorPlot(centers, coefficients, 40)
49/459:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
    #X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
    #ax.set_zlim(-1.01, 1.01)
    ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
    ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
49/460:
%%time
superiorPlot(centers, coefficients, 40)
49/461:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
    #X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
    #ax.set_zlim(-1.01, 1.01)
    #ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
    ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
49/462:
%%time
superiorPlot(centers, coefficients, 40)
49/463:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
    #X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
    #ax.set_zlim(-1.01, 1.01)
    #ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
    #ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
49/464:
%%time
superiorPlot(centers, coefficients, 40)
49/465:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_wireframe(X, Y, Z, color='red')
    ax.set_title('Distribuição de carga na placa com z = 0')
    plt.show()

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})

# Make data.
#X = np.arange(-5, 5, 0.25)
#Y = np.arange(-5, 5, 0.25)
    #X, Y = np.meshgrid(X, Y)
#R = np.sqrt(X**2 + Y**2)
#Z = np.sin(R)

# Plot the surface.
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)

# Customize the z axis.
    #ax.set_zlim(-1.01, 1.01)
    #ax.zaxis.set_major_locator(LinearLocator(10))
# A StrMethodFormatter is used automatically
    #ax.zaxis.set_major_formatter('{x:.02f}')

# Add a color bar which maps values to colors.
    #fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()
49/466:
%%time
superiorPlot(centers, coefficients, 40)
49/467:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)
    plt.show()
49/468:
%%time
superiorPlot(centers, coefficients, 40)
49/469:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)
    plt.show()
49/470:
%%time
inferiorPlot(centers, coefficients, 40)
49/471:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.YlGnBu_r, linewidth=0, antialiased=False)
    plt.show()
49/472:
%%time
inferiorPlot(centers, coefficients, 40)
49/473:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/474:
%%time
inferiorPlot(centers, coefficients, 40)
49/475:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/476:
%%time
superiorPlot(centers, coefficients, 40)
49/477:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.gist_earth, linewidth=0, antialiased=True)
    plt.show()
49/478:
%%time
inferiorPlot(centers, coefficients, 40)
49/479:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.PuBu_r, linewidth=0, antialiased=True)
    plt.show()
49/480:
%%time
inferiorPlot(centers, coefficients, 40)
49/481:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.gist_earth, linewidth=0, antialiased=True)
    plt.show()
49/482:
%%time
inferiorPlot(centers, coefficients, 40)
49/483:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.gist_earth, linewidth=0, antialiased=True)
    plt.show()
49/484:
%%time
superiorPlot(centers, coefficients, 40)
49/485:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.copper, linewidth=0, antialiased=True)
    plt.show()
49/486:
%%time
superiorPlot(centers, coefficients, 40)
49/487:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.CMRmap, linewidth=0, antialiased=True)
    plt.show()
49/488:
%%time
superiorPlot(centers, coefficients, 40)
49/489:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.CMRmap, linewidth=0, antialiased=False)
    plt.show()
49/490:
%%time
superiorPlot(centers, coefficients, 40)
49/491:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.CMRmap, linewidth=0, antialiased=True)
    plt.show()
49/492:
%%time
superiorPlot(centers, coefficients, 40)
49/493:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap='PuBu_r', linewidth=0, antialiased=True)
    plt.show()
49/494:
%%time
superiorPlot(centers, coefficients, 40)
49/495:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolmap, linewidth=0, antialiased=True)
    plt.show()
49/496:
%%time
superiorPlot(centers, coefficients, 40)
49/497:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.colormap, linewidth=0, antialiased=True)
    plt.show()
49/498:
%%time
superiorPlot(centers, coefficients, 40)
49/499:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.colorwarm, linewidth=0, antialiased=True)
    plt.show()
49/500:
%%time
superiorPlot(centers, coefficients, 40)
49/501:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/502:
%%time
superiorPlot(centers, coefficients, 40)
49/503:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/504:
%%time
superiorPlot(centers, coefficients, 40)
49/505:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=False)
    plt.show()
49/506:
%%time
superiorPlot(centers, coefficients, 40)
49/507:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/508:
%%time
superiorPlot(centers, coefficients, 40)
49/509:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/510:
%%time
inferiorPlot(centers, coefficients, 40)
49/511:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.gist_earth, linewidth=0, antialiased=True)
    plt.show()
49/512:
%%time
inferiorPlot(centers, coefficients, 40)
49/513:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/514:
%%time
inferiorPlot(centers, coefficients, 40)
49/515:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/516:
%%time
superiorPlot(centers, coefficients, 40)
49/517:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/518:
%%time
inferiorPlot(centers, coefficients, 40)
49/519:
%%time
superiorPlot(centers, coefficients, 40)
49/520:
%%time
inferiorPlot(centers, coefficients, 40)
49/521:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/522:
%%time
superiorPlot(centers, coefficients, 40)
49/523:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/524:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # criação do gráfico
    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('slab charge distribution at z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/525:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('slab charge distribution at z = d')
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/526:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))dd
    
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('Distribuição de carga na placa com z = 0')
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/527:
Delta = delta(0.1, 40)
centers = setCenters(40, Delta, 0.001)
impedances = setImpedance(40, Delta, centers)
tensions = setTension(40, 1)

coefficients = linear_solution(impedances, tensions)
49/528:
Delta = delta(0.1, 12)
centers = setCenters(12, Delta, 0.001)
impedances = setImpedance(12, Delta, centers)
tensions = setTension(12, 1)

coefficients = linear_solution(impedances, tensions)
49/529:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/530:
%%time
inferiorPlot(centers, coefficients, N)
49/531:
%%time
superiorPlot(centers, coefficients, N)
49/532:
%%time
inferiorPlot(centers, coefficients, N)

%%time
superiorPlot(centers, coefficients, N)
49/533:
%%time
inferiorPlot(centers, coefficients, N)
superiorPlot(centers, coefficients, N)
49/534:
%%time
inferiorPlot(centers, coefficients, N)
49/535:
%%time
superiorPlot(centers, coefficients, N)
49/536:
%%time
superiorPlot(centers, coefficients, N)
49/537:
def total_sigma(coefficients):
    return sum(coefficients)
49/538:
sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/539:
sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/540:
plt.style.use('_mpl-gallery')

fig, ax = plt.subplots()

ax.plot(coefficients, linewidth=2.0)

plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/541:
fig, ax = plt.subplots()

ax.plot(coefficients, linewidth=2.0)

plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/542:
fig, ax = plt.subplots()
plt.title('Pulsos', fontsize=20)
plt.xlabel('Elemento', fontsize=20) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('Amplitude', fontsize=20) # Valor esperado
plt.plot(amp, color='blue', linewidth='2')
ax.plot(coefficients, linewidth=2.0)

plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/543:
plt.title('Pulsos', fontsize=20)
plt.xlabel('Elemento', fontsize=20) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('Amplitude', fontsize=20) # Valor esperado
plt.plot(amp, color='blue', linewidth='2')
plt.plot(coefficients, linewidth=2.0)

plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/544:
plt.title('Pulsos', fontsize=20)
plt.xlabel('Elemento', fontsize=20) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('Amplitude', fontsize=20) # Valor esperado
plt.plot(coefficients, linewidth=2.0)

plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/545:
plt.xlabel('Elemento', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('Amplitude', fontsize=20) # Valor esperado
plt.plot(coefficients, linewidth=2.0)

plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/546:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)

plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/547:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/548:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
print("Gráfico da distribuição de cargas para os valores dados e para N = ", N)
49/549:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
print("Gráfico da distribuição de cargas para os valores dados e para N =", N)
49/550:
for N in range(5, 20):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        print("Gráfico da distribuição de cargas para os valores dados e para N =", N)
        inferiorPlot(centers, coefficients, N)
        superiorPlot(centers, coefficients, N)
49/551:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # criação do gráfico
    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('slab charge distribution at z = 0 when N =', N})
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/552:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # criação do gráfico
    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('slab charge distribution at z = 0 when N =', N)
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/553:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('slab charge distribution at z = d when N =', N)
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/554:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/555:
%%time
inferiorPlot(centers, coefficients, N)
49/556:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # criação do gráfico
    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('slab charge distribution at z = 0 when N =' + N)
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/557:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('slab charge distribution at z = d when N =', N)
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/558:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/559:
%%time
inferiorPlot(centers, coefficients, N)
49/560:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # criação do gráfico
    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    surf = ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/561:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))

    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/562:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/563:
%%time
inferiorPlot(centers, coefficients, N)
49/564:
%%time
superiorPlot(centers, coefficients, N)
49/565:
def total_sigma(coefficients):
    # dado que len(coefficients) = 2*N*N
    return sum(coefficients)
49/566:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/567:
for N in range(5, 20):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        print("Gráfico da distribuição de cargas para os valores dados e para N =", N)
        inferiorPlot(centers, coefficients, N)
        superiorPlot(centers, coefficients, N)
49/568:
for N in range(5, 20):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        inferiorPlot(centers, coefficients, N)
        superiorPlot(centers, coefficients, N)
49/569:
def superiorPlot(centers, coefficients, N):
    centers = np.array(centers)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    return [X, Y, Z]
49/570:
def finalPlot(inf, sup):
    inferior_slab = inf
    superior_slab = sup
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = figr.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/571:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/572:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup)
49/573:
def finalPlot(inf, sup):
    inferior_slab = inf
    superior_slab = sup
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/574:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/575:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup)
49/576:
def finalPlot(inf, sup):
    inferior_slab = inf
    superior_slab = sup
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()

    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
49/577:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/578:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup)
49/579:
def finalPlot(inf, sup):
    inferior_slab = inf
    superior_slab = sup
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inf[0], inf[1], inf[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
    
   
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(sup[0], sup[1], sup[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/580:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/581:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup)
49/582:
def finalPlot(inf, sup):
    inferior_slab = inferior
    superior_slab = superior
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inferior[0], inferior[1], inferior[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
    
   
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(superior[0], superior[1], superior[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/583:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/584:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup)
49/585:
def finalPlot(inferior, superior):
    inferior_slab = inferior
    superior_slab = superior
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inferior[0], inferior[1], inferior[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
    
   
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(superior[0], superior[1], superior[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/586:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/587:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup)
49/588:
def finalPlot(inf, sup, N):
    inferior_slab = inferior
    superior_slab = superior
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inf[0], inf[1], inf[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
    
   
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(sup[0], sup[1], sup[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/589:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/590:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup, N)
49/591:
def finalPlot(inf, sup, N):
    inferior_slab = inf
    superior_slab = sup
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inf[0], inf[1], inf[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
    
   
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(sup[0], sup[1], sup[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/592:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/593:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup, N)
49/594:
def finalPlot(inf, sup, N):
    inferior_slab = inf
    superior_slab = sup
    print(inf[0])
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inf[0], inf[1], inf[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
    
   
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(sup[0], sup[1], sup[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/595:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/596:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup, N)
49/597:
def finalPlot(infe, sup, N):
    inferior_slab = infe
    superior_slab = sup
    print(infe[0])
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inf[0], inf[1], inf[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    plt.show()
    
   
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(sup[0], sup[1], sup[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/598:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/599:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup, N)
49/600:
def finalPlot(infe, sup, N):
    inferior_slab = infe
    superior_slab = sup
    print(infe[0])
    
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inf[0], inf[1], inf[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    #plt.show()
    
   
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(sup[0], sup[1], sup[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/601:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/602:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup, N)
49/603:
def finalPlot(infe, sup, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inf[0], inf[1], inf[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(sup[0], sup[1], sup[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/604:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/605:
%%time
inf = inferiorPlot(centers, coefficients, N)
sup = superiorPlot(centers, coefficients, N)
finalPlot(inf, sup, N)
49/606:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(inf[0], inf[1], inf[2], cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(sup[0], sup[1], sup[2], cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/607:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/608:
%%time
finalPlot(inf, sup, N)
49/609:
%%time
finalPlot(centers, coefficients, N)
49/610:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    plt.show()
49/611:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/612:
%%time
finalPlot(centers, coefficients, N)
49/613:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    figr.tight_layout()
    plt.show()
49/614:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/615:
%%time
finalPlot(centers, coefficients, N)
49/616:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    fig.tight_layout()
    plt.show()
49/617:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/618:
%%time
finalPlot(centers, coefficients, N)
49/619:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.show()
49/620:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/621:
%%time
finalPlot(centers, coefficients, N)
49/622:
%%time
finalPlot(centers, coefficients, N)
49/623:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(1.0))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.show()
49/624:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/625:
%%time
finalPlot(centers, coefficients, N)
49/626:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(2.0))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.show()
49/627:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/628:
%%time
finalPlot(centers, coefficients, N)
49/629:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.25))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.show()
49/630:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/631:
%%time
finalPlot(centers, coefficients, N)
49/632:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.35))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.show()
49/633:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/634:
%%time
finalPlot(centers, coefficients, N)
49/635:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.show()
49/636:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/637:
%%time
finalPlot(centers, coefficients, N)
49/638:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.show()
49/639:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/640:
%%time
finalPlot(centers, coefficients, N)
49/641:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.subplots_adjust(hspace=0.35)
    plt.show()
49/642:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/643:
%%time
finalPlot(centers, coefficients, N)
49/644:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.subplots_adjust(hspace=0.55)
    plt.show()
49/645:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/646:
%%time
finalPlot(centers, coefficients, N)
49/647:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.subplots_adjust(hspace=1.0)
    plt.show()
49/648:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/649:
%%time
finalPlot(centers, coefficients, N)
49/650:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.subplots_adjust(hspace=5.0)
    plt.show()
49/651:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/652:
%%time
finalPlot(centers, coefficients, N)
49/653:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
49/654:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/655:
%%time
finalPlot(centers, coefficients, N)
49/656:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    ax = fig.add_subplot(1, 2, 1, projection='3d', constrained_layout=True)
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 2, projection='3d', constrained_layout=True)
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
49/657:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/658:
%%time
finalPlot(centers, coefficients, N)
49/659:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5), constrained_layout=True)
    
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)
    
    #fig.tight_layout()
    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
49/660:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/661:
%%time
finalPlot(centers, coefficients, N)
49/662:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.5), constrained_layout=True)
    
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)

    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
49/663:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/664:
%%time
finalPlot(centers, coefficients, N)
49/665:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.25), constrained_layout=True)
    
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)

    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
49/666:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/667:
%%time
finalPlot(centers, coefficients, N)
49/668:
def finalPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.25))
    
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)

    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
49/669:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/670:
%%time
finalPlot(centers, coefficients, N)
49/671:
def total_sigma(coefficients):
    # dado que len(coefficients) = 2*N*N
    return sum(coefficients)
49/672:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/673:
for N in range(5, 20):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        inferiorPlot(centers, coefficients, N)
        superiorPlot(centers, coefficients, N)
49/674:
def inferior_superior_plot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.25))
    
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)

    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
49/675:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/676:
%%time
inferior_superior_plot(centers, coefficients, N)
49/677:
def total_sigma(coefficients):
    # dado que len(coefficients) = 2*N*N
    return sum(coefficients)
49/678:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma)
49/679:
for N in range(5, 20):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        inferior_superior_plot(centers, coefficients, N)
49/680:
for N in range(5, 26):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        inferior_superior_plot(centers, coefficients, N)
49/681:
for N in range(5, 46):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        inferior_superior_plot(centers, coefficients, N)
49/682:
for N in range(5, 81):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        inferior_superior_plot(centers, coefficients, N)
49/683: No entanto, analiticamente, a capacitância para esse contexto pode ser definida de acordo com:
49/684:
capacitances = []

for N in range(5, 81):
    if N % 5 == 0:
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)
        coefficients = linear_solution(impedances, tensions)
        
        capacitance_z0 = (Delta * Delta)*(sum(coefficients[:N*N]))/V0
        capacitance_zd = (Delta * Delta)*(sum(coefficients[N*N:2N*N]))/V0
        
        capacitances.append([capacitance_z0, capacitance_zd])
49/685:
capacitances = []

for N in range(5, 81):
    if N % 5 == 0:
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)
        coefficients = linear_solution(impedances, tensions)
        
        capacitance_z0 = (Delta * Delta)*(sum(coefficients[:N*N]))/V0
        capacitance_zd = (Delta * Delta)*(sum(coefficients[N*N:2*N*N]))/V0
        
        capacitances.append([capacitance_z0, capacitance_zd])
49/686:
#plt.plot(np.array(range(5, 80, 5)), np.array(cap)-e0*lado**2/d)
plt.plot(np.array(range(5, 80, 5)), np.zeros(len(range(5, 80, 5))))
plt.show()
49/687:
capacitances = []

for N in range(5, 81):
    if N % 5 == 0:
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)
        coefficients = linear_solution(impedances, tensions)
        
        capacitance_z0 = (Delta * Delta)*(sum(coefficients[:N*N]))/V0
        capacitance_zd = (Delta * Delta)*(sum(coefficients[N*N:2*N*N]))/V0
        
        capacitances.append([capacitance_z0, capacitance_zd])
49/688:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
49/689: #####
49/690:
capacitances = []

for N in range(5, 81):
    if N % 5 == 0:
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)
        coefficients = linear_solution(impedances, tensions)
        
        capacitance_zd = (Delta * Delta)*(sum(coefficients[N*N:2*N*N]))/V0
        
        capacitances.append(capacitance_zd)
49/691:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
49/692:
plt.plot(np.array(range(5, 80, 5)), np.array(capacitances)-analytical_capacitance)
plt.plot(np.array(range(5, 80, 5)), np.zeros(len(range(5, 80, 5))))
plt.show()
49/693:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print(len(capacitances))
49/694:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print
print(len(capacitances))
49/695:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print(analytical_capacitance)
print(len(capacitances))
49/696:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print(analytical_capacitance)
print(len(capacitances))
capacitances.pop(len(capacitances)-1)
print(len(capacitances))
49/697:
plt.plot(np.array(range(5, 80, 5)), np.array(capacitances)-analytical_capacitance)
plt.plot(np.array(range(5, 80, 5)), np.zeros(len(range(5, 80, 5))))
plt.show()
49/698:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print(analytical_capacitance)
print(len(capacitances))
49/699:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print("capacitância")
49/700:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print("capacitância analítica =", analytical_capacitance)
49/701:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print("capacitância analítica =", analytical_capacitance, "f")
49/702:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print("capacitância analítica =", analytical_capacitance, "F")
49/703:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma, 'C/m²')
49/704:
import numpy as np
import matplotlib.pyplot as plt
import math
from matplotlib import cm
from matplotlib.ticker import LinearLocator
49/705:
def delta(L, N):
    return L/N
49/706:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([j*Delta - Delta/2, i*Delta - Delta/2, k*d])
                
    
    return centers
49/707:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/708:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []

    for n in range(0, 2*N*N):
        tensions.append(0 if (n < N*N) else V0)
    
    return tensions
49/709:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/710:
def inferiorPlot(centers, coefficients, N):
    centers = np.array(centers)

    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    return [X, Y, Z]
49/711:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/712:
def inferior_superior_plot(centers, coefficients, N):
    centers = np.array(centers)
    
    # Para a placa inferior, com $z = 0$:
    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.25))
    
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # Para a placa superior, com $z = d$:
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)

    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
49/713:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/714:
%%time
inferior_superior_plot(centers, coefficients, N)
49/715:
def total_sigma(coefficients):
    # dado que len(coefficients) = 2*N*N
    return sum(coefficients)
49/716:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma, 'C/m²')
49/717:
capacitances = []

for N in range(5, 81):
    if N % 5 == 0:
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)
        coefficients = linear_solution(impedances, tensions)
        
        capacitance_zd = (Delta * Delta)*(sum(coefficients[N*N:2*N*N]))/V0
        
        capacitances.append(capacitance_zd)
49/718:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print("capacitância analítica =", analytical_capacitance, "F")
49/719:
plt.plot(np.array(range(5, 85, 5)), np.array(capacitances)-analytical_capacitance)
plt.plot(np.array(range(5, 85, 5)), np.zeros(len(range(5, 80, 5))))
plt.show()
49/720:
plt.plot(np.array(range(5, 85, 5)), np.array(capacitances)-analytical_capacitance)
plt.plot(np.array(range(5, 85, 5)), np.zeros(len(range(5, 85, 5))))
plt.show()
49/721:
for N in range(5, 81):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        inferior_superior_plot(centers, coefficients, N)
49/722:
import numpy as np
import matplotlib.pyplot as plt
import math
from matplotlib import cm
from matplotlib.ticker import LinearLocator
49/723:
def delta(L, N):
    return L/N
49/724:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([j*Delta - Delta/2, i*Delta - Delta/2, k*d])
                
    
    return centers
49/725:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
49/726:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []

    for n in range(0, 2*N*N):
        tensions.append(0 if (n < N*N) else V0)
    
    return tensions
49/727:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
49/728:
def inferior_superior_plot(centers, coefficients, N):
    centers = np.array(centers)
    
    # Para a placa inferior, com $z = 0$:
    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.25))
    
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # Para a placa superior, com $z = d$:
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)

    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
49/729:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
49/730:
%%time
inferior_superior_plot(centers, coefficients, N)
49/731:
def total_sigma(coefficients):
    # dado que len(coefficients) = 2*N*N
    return sum(coefficients)
49/732:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma, 'C/m²')
49/733:
for N in range(5, 81):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        inferior_superior_plot(centers, coefficients, N)
49/734:
capacitances = []

for N in range(5, 81):
    if N % 5 == 0:
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)
        coefficients = linear_solution(impedances, tensions)
        
        capacitance_zd = (Delta * Delta)*(sum(coefficients[N*N:2*N*N]))/V0
        
        capacitances.append(capacitance_zd)
54/1:
import numpy as np
import matplotlib.pyplot as plt
import math
from matplotlib import cm
from matplotlib.ticker import LinearLocator
54/2:
def delta(L, N):
    return L/N
54/3:
# "centers" é a matriz com os 2*N*N valores para os centros dos 2*N*N quadrados discretizados
# dado que rn(xi, yj, zk) = (i*delta - delta/2,  j*delta - delta/2, kd)

def setCenters(N, Delta, d): #assumindo Delta = delta(L, N) previamente
    centers = []
    
    for k in range(0, 2):
        for i in range(1, N+1):
            for j in range(1, N+1):
                centers.append([j*Delta - Delta/2, i*Delta - Delta/2, k*d])
                
    
    return centers
54/4:
# "impedances" é a matriz de impedâncias para um certo mn
# é dado que rm = (xp, yq, zs) é o ponto a ser calculado o potencial sobo referencial dado por rn = (xi, yj, zk)

def setImpedance(N, Delta, centers):
    epsilon0 = 8.85e-12
    impedances = []
    
    for m in range(0, 2*N*N):
        impedances.append([])
        for n in range(0, 2*N*N):
            if m == n:
                # zmn = delta/piE0 * ln[1 + raiz(2)]
                zmn = (Delta/(math.pi * epsilon0))*(math.log(1 + math.sqrt(2)))
                impedances[m].append(zmn)
            else:
                # zmn = 1/4piE0 * delta²/raiz[(xp - xi)² + (yq - yj)² + (zs - zk)²]
                zmn = 1/(4 * math.pi * epsilon0) * pow(Delta, 2)/math.sqrt(pow(centers[m][0] - centers[n][0], 2) + pow(centers[m][1] - centers[n][1], 2) + pow(centers[m][2] - centers[n][2], 2))
                impedances[m].append(zmn)
    
    return impedances
54/5:
# "tensions" é a matriz de tensões
# como foi definido previamente, a placa em z = 0 tem potencial nulo, enquanto que a placa em z = d tem potencial V0

def setTension(N, V0):
    tensions = []

    for n in range(0, 2*N*N):
        tensions.append(0 if (n < N*N) else V0)
    
    return tensions
54/6:
def linear_solution(impedances, tensions):
    coefficients = np.linalg.solve(impedances, tensions)
    
    return coefficients
54/7:
def inferior_superior_plot(centers, coefficients, N):
    centers = np.array(centers)
    
    # Para a placa inferior, com $z = 0$:
    # X, Y e Z são setados de acordo com os N*N primeiros quadrados discretizados
    X = centers[:len(centers)//2, 1]
    Y = centers[:len(centers)//2, 0]
    Z = np.array(coefficients[:len(coefficients)//2])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
    
    # Gráfico plotado para a placa em z = 0
    fig = plt.figure(figsize=plt.figaspect(0.25))
    
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    ax.set_title('slab charge distribution at z = 0 when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=plt.cm.YlGnBu_r, linewidth=0, antialiased=True)
    
    # Para a placa superior, com $z = d$:
    # X, Y e Z são setados de acordo com os N*N últimos quadrados discretizados
    X = centers[len(centers)//2:len(centers), 1]
    Y = centers[len(centers)//2:len(centers), 0]
    Z = np.array(coefficients[len(coefficients)//2:len(coefficients)])

    X = np.reshape(X,(N, N))
    Y = np.reshape(Y,(N, N))
    Z = np.reshape(Z,(N, N))
   
    # Gráfico plotado para a placa em z = d
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    ax.set_title('slab charge distribution at z = d when N =' + str(N))
    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=True)

    plt.subplots_adjust(left=0.125,
                    bottom=0.1, 
                    right=0.9, 
                    top=0.9, 
                    wspace=0.2, 
                    hspace=0.35)
    plt.show()
54/8:
# data
L = 0.1
d = 0.001
V0 = 1
N = 12

Delta = delta(L, N)
centers = setCenters(N, Delta, d)
impedances = setImpedance(N, Delta, centers)
tensions = setTension(N, V0)

coefficients = linear_solution(impedances, tensions)
54/9:
%%time
inferior_superior_plot(centers, coefficients, N)
54/10:
def total_sigma(coefficients):
    # dado que len(coefficients) = 2*N*N
    return sum(coefficients)
54/11:
plt.xlabel('elements', fontsize=16) # Elemento = parte da placa dividida pelo segmento (N)
plt.ylabel('amplitude', fontsize=16) # Valor esperado
plt.plot(coefficients, linewidth=2.0)
plt.show()

sigma = total_sigma(coefficients)
print('sigma = ', sigma, 'C/m²')
54/12:
for N in range(5, 81):
    if N % 5 == 0:
        
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)

        coefficients = linear_solution(impedances, tensions)
        
        inferior_superior_plot(centers, coefficients, N)
54/13:
capacitances = []

for N in range(5, 81):
    if N % 5 == 0:
        Delta = delta(L, N)
        centers = setCenters(N, Delta, d)
        impedances = setImpedance(N, Delta, centers)
        tensions = setTension(N, V0)
        coefficients = linear_solution(impedances, tensions)
        
        capacitance_zd = (Delta * Delta)*(sum(coefficients[N*N:2*N*N]))/V0
        
        capacitances.append(capacitance_zd)
54/14:
epsilon0 = 8.85e-12
analytical_capacitance = epsilon0 * L * L / d
print("capacitância analítica =", analytical_capacitance, "F")
54/15:
plt.plot(np.array(range(5, 85, 5)), np.array(capacitances)-analytical_capacitance)
plt.plot(np.array(range(5, 85, 5)), np.zeros(len(range(5, 85, 5))))
plt.show()
58/1: 1+1
59/1: 1+1
60/1: import numpy as np
60/2:
import numpy as np
import pandas as pd
60/3:
df = pd.read_csv(r'data.csv')
print(df)
60/4:
df = pd.read_csv(r'data.csv')
print(df)
60/5:
data = pd.read_csv(r'data.csv')
df = pd.DataFrame(data, columns=['nota-avaliacao', 'iniciacao_cientifica'])
print(df)
60/6:
data = pd.read_csv(r'data.csv')

print(df)
60/7:
data = pd.read_csv(r'data.csv')

print(data)
60/8:
data = pd.read_csv(r'data.csv')
df = pd.DataFrame(data, columns=['nota_avaliacao', 'iniciacao_cientifica'])
print(df)
60/9:
data = pd.read_csv(r'data.csv')
df = pd.DataFrame(data, columns=['nota_avaliacao', 'iniciacao_cientifica'])
X = df.to_numpy()
print(df)
print(X)
60/10:
data = pd.read_csv(r'data.csv')
df = pd.DataFrame(data, columns=['nota_avaliacao', 'iniciacao_cientifica'])
X = df.to_numpy()
print(data)
print(df)
print(X)
60/11:
data = pd.read_csv(r'data.csv')
df = pd.DataFrame(data, columns=['nota_avaliacao', 'nota_ingles', 'nota_dinamica', 'nota_entrevista', 'monitoria', 'iniciacao_cientifica', 'vulnerabilidades_reportadas'])
X = df.to_numpy()
print(data)
print(df)
print(X)
60/12:
data = pd.read_csv(r'data.csv')
df = pd.DataFrame(data, columns=['nota_avaliacao', 'nota_ingles', 'nota_dinamica', 'nota_entrevista', 'monitoria', 'iniciacao_cientifica', 'vulnerabilidades_reportadas'])
X = df.to_numpy()
print(X)
60/13:
data = pd.read_csv(r'data.csv')
x_df = pd.DataFrame(data, columns=['nota_avaliacao', 'nota_ingles', 'nota_dinamica', 'nota_entrevista', 'monitoria', 'iniciacao_cientifica', 'vulnerabilidades_reportadas'])
X = x_df.to_numpy()
y_df = pd.DataFrame(data, columns=['Aprovado'])
Y = y_df.to_numpy()
print(X)
print(Y)
60/14:
data = pd.read_csv(r'data.csv')
x_df = pd.DataFrame(data, columns=['nota_avaliacao', 'nota_ingles', 'nota_dinamica', 'nota_entrevista', 'monitoria', 'iniciacao_cientifica', 'vulnerabilidades_reportadas'])
X = x_df.to_numpy()
y_df = pd.DataFrame(data, columns=['Aprovado'])
y = y_df.to_numpy()
print(X)
print(Y)
60/15:
data = pd.read_csv(r'data.csv')
x_df = pd.DataFrame(data, columns=['nota_avaliacao', 'nota_ingles', 'nota_dinamica', 'nota_entrevista', 'monitoria', 'iniciacao_cientifica', 'vulnerabilidades_reportadas'])
X = x_df.to_numpy()
y_df = pd.DataFrame(data, columns=['Aprovado'])
y = y_df.to_numpy()
print(X)
print(y)
60/16:
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
60/17: model = LogisticRegression()
60/18:
model = LogisticRegression()
model.fit(X, y)
60/19:
data = pd.read_csv(r'data.csv')
x_df = pd.DataFrame(data, columns=['nota_avaliacao', 'nota_ingles', 'nota_dinamica', 'nota_entrevista', 'monitoria', 'iniciacao_cientifica', 'vulnerabilidades_reportadas'])
X = x_df.to_numpy()
y_df = pd.DataFrame(data, columns=['Aprovado'])
y = y_df.to_numpy().ravel()
print(X)
print(y)
60/20:
model = LogisticRegression()
model.fit(X, y)
60/21:
model = LogisticRegression()
model.fit(X, y)
60/22:
model = LogisticRegression()
model.fit(X, y)
60/23:
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
60/24:
data = pd.read_csv(r'data.csv')
x_df = pd.DataFrame(data, columns=['nota_avaliacao', 'nota_ingles', 'nota_dinamica', 'nota_entrevista', 'monitoria', 'iniciacao_cientifica', 'vulnerabilidades_reportadas'])
X = x_df.to_numpy()
y_df = pd.DataFrame(data, columns=['Aprovado'])
y = y_df.to_numpy().ravel()
print(X)
print(y)
60/25:
model = LogisticRegression()
model.fit(X, y)
60/26:
y_pred = model.predict(X)
print("prediction on the training set: ", y_pred)
60/27: print("accuracy: ", model.score(X, y))
60/28:
to_predict = np.array([[8, 9, 7, 9, 1, 0, 0], [6, 10, 8, 8, 0, 0, 1], [8, 9, 8, 7, 0, 0, 0]])
print(to_predict)
60/29:
to_predict = np.array([[8, 9, 7, 9, 1, 0, 0], [6, 10, 8, 8, 0, 0, 1], [8, 9, 8, 7, 0, 0, 0]])
predicted = model.predict(to_predict)

print("prediction on requested array: ", predicted)
60/30:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
print(X_train)
model = LogisticRegression()
model.fit(X, y)
60/31:
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
60/32:
data = pd.read_csv(r'data.csv')
x_df = pd.DataFrame(data, columns=['nota_avaliacao', 'nota_ingles', 'nota_dinamica', 'nota_entrevista', 'monitoria', 'iniciacao_cientifica', 'vulnerabilidades_reportadas'])
X = x_df.to_numpy()
y_df = pd.DataFrame(data, columns=['Aprovado'])
y = y_df.to_numpy().ravel()
print(X)
print(y)
60/33:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
print(X_train)
model = LogisticRegression()
model.fit(X, y)
60/34:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
print(X_train)
print(y_train)
model = LogisticRegression()
model.fit(X, y)
60/35:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
print(X_train)
print(y_train)
model = LogisticRegression()
model.fit(X, y)
60/36:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
print(X_train)
print(y_train)
print(X_test)
print(y_text)
model = LogisticRegression()
model.fit(X, y)
60/37:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
print(X_train)
print(y_train)
print(X_test)
print(y_test)
model = LogisticRegression()
model.fit(X, y)
60/38:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

model = LogisticRegression()
model.fit(X_train, y_train)
60/39:
y_test_pred = model.predict(x_test)
print("prediction on the testing set: ", y_test_pred)
print("dedicated y_test set: ", y_test)
60/40:
y_test_pred = model.predict(X_test)
print("prediction on the testing set: ", y_test_pred)
print("dedicated y_test set: ", y_test)
60/41: print("accuracy: ", model.score(X, y))
60/42:
to_predict = np.array([[8, 9, 7, 9, 1, 0, 0], [6, 10, 8, 8, 0, 0, 1], [8, 9, 8, 7, 0, 0, 0]])
predicted = model.predict(to_predict)

print("prediction on requested array: ", predicted)
60/43:
to_predict = np.array([[8, 9, 7, 9, 1, 0, 0], [6, 10, 8, 8, 0, 0, 1], [8, 9, 8, 7, 0, 0, 0]])
predicted = model.predict(to_predict)

print("prediction on requested array: ", predicted)
60/44:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

model = LogisticRegression()
model.fit(X_train, y_train)
60/45:
y_test_pred = model.predict(X_test)
print("prediction on the testing set: ", y_test_pred)
print("dedicated y_test set: ", y_test)
60/46: print("accuracy: ", model.score(X, y))
60/47:
to_predict = np.array([[8, 9, 7, 9, 1, 0, 0], [6, 10, 8, 8, 0, 0, 1], [8, 9, 8, 7, 0, 0, 0]])
predicted = model.predict(to_predict)

print("prediction on requested array: ", predicted)
62/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn import preprocessing
pd.set_option('display.max_columns', None)
62/2: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
62/3: train_base.shape
62/4: train_base.describe()
62/5: train_base.head(10)
62/6: train_base.shape
62/7:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    # missing.head(10)
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant, once the datasets are masked and some features
    # are difficult to understand and to not subdue them, all were considered essential
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1)

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)
    
    # dropping column ID for further modeling
    cleaned = cleaned.drop(columns='ID', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
62/8: cleaned_train = data_cleaning(train_base)
62/9:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
62/10:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.head(5)
62/11:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype == 'object' or dtype == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
62/12:
categorical_transform(cleaned_train)
cleaned_train.head(10)
62/13:
categorical_transform(cleaned_train)
cleaned_train.head(10)
62/14:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
62/15:
categorical_transform(cleaned_train)
cleaned_train.head(10)
62/16:
## check for dtypes for current dataset cleaned and transformed
cleaned_train.dtypes
## only integers or floats, all numeric, now we can apply training
62/17:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
62/18: X_train, y_train = prepare_data_vectors(cleaned_train)
62/19:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
62/20: X_train = scale_features(X_train)
62/21: y_train.head()
62/22: y_train.head()
62/23: X_train.head(10)
62/24: y_train.value_counts()
62/25:
y_train.value_counts()
print("percentage of 1: ", 69768/(69768+17025))
print("percentage of 0: ", 17025/(69768+17025))
62/26: X_train.head(10)
62/27: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0)
62/28: test_base.head()
62/29:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
62/30:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
62/31:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
y_test.value
62/32:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
y_test.value_counts()
62/33:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
y_test.value_counts()
print("percentage of 1: ", 34369/(34369+8423))
print("percentage of 0: ", 8423/(34369+8423))
62/34:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

total_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
total_good_train.head()
62/35:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

total_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
total_good_train.shape
62/36:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

total_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
total_good_train.value_counts()
62/37:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

total_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
total_good_train.TARGET.?value_counts()
62/38:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

total_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
total_good_train.TARGET.value_counts()
62/39:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0]
bad_train
62/40:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
bad_train
62/41:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_test = pd.concat([even_good_train, bad_train], axis=0)
new_cleaned_test.head()
62/42:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_test = pd.concat([even_good_train, bad_train], axis=0)
new_cleaned_test.TARGET.value_counts()
62/43:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)
new_cleaned_train.TARGET.value_counts()
62/44:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
y_pred = rfc_over.predict_proba(X_test)
y_pred = pd.DataFrame(y_pred)
compare = pd.concat([y_pred, y_test], axis=1)
62/45:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
62/46:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X_test, y_test):
    y_pred = model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred, labels=lr.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
    
    return precision_score(y_test, y_pred)
62/47:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

#X_test, y_test = prepare_data_vectors(new_cleaned_test)
#X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_train, y_train)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/48:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X_test, y_test):
    y_pred = model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred, labels=lr.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
    
    return precision_score(y_test, y_pred)
62/49:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X_test, y_test):
    y_pred = model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
    
    return precision_score(y_test, y_pred)
62/50:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

#X_test, y_test = prepare_data_vectors(new_cleaned_test)
#X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_train, y_train)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/51:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
62/52:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/53:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/54:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/55:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga', C=0.5)
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/56:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga', C=0.3)
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/57:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga', C=0.01)
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/58:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga', C=0.1)
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/59:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga', C=2.0)
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/60:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga', C=3.0)
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/61:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='newton-cholesky')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/62:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
62/63:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegressionCV(max_iter=100, solver='newton-cholesky')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/64:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegressionCV(max_iter=100, solver='newton-cg')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
62/65:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegressionCV(max_iter=100, solver='newton-cg')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sklearn.__version__)
62/66:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
62/67:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegressionCV(max_iter=100, solver='newton-cg')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
62/68:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
62/69:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegressionCV(max_iter=100, solver='newton-cholesky')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
63/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
print(sk.__version__)
63/2:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
print(sk.__version__)
63/3:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
print(sk.__version__)
65/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
print(sk.__version__)
65/2:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegressionCV(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
65/3:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
print(sk.__version__)
65/4: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
66/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
print(sk.__version__)
66/2: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
66/3: train_base.shape
66/4: train_base.describe()
66/5: train_base.head(10)
66/6: train_base.shape
66/7:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    # missing.head(10)
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant, once the datasets are masked and some features
    # are difficult to understand and to not subdue them, all were considered essential
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1)

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)
    
    # dropping column ID for further modeling
    cleaned = cleaned.drop(columns='ID', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
66/8: cleaned_train = data_cleaning(train_base)
66/9:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
66/10:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.head(5)
66/11:
idades = base['IDADE']
plt.hist(idades, color='blue', edgecolor='black')
66/12:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
66/13:
categorical_transform(cleaned_train)
cleaned_train.head(10)
66/14:
## check for dtypes for current dataset cleaned and transformed
cleaned_train.dtypes
## only integers or floats, all numeric, now we can apply training
66/15:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
66/16: X_train, y_train = prepare_data_vectors(cleaned_train)
66/17:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
66/18: X_train = scale_features(X_train)
66/19: y_train.head()
66/20:
y_train.value_counts()
print("percentage of 1: ", 69768/(69768+17025))
print("percentage of 0: ", 17025/(69768+17025))
66/21: X_train.head(10)
66/22:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
66/23:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_pred = rfc.predict(X_train)

cm = confusion_matrix(y_train, y_pred, labels=lr.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr.classes_)
display.plot()
plt.title(type(rfc).__name__)
plt.show()
    
print("Precision score: ", precision_score(y_train, y_pred))
print("Recall score: ", recall_score(y_train, y_pred))
print("F1 score: ", f1_score(y_train, y_pred))
print("Accuracy score: ", accuracy_score(y_train, y_pred))
66/24: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0)
66/25: test_base.head()
66/26:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
66/27:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
y_test.value_counts()
print("percentage of 1: ", 34369/(34369+8423))
print("percentage of 0: ", 8423/(34369+8423))
66/28:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X_test, y_test):
    y_pred = model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
    
    return precision_score(y_test, y_pred)
66/29: plot_confusion_matrix(lr)
66/30: #score = plot_confusion_matrix(rfc)
66/31: #plot_confusion_matrix(dtc)
66/32:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
66/33:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegressionCV(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
66/34:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
66/35: from sklearn.utils.class_weight import compute_class_weight
66/36:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight(‘balanced’, y.unique(), y)
66/37:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', y.unique(), y)
66/38:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', y_train.unique(), y_train)
66/39:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
66/40:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
weights
66/41:
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/42:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
weights
66/43:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
weights
66/44:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
print(weights)
66/45:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
print(weights)


class_weights = {
 0:0.62201152,
 1:2.54898678
}

lr_mc = LogisticRegression(C=3.0, fit_intercept=True, warm_start = True, class_weight=class_weights)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/46:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
print(weights)


class_weights = {
 0:0.62201152,
 1:2.54898678
}

lr_mc = LogisticRegression(max_iter=100, solver='saga', C=3.0, fit_intercept=True, warm_start = True, class_weight=class_weights)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/47:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
print(weights)


class_weights = {
 0:0.62201152,
 1:2.54898678
}

lr_mc = LogisticRegression(max_iter=100, solver='saga', fit_intercept=True, warm_start = True, class_weight=class_weights)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/48:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
print(weights)


class_weights = {
 0:0.62201152,
 1:2.54898678
}

lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight=class_weights)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/49:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
print(weights)


class_weights = {
 0:2.54898678,
 1:0.62201152
}

lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight=class_weights)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/50:
from sklearn.utils.class_weight import compute_class_weight
weights = compute_class_weight('balanced', np.unique(y_train.to_numpy()), y_train.to_numpy())
print(weights)


class_weights = {
 0:2.54898678,
 1:0.62201152
}

lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/51:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/52:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', C=0.5)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/53:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', C=0.1)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/54:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/55: from sklearn.metrics import roc_curve, roc_auc_score
66/56:
from sklearn.metrics import roc_curve, roc_auc_score

y_pred = lr_mc.predict(X_test)

fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)
auc = roc_auc_score(y_test, y_pred)

plt.figure(figsize=(12,6))
plt.plot(fpr, tpr,label='AUC')
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random')
plt.title('AUC:{}'.format(auc))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
66/57:
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/58:
lr_mc = LogisticRegression(max_iter=100, solver='newton-cholesky', class_weight='balanced')
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/59:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/60:
from sklearn.metrics import roc_curve, roc_auc_score

y_pred = lr_mc.predict(X_test)

fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)
auc = roc_auc_score(y_test, y_pred)

plt.figure(figsize=(12,6))
plt.plot(fpr, tpr,label='AUC')
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random')
plt.title('AUC:{}'.format(auc))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
66/61:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
66/62:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
66/63:
from sklearn.metrics import roc_curve, roc_auc_score

y_pred = lr_over.predict(X_test)

fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)
auc = roc_auc_score(y_test, y_pred)

plt.figure(figsize=(12,6))
plt.plot(fpr, tpr,label='AUC')
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random')
plt.title('AUC:{}'.format(auc))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
66/64:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
66/65:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.2)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.2)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
66/66:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.2)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.3)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
66/67:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.2)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.4)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
66/68:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.4)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.4)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
66/69:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.4)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.4)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

lr_mc_over = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc_over.fit(X_train, y_train)
plot_confusion_matrix(lr_mc_over, X_test, y_test)
66/70:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.5)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.5)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

lr_mc_over = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc_over.fit(X_train, y_train)
plot_confusion_matrix(lr_mc_over, X_test, y_test)
66/71:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.6)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.6)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

lr_mc_over = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc_over.fit(X_train, y_train)
plot_confusion_matrix(lr_mc_over, X_test, y_test)
66/72:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.7)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.7)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

lr_mc_over = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc_over.fit(X_train, y_train)
plot_confusion_matrix(lr_mc_over, X_test, y_test)
66/73:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.8)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.8)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

lr_mc_over = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc_over.fit(X_train, y_train)
plot_confusion_matrix(lr_mc_over, X_test, y_test)
66/74:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.75)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.75)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

lr_mc_over = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc_over.fit(X_train, y_train)
plot_confusion_matrix(lr_mc_over, X_test, y_test)
66/75:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.65)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.65)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

lr_mc_over = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc_over.fit(X_train, y_train)
plot_confusion_matrix(lr_mc_over, X_test, y_test)
66/76:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.9)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.9)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

lr_mc_over = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc_over.fit(X_train, y_train)
plot_confusion_matrix(lr_mc_over, X_test, y_test)
66/77:
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/78:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/79: test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
66/80:
test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
test_proba
66/81:
test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
test_proba.isnull().sum()
66/82:
test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
test_proba
66/83:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
for threshold in thresholds:
    print ('\n******** For i = {} ******'.format(i))
    y_pred = test_proba.applymap(lambda x: 1 if x>i else 0)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
66/84:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
for threshold in thresholds:
    print ('\n******** For i = {} ******'.format(i))
    y_pred = test_proba.applymap(lambda x: 1 if x>threshold else 0)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
66/85:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
for threshold in thresholds:
    print ('\n******** For i = {} ******'.format(threshold))
    y_pred = test_proba.applymap(lambda x: 1 if x>threshold else 0)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
66/86:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
for threshold in thresholds:
    print ('\n******** For i = {} ******'.format(threshold))
    y_pred = test_proba.applymap(lambda x: 1 if x>threshold else 0)
    print(y_pred)
66/87:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
for threshold in thresholds:
    print ('\n******** For i = {} ******'.format(threshold))
    y_pred = test_proba.applymap(lambda x: 1 if x>threshold else 0)
    print(y_pred.value_counts())
66/88:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
for threshold in thresholds:
    print ('\n******** For i = {} ******'.format(threshold))
    y_pred = test_proba.applymap(lambda x: 1 if x>threshold else 0)
    y_pred
66/89:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
for threshold in thresholds:
    print ('\n******** For i = {} ******'.format(threshold))
    y_pred = test_proba.applymap(lambda x: 1 if x>threshold else 0)
    print(y_pred.shape)
66/90:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
for threshold in thresholds:
    print ('\n******** For i = {} ******'.format(threshold))
    y_pred = test_proba.applymap(lambda x: 1 if x>threshold else 0)
    print(y_pred.value_counts())
66/91:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/92:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
test_proba = test_proba.drop(columns='0', axis=1)
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/93:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
test_proba = test_proba.drop(columns='0')
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/94:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
test_proba = test_proba.drop(columns='0', axis=1)
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/95:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/96:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
test_proba = test_proba['1']
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/97:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/98:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
test_proba
66/99:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
test_proba.0
66/100:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
test_proba['0?
66/101:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
test_proba['0']
66/102:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
for col in test_proba:
    print(col)
66/103:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
test_proba.get("0")
66/104:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
a = test_proba.get("0")
a
66/105:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
a = test_proba.get("0")
print(a)
66/106:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred = y_pred['0']
66/107:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/108:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred.get("0")?
66/109:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred.get("0")
66/110:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
a =y_pred.get("0")
a
66/111:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred.0
66/112:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred['0']
66/113:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred.loc('0')
66/114:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/115:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred.rename(columns={"0": "zero", "1": "one"}
66/116:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred.rename(columns={"0": "zero", "1": "one"})
66/117:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred = y_pred.rename(columns={"0": "zero", "1": "one"})
66/118:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred = y_pred.rename(columns={"0": "zero", "1": "one"})
y_pred
66/119:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred = y_pred.iloc[:,1].as_matrix().reshape(y_pred.iloc[:,1].as_matrix().size,1)
66/120:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
66/121:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
y_pred
66/122:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
y_pred = pd.DataFrame(y_pred)
66/123:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
y_pred = pd.DataFrame(y_pred)
y_pred
66/124:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred
66/125:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]
y_pred = test_proba.applymap(lambda x: 1 if x>0.45 else 0)
y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
y_pred = pd.DataFrame(y_pred)
y_pred
66/126:
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/127:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/128:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]

for threshold in thresholds:
    print ('\n******** For threshold = {} ******'.format(threshold))
    y_pred = y_test_proba.applymap(lambda x: 1 if x>0.45 else 0)
    y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
    y_pred = pd.DataFrame(y_pred)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
66/129:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]

for threshold in thresholds:
    print ('\n******** For threshold = {} ******'.format(threshold))
    y_pred = y_test_proba.applymap(lambda x: 1 if x>threshold else 0)
    y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
    y_pred = pd.DataFrame(y_pred)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
66/130:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85]

for threshold in thresholds:
    print ('\n******** For threshold = {} ******'.format(threshold))
    y_pred = y_test_proba.applymap(lambda x: 1 if x>threshold else 0)
    y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
    y_pred = pd.DataFrame(y_pred)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
66/131:
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/132:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_train, y_train)
66/133:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced')
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
plot_confusion_matrix(lr_mc, X_train, y_train)
66/134:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
plot_confusion_matrix(lr_mc, X_train, y_train)
66/135:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
plot_confusion_matrix(lr_mc, X_train, y_train)

weight_vector = list(lr_mc.coef_[0])
print(weight_vector)
66/136:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
dist
66/137:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
dist.shape
66/138:
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/139:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
plot_confusion_matrix(lr_mc, X_train, y_train)
66/140:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
dist.shape
66/141:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
X_train.shape
66/142:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
dist.shape
66/143:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
dist
66/144:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]
y_dist
66/145:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]
dist
66/146:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]
y_dist
66/147:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()
66/148:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show(figsize=(8,6))

val = np.percentile(y_dist, 20)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/149:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.figure(figsize=(8.6))
plt.show()

val = np.percentile(y_dist, 20)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/150:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 20)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/151:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 2)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/152:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 3)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/153:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 1)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/154:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 4)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/155:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 5)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/156:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.5)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/157:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.2)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/158:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.3)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/159:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.1)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/160:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.01)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/161:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.001)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/162:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 20)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
66/163:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 20)
print("Threshold Val: ", val)

y_train[(y_dist < val)].value_counts()
print(y_train.shape)
66/164:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 20)
print("Threshold Val: ", val)

print(y_train.shape)

y_train[(y_dist < val)].value_counts()
66/165:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 10)
print("Threshold Val: ", val)

print(y_train.shape)

y_train[(y_dist < val)].value_counts()
66/166:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 30)
print("Threshold Val: ", val)

print(y_train.shape)

y_train[(y_dist < val)].value_counts()
66/167:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 40)
print("Threshold Val: ", val)

print(y_train.shape)

y_train[(y_dist < val)].value_counts()
66/168:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 60)
print("Threshold Val: ", val)

print(y_train.shape)

y_train[(y_dist < val)].value_counts()
66/169:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 70)
print("Threshold Val: ", val)

print(y_train.shape)

y_train[(y_dist < val)].value_counts()
66/170:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 75)
print("Threshold Val: ", val)

print(y_train.shape)

y_train[(y_dist < val)].value_counts()
66/171:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 80)
print("Threshold Val: ", val)

print(y_train.shape)

y_train[(y_dist < val)].value_counts()
66/172:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 80)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]
66/173:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 80)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/174:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 80)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

y_train_new
66/175:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 80)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

y_train_new.value_counts()
66/176:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 20)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

y_train_new.value_counts()
66/177:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 10)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

y_train_new.value_counts()
66/178:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 1)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

y_train_new.value_counts()
66/179:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 10)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

y_train_new.value_counts()
66/180:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 1)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

y_train_new.value_counts()
66/181:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.5)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

y_train_new.value_counts()
66/182:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.3)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

y_train_new.value_counts()
66/183:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.3)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/184:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.5)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/185:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 0.6)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/186:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 1)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/187:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 10)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/188:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 5)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/189:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 4)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/190:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 2)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/191:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 3)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/192:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 2)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/193:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 1)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/194:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 4)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/195:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 3)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train, y_train)
66/196:
# OUTLIERS
weight_vector = list(lr_mc.coef_[0])
print(weight_vector)

dist = np.dot(X_train, weight_vector)
y_dist = dist*[-1 if x==0 else 1 for x in list(y_train)]

import seaborn as sns
import matplotlib.pyplot as plt

sns.kdeplot(y_dist)
plt.xlabel("Distance * Y-class")
plt.grid()
plt.show()

val = np.percentile(y_dist, 3)
print("Threshold Val: ", val)
#print(y_train.shape)

y_train[(y_dist < val)].value_counts()

X_train_new = X_train[(~(y_dist < val))]
y_train_new = y_train[(~(y_dist < val))]

lr_mc2 = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc2.fit(X_train_new, y_train_new)
plot_confusion_matrix(lr_mc2, X_test, y_test)
plot_confusion_matrix(lr_mc2, X_train_new, y_train_new)
66/197:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
66/198: train_base.shape
66/199:

train_base.describe()
66/200: train_base.describe()
66/201: train_base.head()
66/202:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1)

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)
    
    # dropping column ID for further modeling
    cleaned = cleaned.drop(columns='ID', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
66/203: cleaned_train = data_cleaning(train_base)
66/204:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
66/205:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.head(5)
66/206: cleaned_train.tail()
66/207: cleaned_train.tail(1)
66/208:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.head(1)
66/209:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
66/210:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull()
66/211:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()?
66/212:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
66/213:
idades = cleaned_train.IDADE
idades.plot.hist(idades, color='blue', edgecolor='black')
66/214:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
66/215:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar', figsize(8,6))
66/216:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot.bar(figsize(8,6))
66/217:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
66/218: print(cleaned_train.VAR2.value_counts())
66/219:
print(base_train.VAR2.value_counts())
print(cleaned_train.VAR2.value_counts())
66/220:
print(train_base.VAR2.value_counts())
print(cleaned_train.VAR2.value_counts())
66/221:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("/nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
66/222:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("\nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
66/223:
sexos = cleaned_train.VAR2.value_counts()
sexos.plot(kind='bar', x='Sexos da base', y='count')
66/224:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
sexos.plot(kind='bar', x='Sexos da base', y='count')
66/225:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
sexos.plot(kind='bar', x='Sexos da base', y='count', figsize(10,6))
66/226:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
sexos.plot(kind='bar', x='Sexos da base', y='count', figsize=(10,6))
66/227:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
sexos.plot(kind='bar', x='Sexos da base', y='count')
66/228:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
sexos.plot(kind='bar')
66/229:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
xticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', xticks=sticks)
66/230:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
xticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', xticks=xticks)
66/231:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', xticks=xticks)
66/232:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
66/233: cleaned_train.VAR2.value_counts().sum()
66/234:
total_mulheres = cleaned_train[cleaned_train.VAR2 == 'F'].value_counts()
total_mulheres
66/235:
mulheres = cleaned_train[cleaned_train.VAR2 == 'F']
mulheres
66/236:
mulheres = cleaned_train[cleaned_train.VAR2 == 'F']
total_mulheres = mulheres.VAR2.value_counts()
total_mulheres
66/237:
mulheres = cleaned_train[cleaned_train.VAR2 == 'F']
total_mulheres = mulheres.VAR2.value_counts().sum()
total_mulheres
66/238:
mulheres = cleaned_train[cleaned_train.VAR2 == 'F']
total_mulheres = mulheres.VAR2.value_counts().sum()
percentual_mulheres = total_mulheres / cleaned_train.VAR2.value_counts().sum()
66/239:
mulheres = cleaned_train[cleaned_train.VAR2 == 'F']
total_mulheres = mulheres.VAR2.value_counts().sum()
percentual_mulheres = total_mulheres / cleaned_train.VAR2.value_counts().sum()
percentual_mulheres
66/240:
mulheres = cleaned_train[cleaned_train.VAR2 == 'F']
total_mulheres = mulheres.VAR2.value_counts().sum()
percentual_mulheres = total_mulheres / cleaned_train.VAR2.value_counts().sum()
print("As mulheres representam" + str(round(percentual_mulheres, 1)) + "% do total da base")
66/241:
mulheres = cleaned_train[cleaned_train.VAR2 == 'F']
total_mulheres = mulheres.VAR2.value_counts().sum()
percentual_mulheres = total_mulheres / cleaned_train.VAR2.value_counts().sum()
print("As mulheres representam " + str(round(percentual_mulheres, 2)) + "% do total da base")
66/242:
mulheres = cleaned_train[cleaned_train.VAR2 == 'F']
total_mulheres = mulheres.VAR2.value_counts().sum()
percentual_mulheres = total_mulheres / cleaned_train.VAR2.value_counts().sum()
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 2)) + "% do total da base")
66/243:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
66/244:
def calcula_percentual_categorias(feature, categoria):
    categoria = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = mulheres[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
66/245: percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
66/246:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
percentual_paulistas
66/247:
def calcula_percentual_categorias(feature, categoria):
    categoria_df = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = categoria_df[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
66/248:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
percentual_paulistas
66/249:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)) + "% do total da base")
66/250:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)*100) + "% do total da base")
66/251:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
66/252:
percentual_mulheres = calcula_percentual_categorias('VAR2', 'F')
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 3)*100) + "% do total da base")
66/253:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
66/254:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.plot.scatter(x='VAR5', y='IDADE', figsize=(10,6))
66/255:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
66/256:
data = pd.concat([cleaned_train.IDADE, cleaned_train.REF_DATE], axis=1)
data.plot(kind='line', x='REF_DATE', y='IDADE', figsize=(8,6))
66/257:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
66/258:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned_train.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
66/259:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MALES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MALES'] = data['MALES'].cumsum()
print("total of males: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='MALES', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'FEMALES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['FEMALES'] = data['FEMALES'].cumsum()
print("total of females: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='FEMALES', figsize=(10,6), ax=ax)
66/260:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'HOMENS'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['HOMENS'] = data['HOMENS'].cumsum()
print("total de homens: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='HOMENS', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MULHERES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MULHERES'] = data['MULHERES'].cumsum()
print("total de mulheres: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='MULHERES', figsize=(10,6), ax=ax)
66/261:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))
66/262:
data = pd.concat([cleaned.IDADE, cleaned.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
66/263:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
66/264:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
data.describe()
66/265:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

mean_age = cleaned_train.IDADE.describe()
66/266:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

cleaned_train.IDADE.describe()
66/267:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
66/268:
data = pd.concat([cleaned_train.VAR5, cleaned_train.VAR141], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
66/269:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

print("Descrição da feature idade")
cleaned_train.IDADE.describe()
66/270:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
66/271:
categorical_transform(cleaned_train)
cleaned_train.head(10)
66/272:
categorical_transform(cleaned_train)
cleaned_train.head()
66/273:
## check for dtypes for current dataset cleaned and transformed
cleaned_train.dtypes
## only integers or floats, all numeric, now we can apply training
66/274:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
66/275:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
66/276: X_train, y_train = prepare_data_vectors(cleaned_train)
66/277:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
66/278: X_train = scale_features(X_train)
66/279: y_train.head()
66/280: y_train.value_counts()
66/281: X_train.head()
66/282:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
66/283:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X_test, y_test):
    y_pred = model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
66/284:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
66/285:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
66/286:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("")
66/287:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
66/288:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
66/289:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
66/290: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0)
66/291: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
66/292: test_base.head()
66/293:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
66/294:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
y_test.value_counts()
66/295: y_test.value_counts()
66/296: X_test.head()
66/297: plot_confusion_matrix(lr, X_test, y_test)
66/298: plot_confusion_matrix(rfc, X_test, y_test)
66/299: plot_confusion_matrix(dtc, X_test, y_test)
66/300:
plot_confusion_matrix(lr, X_test, y_test)
plot_confusion_matrix(rfc, X_test, y_test)
plot_confusion_matrix(dtc, X_test, y_test)
66/301:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
66/302:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
new_cleaned_test.TARGET.value_counts()
66/303:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
66/304: new_cleaned_test.TARGET.value_counts()
66/305: new_cleaned_train.TARGET.value_counts()
66/306: new_cleaned_test.TARGET.value_counts()
66/307:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
66/308:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.40)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.40)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
66/309: new_cleaned_train.TARGET.value_counts()
66/310: new_cleaned_test.TARGET.value_counts()
66/311:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
66/312:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.35)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.35)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
66/313: new_cleaned_train.TARGET.value_counts()
66/314: new_cleaned_test.TARGET.value_counts()
66/315:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
66/316:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.20)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.20)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
66/317: new_cleaned_train.TARGET.value_counts()
66/318: new_cleaned_test.TARGET.value_counts()
66/319:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
66/320: new_cleaned_train.TARGET.value_counts()
66/321: new_cleaned_test.TARGET.value_counts()
66/322:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
#y_pred = rfc_over.predict_proba(X_test)
#y_pred = pd.DataFrame(y_pred)
#compare = pd.concat([y_pred, y_test], axis=1)
print(sk.__version__)
66/323:
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV, StratifiedKFold

X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
66/324:
from sklearn.metrics import roc_curve, roc_auc_score

y_pred = lr_over.predict(X_test)

fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)
auc = roc_auc_score(y_test, y_pred)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr,label='AUC')
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random')
plt.title('AUC:{}'.format(auc))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
66/325:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/326:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
66/327:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
66/328: new_cleaned_train.TARGET.value_counts()
66/329: new_cleaned_test.TARGET.value_counts()
66/330:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
66/331:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/332:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/333:
plot_confusion_matrix(lr_mc, X_test, y_test)
plot_confusion_matrix(lr_mc, X_train, y_train)
66/334:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/335:
rfc_over = RandomForestClassifier(n_jobs=-1, class_weight='balanced')
rfc_over.fit(X_train, y_train)
66/336:
rfc_mc = RandomForestClassifier(n_jobs=-1, class_weight='balanced')
rfc_mc.fit(X_train, y_train)
66/337: plot_confusion_matrix(rfc_mc, X_test, y_test)
66/338:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/339:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/340:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/341:
rfc_mc = RandomForestClassifier(n_jobs=-1, class_weight='balanced')
rfc_mc.fit(X_train, y_train)
66/342: plot_confusion_matrix(rfc_mc, X_test, y_test)
66/343:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)
new_cleaned_test
## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/344:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
new_cleaned_test.head()
66/345:
## dropando features categóricas a fim de address overfitting
overfitting_features += ['VAR6', 'VAR9', 'VAR10', 'VAR39', 'VAR40', 'VAR41', 'VAR147']

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

new_cleaned_test.head()
66/346:
## dropando features categóricas a fim de address overfitting
overfitting_features += ['VAR6', 'VAR9', 'VAR10', 'VAR39', 'VAR40', 'VAR41', 'VAR147']

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

X_test.head()
66/347:
## dropando features categóricas a fim de address overfitting
overfitting_features += ['VAR6', 'VAR9', 'VAR10', 'VAR39', 'VAR40', 'VAR41', 'VAR147']

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

X_test.head()
66/348:
## dropando features categóricas a fim de address overfitting
overfitting_features += ['VAR6', 'VAR9', 'VAR10', 'VAR39', 'VAR40', 'VAR41', 'VAR147']

new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

X_test.head()
66/349:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/350:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/351:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

X_test.head()
66/352:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/353:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/354:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/355:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/356: score = precision_score(y_test, lr_mc.predict(X_test))
66/357:
score = precision_score(y_test, lr_mc.predict(X_test))
score
66/358: as_is = test_base[test_base.IDADE > 28.0]
66/359:
as_is = test_base[test_base.IDADE > 28.0]
as_is.head()
66/360:
as_is = test_base[test_base.IDADE > 28.0]
as_is.shape
66/361:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
66/362: mas_pagadoras = as_is[as_is.TARGET == 0]
66/363: mas_pagadoras = as_is[as_is.TARGET == 0]
66/364:
mas_pagadoras = as_is[as_is.TARGET == 0]
mas_pagadoras.head()
66/365:
mas_pagadoras = as_is[as_is.TARGET == 0]
mas_pagadoras.shape
66/366:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.head()
66/367:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
66/368: divida_total = mas_pagadoras.shape[0] * 1000
66/369:
divida_total = mas_pagadoras.shape[0] * 1000
divida_total
66/370:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado
66/371:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 3) * 100
66/372:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 3) * 100
percentual_negado
66/373:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 3)
percentual_negado
66/374:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 3)
percentual_negado *= 100
percentual_negado
66/375:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 4)
percentual_negado *= 100
percentual_negado
66/376:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 3) * 100
66/377:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 3) * 100
percentual_negado
66/378:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 2) * 100
percentual_negado
66/379:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 3) * 100
percentual_negado
66/380:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado = round(percentual_negado, 3) * 100
round(percentual_negado, 3)
66/381:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado *= 100
round(percentual_negado, 3)
66/382:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado *= 100
round(percentual_negado, 1)
66/383:
for col in train_base:
    q75,q25 = np.percentile(train_base.loc[:,x],[75,25])
    intr_qr = q75-q25
    print(intr_qr)
66/384:
for col in train_base:
    q75,q25 = np.percentile(train_base.loc[:,col],[75,25])
    intr_qr = q75-q25
    print(intr_qr)
66/385:
for col in train_base:
    q75,q25 = np.percentile(train_base.loc[:,col],[75,25])
    q75
66/386:
for col in train_base:
    q75,q25 = np.percentile(train_base.loc[:,col],[75,25])
    print(q75)
66/387: train_base.describe()
66/388: train_base.IDADE.describe()
66/389: train_base.IDADE.describe()[0]
66/390: train_base.IDADE.describe()[1]
66/391: train_base.IDADE.describe()
66/392: train_base.IDADE.describe()[4]
66/393: train_base.IDADE.describe()
66/394:
train_base.IDADE.describe()[4]
for col in train_base:
    if not (col.dtype.name == 'object' or col.dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train
        q75,q25 = np.percentile(train_base.loc[:,col],[75,25])
        print(q75)
66/395:
train_base.IDADE.describe()[4]
for col in train_base:
    if not (col.dtype.name == 'object' or col.dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        print(intr_qr)
66/396:
train_base.IDADE.describe()[4]
for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        print(intr_qr)
66/397:
train_base.IDADE.describe()[4]
for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
66/398:
train_base.IDADE.describe()[4]
for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
        print(max)
66/399:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan

after = train_base.isnull().sum()
compare = pd.concat([before, after])
compare
66/400:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan

after = train_base.isnull().sum()
compare = pd.concat([before, after])
compare
66/401:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan

after = train_base.isnull().sum()
compare = pd.concat([before, after])
compare
66/402:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan

after = train_base.isnull().sum()
compare = pd.concat([before, after])
compare
66/403:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan

after = train_base.isnull().sum()
compare = pd.concat([before, after])
compare
66/404:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan

after = train_base.isnull().sum()
compare = pd.concat([before, after])
compare
66/405:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan

after = train_base.isnull().sum()
compare = pd.concat([before, after])
compare
66/406:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)
outlier.head()
66/407:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)
outlier.head()
66/408:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)
outlier.head()
66/409:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

X_train, y_train = prepare_data_vectors(outlier)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)

lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/410:
before = train_base.isnull().sum()

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier
66/411:
before = train_base.isnull().sum()

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier
66/412:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier
66/413:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier
66/414:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier
66/415:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

X_train, y_train = prepare_data_vectors(outlier)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)

lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/416:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

X_train, y_train = prepare_data_vectors(outlier)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)

lr_mc.fit(X_train, y_train)
66/417: plot_confusion_matrix(lr_mc, X_test, y_test)
66/418:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier
66/419:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
66/420: cleaned_test
66/421: cleaned_train
66/422: cleaned_test
66/423:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier_test = cleaned_test.drop(columns=['VAR42'])

outlier
66/424: outlier_test
66/425:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier_test = cleaned_test.drop(columns=['VAR42'])

X_train, y_train = prepare_data_vectors(outlier)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(outlier_test)
X_test = scale_features(X_test)
X_train
66/426:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier_test = cleaned_test.drop(columns=['VAR42'])

X_train, y_train = prepare_data_vectors(outlier)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(outlier_test)
X_test = scale_features(X_test)
X_test
66/427:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier_test = cleaned_test.drop(columns=['VAR42'])

X_train, y_train = prepare_data_vectors(outlier)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(outlier_test)
X_test = scale_features(X_test)

lr_mc.fit(X_train, y_train)
66/428: plot_confusion_matrix(lr_mc, X_test, y_test)
66/429:
lr_mc = LogisticRegressionCV(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/430:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

X_test.head()
66/431:
lr_mc = LogisticRegressionCV(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/432:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/433:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/434:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/435:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train
new_cleaned_test = cleaned_test

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

X_test.head()
66/436:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/437:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/438:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

X_test.head()
66/439:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/440:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/441:
score = precision_score(y_test, lr_mc.predict(X_test))
score
66/442: score
66/443: round(score, 3) * 100
66/444: round(score, 3) * 100 + '%'
66/445: str(round(score, 3) * 100) + '%'
66/446: score
66/447:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

X_test.head()
66/448:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
66/449:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
66/450:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
66/451:
y_pred_proba = lr_mc.predict_proba(X_test, y_test)
y_pred_proba
66/452:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba
66/453:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba
66/454:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba
66/455:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba
66/456:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba
66/457:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)
66/458:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba
66/459:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1)

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
66/460: cleaned_train = data_cleaning(train_base)
66/461:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
66/462:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
66/463: cleaned_train.head(1)
66/464: cleaned_train.tail(1)
66/465:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
66/466:
categorical_transform(cleaned_train)
cleaned_train.head()
66/467:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
66/468:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
66/469: X_train, y_train = prepare_data_vectors(cleaned_train)
66/470:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
66/471: X_train = scale_features(X_train)
66/472: y_train.value_counts()
66/473: X_train.head()
66/474:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

## saga solver was pointed out as a good fit for a large dataset
lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
66/475:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
66/476:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
66/477: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
66/478: test_base.head()
66/479:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
66/480:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
66/481: y_test.value_counts()
66/482: X_test.head()
66/483:
plot_confusion_matrix(lr, X_test, y_test)
plot_confusion_matrix(rfc, X_test, y_test)
plot_confusion_matrix(dtc, X_test, y_test)
67/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
67/2: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
67/3: train_base.shape
67/4: train_base.describe()
67/5: train_base.head()
67/6:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1)

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
67/7: cleaned_train = data_cleaning(train_base)
67/8:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
67/9:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
67/10: cleaned_train.head(1)
67/11: cleaned_train.tail(1)
67/12:
idades = cleaned_train.IDADE
idades.plot.hist(idades, color='blue', edgecolor='black')
67/13:
def calcula_percentual_categorias(feature, categoria):
    categoria_df = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = categoria_df[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
67/14:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)*100) + "% do total da base")
67/15:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
67/16:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("\nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
67/17:
percentual_mulheres = calcula_percentual_categorias('VAR2', 'F')
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 3)*100) + "% do total da base")
67/18:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
67/19:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
67/20:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned_train.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
67/21:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'HOMENS'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['HOMENS'] = data['HOMENS'].cumsum()
print("total de homens: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='HOMENS', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MULHERES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MULHERES'] = data['MULHERES'].cumsum()
print("total de mulheres: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='MULHERES', figsize=(10,6), ax=ax)
67/22:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

print("Descrição da feature idade")
cleaned_train.IDADE.describe()
67/23:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
67/24:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
67/25:
categorical_transform(cleaned_train)
cleaned_train.head()
67/26:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
67/27:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
67/28: X_train, y_train = prepare_data_vectors(cleaned_train)
67/29:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
67/30: X_train = scale_features(X_train)
67/31: y_train.value_counts()
67/32: X_train.head()
67/33:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

## saga solver was pointed out as a good fit for a large dataset
lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
67/34:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
67/35:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
67/36: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
67/37: test_base.head()
67/38:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
67/39:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
67/40: y_test.value_counts()
67/41: X_test.head()
67/42:
plot_confusion_matrix(lr, X_test, y_test)
plot_confusion_matrix(rfc, X_test, y_test)
plot_confusion_matrix(dtc, X_test, y_test)
67/43:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
67/44: new_cleaned_train.TARGET.value_counts()
67/45: new_cleaned_test.TARGET.value_counts()
67/46:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
67/47:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
67/48:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
67/49:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
67/50:
score = precision_score(y_test, lr_mc.predict(X_test))
score
67/51:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85]

for threshold in thresholds:
    print ('\n******** For threshold = {} ******'.format(threshold))
    y_pred = y_test_proba.applymap(lambda x: 1 if x>threshold else 0)
    y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
    y_pred = pd.DataFrame(y_pred)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
67/52: score
67/53:
## tamanho credito = total emprestado aprovado pela política
## divida total = total aprovado referente aos mau pagadores
67/54: ## cross-validation set??
67/55:
as_is = test_base[test_base.IDADE > 28.0]
as_is.shape
67/56:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
67/57:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
67/58:
divida_total = mas_pagadoras.shape[0] * 1000
divida_total
67/59:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado *= 100
round(percentual_negado, 1)
67/60:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)
67/61:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier_test = cleaned_test.drop(columns=['VAR42'])

X_train, y_train = prepare_data_vectors(outlier)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(outlier_test)
X_test = scale_features(X_test)

lr_mc.fit(X_train, y_train)
67/62: plot_confusion_matrix(lr_mc, X_test, y_test)
67/63:
as_is = cleaned_test[cleaned_test.IDADE > 28.0]
as_is.shape
67/64:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
67/65:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
67/66:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
67/67:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
67/68:
divida_total = mas_pagadoras.shape[0] * 1000
divida_total
67/69:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado *= 100
round(percentual_negado, 1)
67/70:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

cleaned_test = cleaned_test.drop(columns=['TARGET'])
cleaned_test
67/71:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

cleaned_test = cleaned_test.drop(columns=['TARGET'])
67/72:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

cleaned_test
67/73:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# cleaned_test = cleaned_test.drop(columns=['TARGET'])
cleaned_test['TARGET'] = y_pred_proba
67/74:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# cleaned_test = cleaned_test.drop(columns=['TARGET'])
cleaned_test['TARGET'] = y_pred_proba
cleaned_test
67/75:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# cleaned_test = cleaned_test.drop(columns=['TARGET'])
cleaned_test['TARGET'] = y_pred_proba
cleaned_test[cleaned_test.TARGET < 0.307]
67/76:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# recreating target column
cleaned_test['SCORE'] = y_pred_proba
67/77:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]
percentual_negado *= 100

## este será o ponto de corte da política TO_BE
round(percentual_negado, 1)
67/78:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 1)
67/79:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 3)
67/80:
percentual_negado = (test_base.shape[0] - as_is.shape[0]) / test_base.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 5)
67/81: to_be = cleaned_test[cleaned_test.SCORE > 0.30689]
67/82:
to_be = cleaned_test[cleaned_test.SCORE > 0.30689]
to_be.shape
67/83:
to_be = cleaned_test[cleaned_test.SCORE > 1-score]
to_be.shape
67/84:
percentual_negado = (cleaned_test.shape[0] - as_is.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 5)
67/85:
as_is = cleaned_test[cleaned_test.IDADE > 28.0]
as_is.shape
67/86:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
67/87:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
67/88:
percentual_negado = (cleaned_test.shape[0] - as_is.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 5)
67/89:
to_be_negado = 0.19321

to_be_shape = cleaned_
67/90:
to_be_negado = 0.19321

to_be_shape = cleaned_test.shape[0] - (cleaned_test.shape[0] * to_be_negado)
67/91:
to_be_negado = 0.19321

to_be_shape = cleaned_test.shape[0] - (cleaned_test.shape[0] * to_be_negado)
to_be_shape
68/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
68/2: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
68/3: train_base.shape
68/4: train_base.describe()
68/5: train_base.head()
68/6:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1)

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
68/7: cleaned_train = data_cleaning(train_base)
68/8:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
68/9:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
68/10: cleaned_train.head(1)
68/11: cleaned_train.tail(1)
68/12:
idades = cleaned_train.IDADE
idades.plot.hist(idades, color='blue', edgecolor='black')
68/13:
def calcula_percentual_categorias(feature, categoria):
    categoria_df = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = categoria_df[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
68/14:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)*100) + "% do total da base")
68/15:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
68/16:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("\nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
68/17:
percentual_mulheres = calcula_percentual_categorias('VAR2', 'F')
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 3)*100) + "% do total da base")
68/18:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
68/19:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
68/20:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned_train.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
68/21:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'HOMENS'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['HOMENS'] = data['HOMENS'].cumsum()
print("total de homens: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='HOMENS', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MULHERES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MULHERES'] = data['MULHERES'].cumsum()
print("total de mulheres: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='MULHERES', figsize=(10,6), ax=ax)
68/22:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

print("Descrição da feature idade")
cleaned_train.IDADE.describe()
68/23:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
68/24:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
68/25:
categorical_transform(cleaned_train)
cleaned_train.head()
68/26:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
68/27:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
68/28: X_train, y_train = prepare_data_vectors(cleaned_train)
68/29:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
68/30: X_train = scale_features(X_train)
68/31: y_train.value_counts()
68/32: X_train.head()
68/33:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

## saga solver was pointed out as a good fit for a large dataset
lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
68/34:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
68/35:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
68/36: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
68/37: test_base.head()
68/38:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
68/39:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
68/40: y_test.value_counts()
68/41: X_test.head()
68/42:
plot_confusion_matrix(lr, X_test, y_test)
plot_confusion_matrix(rfc, X_test, y_test)
plot_confusion_matrix(dtc, X_test, y_test)
68/43:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
68/44: new_cleaned_train.TARGET.value_counts()
68/45: new_cleaned_test.TARGET.value_counts()
68/46:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
68/47:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
68/48:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
68/49:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
68/50:
score = precision_score(y_test, lr_mc.predict(X_test))
score
68/51:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85]

for threshold in thresholds:
    print ('\n******** For threshold = {} ******'.format(threshold))
    y_pred = y_test_proba.applymap(lambda x: 1 if x>threshold else 0)
    y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
    y_pred = pd.DataFrame(y_pred)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
68/52: score
68/53:
## tamanho credito = total emprestado aprovado pela política
## divida total = total aprovado referente aos mau pagadores
68/54: ## cross-validation set??
68/55:
as_is = cleaned_test[cleaned_test.IDADE > 28.0]
as_is.shape
68/56:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
68/57:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
68/58:
divida_total = mas_pagadoras.shape[0] * 1000
divida_total
68/59:
percentual_negado = (cleaned_test.shape[0] - as_is.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 5)
68/60:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# recreating target column
cleaned_test['SCORE'] = y_pred_proba
68/61:
to_be_negado = 0.19321

to_be_shape = cleaned_test.shape[0] - (cleaned_test.shape[0] * to_be_negado)
to_be_shape
68/62:
train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)

for col in train_base:
    if col == 'TARGET':
        continue
    dtype = train_base[col].dtypes
    if not (dtype.name == 'object' or dtype.name == 'category'):
        q25 = train_base[col].describe()[4]
        q75 = train_base[col].describe()[6]
        intr_qr = q75-q25
        
        max = q75+(1.5*intr_qr)
        min = q25-(1.5*intr_qr)
                
        train_base.loc[train_base[col] < min, col] = np.nan
        train_base.loc[train_base[col] > max, col] = np.nan


outlier = data_cleaning(train_base)
outlier = sort_ref_date(outlier)
categorical_transform(outlier)

outlier_test = cleaned_test.drop(columns=['VAR42'])

X_train, y_train = prepare_data_vectors(outlier)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(outlier_test)
X_test = scale_features(X_test)

lr_mc.fit(X_train, y_train)
68/63: plot_confusion_matrix(lr_mc, X_test, y_test)
68/64: cleaned_test[cleaned_test.SCORE > 0.3]
68/65: cleaned_test[cleaned_test.SCORE > 0.307]
68/66: cleaned_test[cleaned_test.SCORE > 0.31]
68/67: cleaned_test[cleaned_test.SCORE > 0.35]
68/68: cleaned_test[cleaned_test.SCORE > 0.34]
68/69: cleaned_test[cleaned_test.SCORE > 0.33]
68/70: cleaned_test[cleaned_test.SCORE > 0.335]
68/71: cleaned_test[cleaned_test.SCORE > 0.33]
68/72: cleaned_test[cleaned_test.SCORE > 0.335]
68/73: cleaned_test[cleaned_test.SCORE > 0.32]
68/74: cleaned_test[cleaned_test.SCORE > 0.325]
68/75: cleaned_test[cleaned_test.SCORE > 0.3245]
68/76: cleaned_test[cleaned_test.SCORE > 0.324]
68/77: cleaned_test[cleaned_test.SCORE > 0.323]
68/78: cleaned_test[cleaned_test.SCORE > 0.324]
68/79: cleaned_test[cleaned_test.SCORE > 0.326]
68/80: cleaned_test[cleaned_test.SCORE > 0.327]
68/81: cleaned_test[cleaned_test.SCORE > 0.3272]
68/82: cleaned_test[cleaned_test.SCORE > 0.3275]
68/83: cleaned_test[cleaned_test.SCORE > 0.327]
68/84: cleaned_test[cleaned_test.SCORE > 0.3268]
68/85: cleaned_test[cleaned_test.SCORE > 0.3269]
68/86: cleaned_test[cleaned_test.SCORE > 0.3267]
68/87: cleaned_test[cleaned_test.SCORE > 0.3266]
68/88: cleaned_test[cleaned_test.SCORE > 0.3265]
68/89: cleaned_test[cleaned_test.SCORE > 0.3266]
68/90: cleaned_test[cleaned_test.SCORE > 0.3267]
68/91: cleaned_test[cleaned_test.SCORE > 0.3266]
68/92: cleaned_test[cleaned_test.SCORE > 0.3265]
68/93: cleaned_test[cleaned_test.SCORE > 0.3266]
68/94:
to_be = cleaned_test[cleaned_test.SCORE > 0.3266]
to_be
68/95:
to_be = cleaned_test[cleaned_test.SCORE > 0.3266]
to_be.shape
68/96:
total_emprestado_tobe = to_be.shape[0] * 1000
total_emprestado_tobe
68/97:
mas_pagadoras_tobe = to_be[to_be.TARGET == 0]
print(mas_pagadoras_tobe.shape)
mas_pagadoras_tobe.TARGET.value_counts()
68/98:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado_tobe, 5)
68/99:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado_tobe, 3)
68/100:
percentual_negado = (cleaned_test.shape[0] - as_is.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 3)
68/101:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# recreating target column
cleaned_test['SCORE'] = y_pred_proba
cleaned_test.head()
68/102:
to_be_negado = 0.19321

to_be_shape = cleaned_test.shape[0] - (cleaned_test.shape[0] * to_be_negado)

# quantidade de empréstimos aprovado no cleaned test set
to_be_shape
68/103:
divida_total = mas_pagadoras.shape[0] * 1000
divida_total
68/104:
divida_total = mas_pagadoras_tobe.shape[0] * 1000
divida_total
68/105:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado_tobe, 3)
69/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
69/2: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
69/3: train_base.shape
69/4: train_base.describe()
69/5: train_base.head()
69/6:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1)

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
69/7: cleaned_train = data_cleaning(train_base)
69/8:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
69/9:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
69/10: cleaned_train.head(1)
69/11: cleaned_train.tail(1)
69/12:
idades = cleaned_train.IDADE
idades.plot.hist(idades, color='blue', edgecolor='black')
69/13:
def calcula_percentual_categorias(feature, categoria):
    categoria_df = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = categoria_df[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
69/14:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)*100) + "% do total da base")
69/15:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
69/16:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("\nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
69/17:
percentual_mulheres = calcula_percentual_categorias('VAR2', 'F')
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 3)*100) + "% do total da base")
69/18:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
69/19:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
69/20:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned_train.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
69/21:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'HOMENS'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['HOMENS'] = data['HOMENS'].cumsum()
print("total de homens: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='HOMENS', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MULHERES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MULHERES'] = data['MULHERES'].cumsum()
print("total de mulheres: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='MULHERES', figsize=(10,6), ax=ax)
69/22:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

print("Descrição da feature idade")
cleaned_train.IDADE.describe()
69/23:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
69/24:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
69/25:
categorical_transform(cleaned_train)
cleaned_train.head()
69/26:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
69/27:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
69/28: X_train, y_train = prepare_data_vectors(cleaned_train)
69/29:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
69/30: X_train = scale_features(X_train)
69/31: y_train.value_counts()
69/32: X_train.head()
69/33:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

## saga solver was pointed out as a good fit for a large dataset
lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
69/34:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
69/35:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
69/36: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
69/37: test_base.head()
69/38:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
69/39:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
69/40: y_test.value_counts()
69/41: X_test.head()
69/42:
plot_confusion_matrix(lr, X_test, y_test)
plot_confusion_matrix(rfc, X_test, y_test)
plot_confusion_matrix(dtc, X_test, y_test)
69/43:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
69/44: new_cleaned_train.TARGET.value_counts()
69/45: new_cleaned_test.TARGET.value_counts()
69/46:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
69/47:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
69/48:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
69/49:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
69/50:
score = precision_score(y_test, lr_mc.predict(X_test))
score
69/51:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85]

for threshold in thresholds:
    print ('\n******** For threshold = {} ******'.format(threshold))
    y_pred = y_test_proba.applymap(lambda x: 1 if x>threshold else 0)
    y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
    y_pred = pd.DataFrame(y_pred)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
69/52: score
69/53:
## tamanho credito = total emprestado aprovado pela política
## divida total = total aprovado referente aos mau pagadores
69/54: ## cross-validation set??
69/55:
as_is = cleaned_test[cleaned_test.IDADE > 28.0]
as_is.shape
69/56:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
69/57:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
69/58:
divida_total = mas_pagadoras.shape[0] * 1000
divida_total
69/59:
percentual_negado = (cleaned_test.shape[0] - as_is.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 3)
69/60:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# recreating target column
cleaned_test['SCORE'] = y_pred_proba
cleaned_test.head()
69/61:
to_be_negado = 0.19321

to_be_shape = cleaned_test.shape[0] - (cleaned_test.shape[0] * to_be_negado)

# quantidade de empréstimos aprovado no cleaned test set
to_be_shape
69/62:
to_be = cleaned_test[cleaned_test.SCORE > 0.3266]
to_be.shape
69/63:
total_emprestado_tobe = to_be.shape[0] * 1000
total_emprestado_tobe
69/64:
mas_pagadoras_tobe = to_be[to_be.TARGET == 0]
print(mas_pagadoras_tobe.shape)
mas_pagadoras_tobe.TARGET.value_counts()
69/65:
divida_total = mas_pagadoras_tobe.shape[0] * 1000
divida_total
69/66:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado_tobe, 3)
69/67:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado_tobe, 3)
print(round(percentual_negado, 3) == round(percentual_negado_tobe, 3))
69/68:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
print(round(percentual_negado, 3) == round(percentual_negado_tobe, 3))
round("Percentual de solicitações negadas: ", percentual_negado_tobe, 3)
69/69:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
print(round(percentual_negado, 3) == round(percentual_negado_tobe, 3))
print("Percentual de solicitações negadas: ", + round(percentual_negado_tobe, 3))
71/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
71/2: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
71/3: train_base.shape
71/4: train_base.describe()
71/5: train_base.head()
71/6:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1)

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
71/7: cleaned_train = data_cleaning(train_base)
71/8:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
71/9:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
71/10: cleaned_train.head(1)
71/11: cleaned_train.tail(1)
71/12:
idades = cleaned_train.IDADE
idades.plot.hist(idades, color='blue', edgecolor='black')
71/13:
def calcula_percentual_categorias(feature, categoria):
    categoria_df = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = categoria_df[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
71/14:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)*100) + "% do total da base")
71/15:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
71/16:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("\nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
71/17:
percentual_mulheres = calcula_percentual_categorias('VAR2', 'F')
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 3)*100) + "% do total da base")
71/18:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
71/19:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
71/20:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned_train.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
71/21:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'HOMENS'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['HOMENS'] = data['HOMENS'].cumsum()
print("total de homens: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='HOMENS', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MULHERES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MULHERES'] = data['MULHERES'].cumsum()
print("total de mulheres: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='MULHERES', figsize=(10,6), ax=ax)
71/22:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

print("Descrição da feature idade")
cleaned_train.IDADE.describe()
71/23:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
71/24:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
71/25:
categorical_transform(cleaned_train)
cleaned_train.head()
71/26:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
71/27:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
71/28: X_train, y_train = prepare_data_vectors(cleaned_train)
71/29:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
71/30: X_train = scale_features(X_train)
71/31: y_train.value_counts()
71/32: X_train.head()
71/33:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

## saga solver was pointed out as a good fit for a large dataset
lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
71/34:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
71/35:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
71/36: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
71/37: test_base.head()
71/38:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
71/39:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
71/40: y_test.value_counts()
71/41: X_test.head()
71/42:
plot_confusion_matrix(lr, X_test, y_test)
plot_confusion_matrix(rfc, X_test, y_test)
plot_confusion_matrix(dtc, X_test, y_test)
71/43:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
71/44: new_cleaned_train.TARGET.value_counts()
71/45: new_cleaned_test.TARGET.value_counts()
71/46:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
71/47:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
71/48:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
71/49:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
71/50:
score = precision_score(y_test, lr_mc.predict(X_test))
score
71/51:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85]

for threshold in thresholds:
    print ('\n******** For threshold = {} ******'.format(threshold))
    y_pred = y_test_proba.applymap(lambda x: 1 if x>threshold else 0)
    y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
    y_pred = pd.DataFrame(y_pred)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
71/52: score
71/53:
## tamanho credito = total emprestado aprovado pela política
## divida total = total aprovado referente aos mau pagadores
71/54: ## cross-validation set??
71/55:
as_is = cleaned_test[cleaned_test.IDADE > 28.0]
as_is.shape
71/56:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
71/57:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
71/58:
divida_total = mas_pagadoras.shape[0] * 1000
divida_total
71/59:
percentual_negado = (cleaned_test.shape[0] - as_is.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 3)
71/60:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# recreating target column
cleaned_test['SCORE'] = y_pred_proba
cleaned_test.head()
71/61:
to_be_negado = 0.19321

to_be_shape = cleaned_test.shape[0] - (cleaned_test.shape[0] * to_be_negado)

# quantidade de empréstimos aprovado no cleaned test set
to_be_shape
71/62:
to_be = cleaned_test[cleaned_test.SCORE > 0.3266]
to_be.shape
71/63:
total_emprestado_tobe = to_be.shape[0] * 1000
total_emprestado_tobe
71/64:
mas_pagadoras_tobe = to_be[to_be.TARGET == 0]
print(mas_pagadoras_tobe.shape)
mas_pagadoras_tobe.TARGET.value_counts()
71/65:
divida_total = mas_pagadoras_tobe.shape[0] * 1000
divida_total
71/66:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
print(round(percentual_negado, 3) == round(percentual_negado_tobe, 3))
print("Percentual de solicitações negadas: ", + round(percentual_negado_tobe, 3))
75/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
75/2: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
75/3: train_base.shape
75/4: train_base.describe()
75/5: train_base.head()
75/6:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1)

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
75/7: cleaned_train = data_cleaning(train_base)
75/8:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
75/9:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
75/10: cleaned_train.head(1)
75/11: cleaned_train.tail(1)
75/12:
idades = cleaned_train.IDADE
idades.plot.hist(idades, color='blue', edgecolor='black')
75/13:
def calcula_percentual_categorias(feature, categoria):
    categoria_df = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = categoria_df[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
75/14:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)*100) + "% do total da base")
75/15:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
75/16:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("\nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
75/17:
percentual_mulheres = calcula_percentual_categorias('VAR2', 'F')
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 3)*100) + "% do total da base")
75/18:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
75/19:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
75/20:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned_train.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
75/21:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'HOMENS'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['HOMENS'] = data['HOMENS'].cumsum()
print("total de homens: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='HOMENS', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MULHERES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MULHERES'] = data['MULHERES'].cumsum()
print("total de mulheres: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='MULHERES', figsize=(10,6), ax=ax)
75/22:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

print("Descrição da feature idade")
cleaned_train.IDADE.describe()
75/23:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
75/24:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
75/25:
categorical_transform(cleaned_train)
cleaned_train.head()
75/26:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
75/27:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
75/28: X_train, y_train = prepare_data_vectors(cleaned_train)
75/29:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
75/30: X_train = scale_features(X_train)
75/31: y_train.value_counts()
75/32: X_train.head()
75/33:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

## saga solver was pointed out as a good fit for a large dataset
lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
75/34:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
75/35:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
75/36: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
75/37: test_base.head()
75/38:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
75/39:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
75/40: y_test.value_counts()
75/41: X_test.head()
75/42:
plot_confusion_matrix(lr, X_test, y_test)
plot_confusion_matrix(rfc, X_test, y_test)
plot_confusion_matrix(dtc, X_test, y_test)
75/43:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
75/44: new_cleaned_train.TARGET.value_counts()
75/45: new_cleaned_test.TARGET.value_counts()
75/46:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
75/47:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
75/48:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
75/49:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
75/50:
score = precision_score(y_test, lr_mc.predict(X_test))
score
75/51:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85]

for threshold in thresholds:
    print ('\n******** For threshold = {} ******'.format(threshold))
    y_pred = y_test_proba.applymap(lambda x: 1 if x>threshold else 0)
    y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
    y_pred = pd.DataFrame(y_pred)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
75/52: score
75/53:
## tamanho credito = total emprestado aprovado pela política
## divida total = total aprovado referente aos mau pagadores
75/54: ## cross-validation set??
75/55:
as_is = cleaned_test[cleaned_test.IDADE > 28.0]
as_is.shape
75/56:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
75/57:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
75/58:
divida_total = mas_pagadoras.shape[0] * 1000
divida_total
75/59:
percentual_negado = (cleaned_test.shape[0] - as_is.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 3)
75/60:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# recreating target column
cleaned_test['SCORE'] = y_pred_proba
cleaned_test.head()
75/61:
to_be_negado = 0.19321

to_be_shape = cleaned_test.shape[0] - (cleaned_test.shape[0] * to_be_negado)

# quantidade de empréstimos aprovado no cleaned test set
to_be_shape
75/62:
to_be = cleaned_test[cleaned_test.SCORE > 0.3266]
to_be.shape
75/63:
total_emprestado_tobe = to_be.shape[0] * 1000
total_emprestado_tobe
75/64:
mas_pagadoras_tobe = to_be[to_be.TARGET == 0]
print(mas_pagadoras_tobe.shape)
mas_pagadoras_tobe.TARGET.value_counts()
75/65:
divida_total = mas_pagadoras_tobe.shape[0] * 1000
divida_total
75/66:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
print(round(percentual_negado, 3) == round(percentual_negado_tobe, 3))
print("Percentual de solicitações negadas: ", + round(percentual_negado_tobe, 3))
77/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
77/2: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
77/3: train_base.shape
77/4: train_base.head()
77/5: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
77/6:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
print(round(percentual_negado, 3) == round(percentual_negado_tobe, 3))
print("Percentual de solicitações negadas: ", round(percentual_negado_tobe, 3))
78/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
print(sk.__version__)
pd.set_option('display.max_columns', None)
78/2:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
79/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
79/2:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
print(sk.__version__)
pd.set_option('display.max_columns', None)
79/3:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
80/1:
import pandas as pd 
import random as rnd
import numpy as np 
import matplotlib.pyplot as plt
80/2: gdf = pd.read_csv('nba_games_stats.csv')
80/3: gdf.columns
80/4: gdf.head()
80/5:
gswdf = gdf[gdf.Team == 'GSW']
cldf = gdf[gdf.Team == 'CLE']
80/6: gdf.columns
80/7:
gswdf = gdf[gdf.Team == 'GSW']
cldf = gdf[gdf.Team == 'CLE']
80/8:
gswdf.Date = gswdf.Date.apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d', errors='ignore'))
gswdf = gswdf[gswdf['Date'] > pd.to_datetime('20171001', format='%Y%m%d', errors='ignore')]

cldf.Date = cldf.Date.apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d', errors='ignore'))
cldf = cldf[cldf['Date'] > pd.to_datetime('20171001', format='%Y%m%d', errors='ignore')]
80/9: gswdf.Date
80/10: gswdf.head()
80/11: gdf.head()
80/12: gdf.columns
80/13: gswdf.head()
80/14:
gswdf.TeamPoints.hist()
cldf.TeamPoints.hist()
80/15:
gswdf.OpponentPoints.hist()
cldf.OpponentPoints.hist()
80/16:
gswmeanpts = gswdf.TeamPoints.mean()
clmeanpts = cldf.TeamPoints.mean()
gswsdpts = gswdf.TeamPoints.std()
clsdpts = cldf.TeamPoints.std()

gswmeanopp = gswdf.OpponentPoints.mean()
clmeanopp = cldf.OpponentPoints.mean()
gswsdopp = gswdf.OpponentPoints.std()
clsdopp = cldf.OpponentPoints.std()

print("Golden State Points Mean ", gswmeanpts)
print("Golden State Points SD ", gswsdpts)
print("Cleveland Points Mean ", clmeanpts)
print("Cleveland Points SD ", clsdpts)

print("Golden State OppPoints Mean ", gswmeanopp)
print("Golden State OppPoints SD ", gswsdopp)
print("Cleveland OppPoints Mean ", clmeanopp)
print("Cleveland OppPoints SD ", clsdopp)
80/17:
def gameSim():
    GSWScore = (rnd.gauss(gswmeanpts,gswsdpts)+ rnd.gauss(clmeanopp,clsdopp))/2
    CLScore = (rnd.gauss(clmeanpts,clsdpts)+ rnd.gauss(gswmeanopp,gswsdopp))/2
    if int(round(GSWScore)) > int(round(CLScore)):
        return 1
    elif int(round(GSWScore)) < int(round(CLScore)):
        return -1
    else: return 0
80/18:
def gamesSim(ns):
    gamesout = []
    team1win = 0
    team2win = 0
    tie = 0
    for i in range(ns):
        gm = gameSim()
        gamesout.append(gm)
        if gm == 1:
            team1win +=1 
        elif gm == -1:
            team2win +=1
        else: tie +=1 
    print('GSW Win ', team1win/(team1win+team2win+tie),'%')
    print('CLE Win ', team2win/(team1win+team2win+tie),'%')
    print('Tie ', tie/(team1win+team2win+tie), '%')
    return gamesout
80/19: gamesSim(10000)
81/1:
# -*- coding: utf-8 -*-
"""
Created on Mon Jul 15 08:27:42 2019

@author: KJee
"""

import pandas as pd 

data = pd.read_csv('craigslistVehicles.csv')
data.columns

data.describe()

#remove duplcates 
data.drop_duplicates(inplace= True)

#check for nulls / % of nulls 

data.isnull().any()
data.isnull().sum()/ data.shape[0]

#remove columns with certain threshold of nulls
#threshold is the number of columns or rows without nulls 
thresh = len(data)*.6
data.dropna(thresh = thresh, axis = 1)
data.dropna(thresh = 21, axis = 0)

#imputing nulls fillna()
data.odometer.fillna(data.odometer.median())
data.odometer.fillna(data.odometer.mean())

#everything lower or uppercase
data.desc.head()
data.desc.head().apply(lambda x: x.lower())
data.desc.head().apply(lambda x: x.upper())

#use regex .extract
#use strip()
#use replace()
#split 

data.cylinders.dtype
data.cylinders.value_counts()
data.cylinders = data.cylinders.apply(lambda x: str(x).replace('cylinders','').strip())
data.cylinders.value_counts()

#change data type 
data.cylinders = pd.to_numeric(data.cylinders, errors = 'coerce')


#boxplot 
data.boxplot('price')
data.boxplot('odometer')

#outlier detection and normalization remove rows with > 99% / z score 
numeric = data._get_numeric_data()

# with no null values 
from scipy import stats
import numpy as np 

data_outliers = data[(data.price < data.price.quantile(.995)) & (data.price > data.price.quantile(.005))]

data_outliers.boxplot('price')

#remove duplcates, subset, keep, etc.
data.drop_duplicates()

#histogram
data_outliers.price.hist()

#types of normalization 
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(data.cylinders.values.reshape(-1,1))
scaler.transform(data.cylinders.values.reshape(-1,1))
84/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
print(sk.__version__)
pd.set_option('display.max_columns', None)
86/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
print(sk.__version__)
pd.set_option('display.max_columns', None)
90/1: import pandas as pd
90/2: data = pd.read_csv("/dataset/matches.csv")?
90/3: data = pd.read_csv("/dataset/matches.csv")
90/4: data = pd.read_csv("../dataset/matches.csv", header=0)
90/5: data.head()
90/6:
import pandas as pd
pd.set_option('display.max_columns', None)
90/7: data = pd.read_csv("../dataset/matches.csv", header=0)
90/8: data.head()
90/9: argentina = data.loc[:, (data.team1 == 'ARGENTINA' || data.team2 == 'ARGENTINA')]
90/10: argentina = data.loc[:, data.team1 == 'ARGENTINA' || data.team2 == 'ARGENTINA']
90/11: argentina = data[data.team1 == 'ARGENTINA' || data.team2 == 'ARGENTINA']
90/12: argentina = data[data.team1 == 'ARGENTINA']
90/13:
argentina = data[data.team1 == 'ARGENTINA']
argentina.head()
90/14: argentina = data[data.team1 == 'ARGENTINA' or data.team2 == 'ARGENTINA']
90/15: argentina = data[data.team1 == 'ARGENTINA' || data.team2 == 'ARGENTINA']
90/16: argentina = data[data.team1 == 'ARGENTINA' | data.team2 == 'ARGENTINA']
90/17: argentina = data[(data.team1 == 'ARGENTINA') || (data.team2 == 'ARGENTINA')]
90/18: argentina = data[(data.team2 == 'ARGENTINA')]
90/19: argentina.head()
90/20: argentina = data[(data.team2 == 'ARGENTINA') || data.team1 == 'ARGENTINA']
90/21: argentina = data[(data.team2 == 'ARGENTINA') || (data.team1 == 'ARGENTINA')]
90/22: argentina = data[(data['team1'] == 'ARGENTINA') || (data.['team2'] == 'ARGENTINA')]
90/23: argentina = data.loc[['team1', 'team2']
90/24: argentina = data.loc[['team1', 'team2']]
90/25: argentina = data.loc[data.team1 == 'ARGENTINA']
90/26: argentina.head()
90/27: argentina.shape
90/28: argentina.head()
90/29: argentina = data.loc[data.team1 == 'ARGENTINA' || data.team2 == 'ARGENTINA']
90/30: argentina = data.loc[data.team1 == 'ARGENTINA' | data.team2 == 'ARGENTINA']
90/31: argentina = data.loc[data.team1 == 'ARGENTINA' or data.team2 == 'ARGENTINA']
90/32: argentina = (data.team1 == 'ARGENTINA') || (data.team2 == 'ARGENTINA')
90/33: argentina = (data.team1 == 'ARGENTINA') | (data.team2 == 'ARGENTINA')
90/34: argentina.head()
90/35: argentina = (data.team1 == 'ARGENTINA') or (data.team2 == 'ARGENTINA')
90/36: argentina = (data.team1 == 'ARGENTINA') || (data.team2 == 'ARGENTINA')
90/37: argentina_home = data.loc[:, data.team1 == 'ARGENTINA']?
90/38: argentina_home = data.loc[:, data.team1 == 'ARGENTINA']
90/39: argentina_home = data.loc[data.team1 == 'ARGENTINA']
90/40: argentina.head()
90/41: argentina_home.head()
90/42:
argentina_home = data.loc[data.team1 == 'ARGENTINA']
argentina_visitant = data.loc[data.team2 == 'ARGENTINA']
90/43: argentina_visitant
90/44:
argentina_home = data.loc[data.team1 == 'ARGENTINA']
argentina_visitant = data.loc[data.team2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/45: argentina.head(7)
90/46: argentina.shape
90/47: argentina.head(7)
90/48: argentina.columns
90/49:
def replace_spaces(s):
    return s.replace(" ", "_")
90/50:
a = "completed defensive line breaks team1"
a = replace_spaces(a)
a
90/51:
import pandas as pd
pd.set_option('display.max_columns', None)
90/52: data = pd.read_csv("../dataset/matches.csv", header=0)
90/53: data = pd.read_csv("../dataset/matches.csv", header=0)
90/54: data.head()
90/55:
argentina_home = data.loc[data.team1 == 'ARGENTINA']
argentina_visitant = data.loc[data.team2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/56: argentina.head(7)
90/57: argentina.columns
90/58:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col)
new_columuns
90/59:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col)
new_columns
90/60: data = pd.read_csv("../dataset/matches.csv", header=0)
90/61: data.head()
90/62:
argentina_home = data.loc[data.team1 == 'ARGENTINA']
argentina_visitant = data.loc[data.team2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/63: argentina.head(7)
90/64: argentina.columns
90/65:
def replace_spaces(s):
    return s.replace(" ", "_")
90/66:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col)
new_columns
90/67: data.colums
90/68: data.columns
90/69:
argentina_home = data.loc[data.team1 == 'ARGENTINA']
argentina_visitant = data.loc[data.team2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/70: argentina.head(7)
90/71: argentina.columns
90/72:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col)
new_columns
90/73:
data.columns
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col)
90/74: data.columns
90/75:
for col in data.columns:
    print(col)
90/76:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col)
90/77: new_columns
90/78:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col)
    
data.rename(columns=new_columns)
90/79:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col)
    
data.rename(columns=new_columns)
data.str.upper()
90/80:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns)
90/81: data.head(2)
90/82:
import pandas as pd
pd.set_option('display.max_columns', None)
90/83: data = pd.read_csv("../dataset/matches.csv", header=0)
90/84: data.head(2)
90/85:
def replace_spaces(s):
    return s.replace(" ", "_")
90/86:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns)
90/87:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data = data.rename(columns=new_columns)
90/88:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns)
90/89: data.head()
90/90:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns).head()
90/91:
argentina_home = data.loc[data.team1 == 'ARGENTINA']
argentina_visitant = data.loc[data.team2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/92:
argentina_home = data.loc[data.TEAM1 == 'ARGENTINA']
argentina_visitant = data.loc[data.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/93: argentina.head(7)
90/94: argentina.columns
90/95: argentina.sort_values(by='DATA', ascending=True)
90/96: argentina.sort_values(by='DATE', ascending=True)
90/97: argentina.sort_values(by='DATE', ascending=False)
90/98: argentina.DATE
90/99: data.replace({'nov.': '11', 'DEC': '12'})
90/100: data.DATE = data.DATE.str.replace('nov.', '11')
90/101: data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
90/102:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data
90/103:
data.DATE = data.DATE.str.replace(['nov.', 'DEC'], ['11', '12'], regex=False)
data
90/104:
data.DATE = data.DATE.str.replace(['nov.', 'DEC'], ['11', '12'], regex=False)
data
90/105:
data.DATE = data.DATE.str.replace(['nov.', 'DEC'], ['11', '12'])
data
90/106:
data.DATE = data.DATE.str.replace(['nov.'], ['11'], regex=False)
data.DATE = data.DATE.str.replace(['DEC'], ['12'], regex=False)
90/107:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
90/108:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data
90/109:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
90/110:
argentina_home = data.loc[data.TEAM1 == 'ARGENTINA']
argentina_visitant = data.loc[data.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/111: argentina.head(7)
90/112: argentina.sort_values(by='DATE', ascending=True)
90/113: argentina.sort_values(by='DATE', ascending=False)
90/114:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = pd.to_datetime(data.DATE, format='%Y%m%d')
90/115:
data.DATE = data.DATE.str.replace(' nov. ', '11', regex=False)
data.DATE = data.DATE.str.replace(' DEC ', '12', regex=False)
data.DATE = pd.to_datetime(data.DATE, format='%Y%m%d')
90/116:
data.DATE = data.DATE.str.replace(' nov. ', '11', regex=False)
data.DATE = data.DATE.str.replace(' DEC ', '12', regex=False)
90/117:
data.DATE = data.DATE.str.replace(' nov. ', '11', regex=False)
data.DATE = data.DATE.str.replace(' DEC ', '12', regex=False)
data
90/118:
data.DATE = data.DATE.str.replace(' nov. ', '11', regex=False)
data.DATE = data.DATE.str.replace(' DEC ', '12', regex=False)
data.DATE
90/119:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
90/120:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
data
90/121:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
data.DATE = pd.to_datetime(data.DATE, format='%Y%m%d')
90/122:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
data.DATE = pd.to_datetime(data.DATE, format='%d%m%Y')
90/123:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
data.DATE = pd.to_datetime(data.DATE, format='%d%m%Y')
data
90/124:
data.DATE = pd.to_datetime(data.DATE, format='%d%m%Y')
data
90/125:
data.DATE = pd.to_datetime(data.DATE, format='%d%m%Y')
data.sort_values(by='DATE', ascending=True)
90/126: data.DATE = pd.to_datetime(data.DATE, format='%d%m%Y')
90/127:
argentina_home = data.loc[data.TEAM1 == 'ARGENTINA']
argentina_visitant = data.loc[data.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/128: argentina.sort_values(by='DATE', ascending=False)
90/129: argentina.sort_values(by='DATE', ascending=True)
90/130: argentina.DATE
90/131: argentina.index
90/132:
argentina.POSSESSION_TEAM1.hist()
argentina.POSSESSION_TEAM2.hist()
90/133:
argentina.POSSESSION_TEAM1.line()
argentina.POSSESSION_TEAM2.line()
90/134:
argentina.POSSESSION_TEAM1.bar()
argentina.POSSESSION_TEAM2.bar()
90/135:
argentina.POSSESSION_TEAM1.hist()
argentina.POSSESSION_TEAM2.hist()
90/136:
argentina.POSSESSION_TEAM1.scatter()
argentina.POSSESSION_TEAM2.scatter()
90/137:
argentina.POSSESSION_TEAM1.plot.scatter()
argentina.POSSESSION_TEAM2.plot.scatter()
90/138: argentina.plot.scatter(x='POSSESSION_TEAM1', y='POSSESSION_TEAM2')
90/139: argentina.POSSESSION_TEAM1.value_counts()
90/140: argentina.POSSESSION_TEAM1.value_counts().hist()
90/141:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
argentina_home
90/142:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/143:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.dtypes
90/144:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.astype(float)
90/145:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.str.replace('%', '')
90/146:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home
90/147:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
90/148:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.str.replace('%', '')
90/149:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home
90/150:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home.POSSESSION_TEAM1
90/151:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
90/152:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.[:, 'POSSESSION_TEAM1'] = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
90/153:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.loc[:, 'POSSESSION_TEAM1'] = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
90/154:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
90/155:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home
90/156:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.loc[:, 'POSSESSION_TEAM1'] = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home
90/157:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home
90/158:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '') / 100
argentina_home
90/159:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(float)
argentina_home
90/160:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(float) / 100
argentina_home
90/161:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/162:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.loc[:, 'POSSESSION_TEAM1'].str.replace('%', '').astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/163:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home.POSSESSION_TEAM1 = mask.astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/164:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/165:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
90/166:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home.POSSESSION_TEAM1 = mask.astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/167:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home.POSSESSION_TEAM1 = mask.astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/168:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.str.replace('%', '')
argentina_home.POSSESSION_TEAM1 = mask.astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/169:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home.POSSESSION_TEAM1 = mask.astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/170:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home.['POSSESSION_TEAM1'] = mask.astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/171:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home['POSSESSION_TEAM1'] = mask.astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/172:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home.POSSESSION_TEAM1 = mask.astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/173:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/174:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina.POSSESSION_TEAM1.replace('%', '')
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/175:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina.POSSESSION_TEAM1.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/176:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina.POSSESSION_TEAM1.replace('%', '')
argentina_home
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/177:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina.POSSESSION_TEAM1.str.replace('%', '')
argentina_home
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/178:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina.POSSESSION_TEAM1.str.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/179:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina.POSSESSION_TEAM1.str.replace('%', '').index
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/180:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina.POSSESSION_TEAM1.str.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/181:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/182:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.str.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/183:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/184:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '')

#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/185:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1[:2]
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/186:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1[:2]
argentina_home
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/187:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1[:1]
argentina_home
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/188:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1[0:2]
argentina_home
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/189:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/190:
argentina_home = argentina.loc[data.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/191:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA']
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/192:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA']
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/193:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA']
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').copy()
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/194:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA']
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '').copy()
argentina_home.POSSESSION_TEAM1 = 
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/195:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA']
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '').copy()
argentina_home.POSSESSION_TEAM1 = mask
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/196:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA']
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '').copy()
mask
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/197:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA',:]
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '').copy()
mask
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/198:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '').copy()
argentina_home.POSSESSION_TEAM = mask
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/199:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
mask = argentina_home.POSSESSION_TEAM1.str.replace('%', '').copy()
argentina_home.POSSESSION_TEAM1 = mask
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/200:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/201:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.str.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/202:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/203:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace(r'\D', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/204:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/205:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.loc[:, argentina_home.POSSESSION_TEAM1] = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/206:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.loc[:, argentina_home.POSSESSION_TEAM1] = argentina_home.POSSESSION_TEAM1.str.replace('%', '')
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/207:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.loc[:, argentina_home.POSSESSION_TEAM1] = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(float) / 100
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/208:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.loc[:, argentina_home.POSSESSION_TEAM1] = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(float)
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/209:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(float)
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/210:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(float)
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/211:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(float)
argentina_home
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/212:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/213:
import pandas as pd
pd.set_option('display.max_columns', None)
90/214:
import pandas as pd
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
90/215:
argentina_home = data.loc[data.TEAM1 == 'ARGENTINA']
argentina_visitant = data.loc[data.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/216:
argentina_home = data.loc[data.TEAM1 == 'ARGENTINA']
argentina_visitant = data.loc[data.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
90/217: argentina.sort_values(by='DATE', ascending=True)
90/218:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home
#argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/219:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
90/220:
import pandas as pd
import seaborn as sns
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
90/221: data = pd.read_csv("../dataset/matches.csv", header=0)
90/222:
import pandas as pd
import seaborn as sns
sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
90/223:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home.plot(kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/224:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/225:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=10)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/226:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=6)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/227:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=2)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/228:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=1)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/229:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=.5)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/230:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=.5, pallete='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/231:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=.5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/232:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=.5, palette='light')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/233:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=.5, palette='white')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/234:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=.5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/235:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', height=.5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/236:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', height=6, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/237:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', height=5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/238:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', height=10, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/239:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', height=6, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/240:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', height=5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/241:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=.3, height=5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/242:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=5, height=5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/243:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=.5, height=5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/244:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', width=.5, height=6, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/245:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=.5, height=6, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/246:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1, height=6, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/247:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1, height=5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/248:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=2, height=5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/249:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.5, height=5, palette='dark')
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/250:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.5, height=5, palette='dark', alpha=.6)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/251:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.5, height=5, palette='dark', alpha=.7)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/252:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.5, height=5, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION")
90/253:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.5, height=5, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION (%)")
90/254:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.2, height=5, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENT", "ARGENTINA BALL POSSESSION (%)")
90/255:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.2, height=5, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/256:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.2, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/257:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=.8, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/258:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=2, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/259:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.5, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/260:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.2, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/261:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.3, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/262:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
plot = sns.catplot(data=argentina_home, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/263:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1])

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2])

data = pd.concat([argentina_home, argentina_away], axis=0)
data.sort_values(by='DATE', ascending=True)
data

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/264:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1])

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2])

data = pd.concat([argentina_home, argentina_away], axis=0)
data

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/265:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1])
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/266:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)
argentina_home

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/267:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2], axis=1)

data = pd.concat([argentina_home, argentina_away], axis=0)
data

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/268:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2], axis=1)

data = pd.DataFrame([argentina_home, argentina_away], axis=0)
data

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/269:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2], axis=1)

data = pd.DataFrame(index=[argentina_home, argentina_away])
data

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/270:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2], axis=1)

data = pd.DataFrame(data=[argentina_home, argentina_away])
data

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/271:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)
argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/272:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)
argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/273:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)
argentina_home.rename(columns={'TEAM2': 'OPONENT'})
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/274:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)
argentina_home.columns


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/275:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)
argentina_home.rename(columns={'TEAM2': 'OPONENT'})
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/276:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/277:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home.rename(columns={"TEAM2": "OPONENT"})
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/278:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentinaargentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/279:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/280:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/281:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})
argentina_home = pd.concat([argentina_home.OPONENT, argentina_home.POSSESSION], axis=1)
argentina_home


#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/282:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0)
data

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/283:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0)
data

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/284:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data

#plot = sns.catplot(data=data, kind='bar', x='TEAM2', y='POSSESSION_TEAM1', aspect=1.4, height=4, palette='dark', alpha=.8)
#plot.despine(left=True)
#plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/285:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.4, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/286:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.5, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/287:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.7, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/288:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.8, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/289:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.8, height=5, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/290:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.8, height=4.5, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/291:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.8, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
90/292:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).lower()
    
data.rename(columns=new_columns).head()
90/293:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns).head()
90/294:
import pandas as pd
import seaborn as sns
sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
90/295: data = pd.read_csv("../dataset/matches.csv", header=0)
90/296: data.head(2)
90/297:
def replace_spaces(s):
    return s.replace(" ", "_")
90/298:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns).head()
90/299:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
90/300: data.DATE = pd.to_datetime(data.DATE, format='%d%m%Y')
90/301:
import pandas as pd
import seaborn as sns
sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
90/302: data = pd.read_csv("../dataset/matches.csv", header=0)
90/303: data.head(2)
90/304:
def replace_spaces(s):
    return s.replace(" ", "_")
90/305:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns).head()
90/306:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
90/307:
import pandas as pd
import seaborn as sns
sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
90/308: data = pd.read_csv("../dataset/matches.csv", header=0)
90/309: data.head(2)
90/310:
def replace_spaces(s):
    return s.replace(" ", "_")
90/311:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns).head()
90/312:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
94/1:
import pandas as pd
import seaborn as sns
sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/2: data = pd.read_csv("../dataset/matches.csv", header=0)
94/3: data.head(2)
94/4:
def replace_spaces(s):
    return s.replace(" ", "_")
94/5:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns).head()
94/6:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
94/7:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data.rename(columns=new_columns).head()
data
94/8:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data = data.rename(columns=new_columns)
data
94/9:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)
94/10: data.DATE = pd.to_datetime(data.DATE, format='%d%m%Y')
94/11:
argentina_home = data.loc[data.TEAM1 == 'ARGENTINA']
argentina_visitant = data.loc[data.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
94/12: argentina.sort_values(by='DATE', ascending=True)
94/13:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.8, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
94/14:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', orient='h', x='OPONENT', y='POSSESSION', aspect=1.8, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
94/15:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.8, height=4, palette='dark', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
94/16:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.8, height=4, palette='dark', alpha=.8, orient='v')
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
94/17:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='OPONENT', y='POSSESSION', aspect=1.8, height=4, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
94/18:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPONENT', aspect=1.8, height=4, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("OPONENTS", "ARGENTINA BALL POSSESSION (%)")
94/19:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPONENT', aspect=1.8, height=4, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("ARGENTINA BALL POSSESSION (%)", "")
94/20:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPONENT', aspect=1.8, height=3, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("ARGENTINA BALL POSSESSION (%)", "")
94/21:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPONENT', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("ARGENTINA BALL POSSESSION (%)", "")
94/22:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPONENT', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "")
94/23:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPONENT', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Oponents")
94/24:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/25:
import pandas as pd
import seaborn as sns
sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/26: data = pd.read_csv("../dataset/matches.csv", header=0)
94/27: data.head(2)
94/28:
def replace_spaces(s):
    return s.replace(" ", "_")
94/29:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data = data.rename(columns=new_columns)
data
94/30:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)

data.POSSESSION_TEAM1 = data.POSSESSION_TEAM1.str.replace('%', '').astype(int)
data.POSSESSION_TEAM2 = data.POSSESSION_TEAM2.str.replace('%', '').astype(int)
94/31: data.DATE = pd.to_datetime(data.DATE, format='%d%m%Y')
94/32:
argentina_home = data.loc[data.TEAM1 == 'ARGENTINA']
argentina_visitant = data.loc[data.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_visitant], axis=0)
94/33: argentina.sort_values(by='DATE', ascending=True)
94/34:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home.POSSESSION_TEAM1 = argentina_home.POSSESSION_TEAM1.str.replace('%', '').astype(int)
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away.POSSESSION_TEAM2 = argentina_away.POSSESSION_TEAM2.str.replace('%', '').astype(int)
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_home.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/35:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_home.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/36:
import pandas as pd
import seaborn as sns
sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/37: data = pd.read_csv("../dataset/matches.csv", header=0)
94/38: data.head(2)
94/39:
def replace_spaces(s):
    return s.replace(" ", "_")
94/40:
new_columns = {}
for col in data.columns:
    new_columns[col] = replace_spaces(col).upper()
    
data = data.rename(columns=new_columns)
data
94/41:
data.DATE = data.DATE.str.replace('nov.', '11', regex=False)
data.DATE = data.DATE.str.replace('DEC', '12', regex=False)
data.DATE = data.DATE.str.replace(' ', '', regex=False)

data.POSSESSION_TEAM1 = data.POSSESSION_TEAM1.str.replace('%', '').astype(int)
data.POSSESSION_TEAM2 = data.POSSESSION_TEAM2.str.replace('%', '').astype(int)
94/42: data.DATE = pd.to_datetime(data.DATE, format='%d%m%Y')
94/43:
argentina_home = data.loc[data.TEAM1 == 'ARGENTINA']
argentina_away = data.loc[data.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
94/44: argentina.sort_values(by='DATE', ascending=True)
94/45:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_home.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/46:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/47:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=2, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/48:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=3, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/49:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/50:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=1.8, height=3.5, width=4, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/51:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=1.8, height=3.5, width=2, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/52:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=1.8, height=3.5, width=1, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/53:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='DEFENDING', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/54:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data
94/55:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='dark', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/56:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/57: data
94/58: plot = sns.lineplot(data=data, x='DATE', y='POSSESSION')
94/59:
import pandas as pd
import seaborn as sns
sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/60: base = pd.read_csv("../dataset/matches.csv", header=0)
94/61: base.head(2)
94/62:
def replace_spaces(s):
    return s.replace(" ", "_")
94/63:
new_columns = {}
for col in base.columns:
    new_columns[col] = replace_spaces(col).upper()
    
base = base.rename(columns=new_columns)
base
94/64:
base.DATE = base.DATE.str.replace('nov.', '11', regex=False)
base.DATE = base.DATE.str.replace('DEC', '12', regex=False)
base.DATE = base.DATE.str.replace(' ', '', regex=False)

base.POSSESSION_TEAM1 = base.POSSESSION_TEAM1.str.replace('%', '').astype(int)
base.POSSESSION_TEAM2 = base.POSSESSION_TEAM2.str.replace('%', '').astype(int)
94/65: base.DATE = pd.to_datetime(base.DATE, format='%d%m%Y')
94/66:
argentina_home = base.loc[base.TEAM1 == 'ARGENTINA']
argentina_away = base.loc[base.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
94/67: argentina.sort_values(by='DATE', ascending=True)
94/68:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/69: plot = sns.lineplot(data=data, x='DATE', y='POSSESSION')
94/70:
import pandas as pd
import seaborn as sns
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/71: base = pd.read_csv("../dataset/matches.csv", header=0)
94/72: base.head(2)
94/73:
def replace_spaces(s):
    return s.replace(" ", "_")
94/74:
new_columns = {}
for col in base.columns:
    new_columns[col] = replace_spaces(col).upper()
    
base = base.rename(columns=new_columns)
base
94/75:
base.DATE = base.DATE.str.replace('nov.', '11', regex=False)
base.DATE = base.DATE.str.replace('DEC', '12', regex=False)
base.DATE = base.DATE.str.replace(' ', '', regex=False)

base.POSSESSION_TEAM1 = base.POSSESSION_TEAM1.str.replace('%', '').astype(int)
base.POSSESSION_TEAM2 = base.POSSESSION_TEAM2.str.replace('%', '').astype(int)
94/76: base.DATE = pd.to_datetime(base.DATE, format='%d%m%Y')
94/77:
argentina_home = base.loc[base.TEAM1 == 'ARGENTINA']
argentina_away = base.loc[base.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
94/78: argentina.sort_values(by='DATE', ascending=True)
94/79:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/80: plot = sns.lineplot(data=data, x='DATE', y='POSSESSION')
94/81: plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', height=4)
94/82:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.CATEGORY, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.CATEGORY, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/83:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.CATEGORY, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.CATEGORY, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='CATEGORY' aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/84:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.CATEGORY, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.CATEGORY, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', hue='CATEGORY', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/85:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.CATEGORY, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.CATEGORY, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/86: plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', hue='CATEGORY', height=4)
94/87: plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', hue='CATEGORY')
94/88:
plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', hue='CATEGORY')
data
94/89:
plot = sns.lineplot(data=data, x='DATE', y='POSSESSION')
data
94/90:
import pandas as pd
import seaborn as sns
import matplotlib as plt
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/91:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
  
# Set label for y-axis
ax.set_ylabel( "Argentina ball possession" , size = 12 )
data
94/92:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/93:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
  
# Set label for y-axis
ax.set_ylabel( "Argentina ball possession" , size = 12 )
data
94/94:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 6 )
  
# Set label for y-axis
ax.set_ylabel( "Argentina ball possession" , size = 12 )
data
94/95:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
  
# Set label for y-axis
ax.set_ylabel( "Argentina ball possession" , size = 12 )
data
94/96:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
  
# Set label for y-axis
ax.set_ylabel( "Argentina ball possession" , size = 12 )
plt.xticks(fontsize=6)
data
94/97:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
  
# Set label for y-axis
ax.set_ylabel( "Argentina ball possession" , size = 12 )
plt.xticks(fontsize=12)
data
94/98:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
  
# Set label for y-axis
ax.set_ylabel( "Argentina ball possession" , size = 12 )
plt.xticks(fontsize=8)
data
94/99:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
plt.xticks(fontsize=8)
94/100:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
plt.xticks(ticks=ticks, fontsize=8)
94/101:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
ax.set_xticks(ticks=ticks)

plt.xticks(fontsize=8)
94/102:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
plt.xticks(fontsize=8)
94/103:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
ax.set_xyticks(ticks=ticks)
plt.xticks(fontsize=8)
94/104:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
ax.set_yticks(ticks=ticks)
plt.xticks(fontsize=8)
94/105:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
plt.xticks(fontsize=8)
plt.yticks(ticks=ticks, fontsize=12)
94/106:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
plt.xticks(fontsize=8)
plt.yticks(ticks=ticks, fontsize=8)
94/107:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
plt.xticks(fontsize=8)
plt.yticks(ticks=ticks, fontsize=10)
94/108:
fig, ax = plt.subplots(figsize = (5 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
plt.xticks(fontsize=8)
plt.yticks(ticks=ticks, fontsize=8)
94/109:
fig, ax = plt.subplots(figsize = (8 , 3))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
plt.xticks(fontsize=8)
plt.yticks(ticks=ticks, fontsize=8)
94/110:
fig, ax = plt.subplots(figsize = (8 , 5))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
plt.xticks(fontsize=8)
plt.yticks(ticks=ticks, fontsize=8)
94/111:
fig, ax = plt.subplots(figsize = (8 , 5))

plot = sns.lineplot(data=data, x='DATE', y='POSSESSION', ax=ax)
ax.set_xlabel( "Date" , size = 12 )
ax.set_ylabel( "Argentina ball possession (%)" , size = 12 )
ticks = [30, 35, 40, 45, 50, 55, 60, 65]
plt.xticks(fontsize=8)
plt.yticks(ticks=ticks, fontsize=10)
94/112: base.TEAM1.value_counts()
94/113: base.TEAM1.value_counts().reset_index()
94/114:
new_columns = {}
for col in base.columns:
    new_columns[col] = replace_spaces(col).upper()
    
base = base.rename(columns=new_columns)
94/115:
base.DATE = base.DATE.str.replace('nov.', '11', regex=False)
base.DATE = base.DATE.str.replace('DEC', '12', regex=False)
base.DATE = base.DATE.str.replace(' ', '', regex=False)

base.POSSESSION_TEAM1 = base.POSSESSION_TEAM1.str.replace('%', '').astype(int)
base.POSSESSION_TEAM2 = base.POSSESSION_TEAM2.str.replace('%', '').astype(int)

base
94/116:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/117: base = pd.read_csv("../dataset/matches.csv", header=0)
94/118: base.head(2)
94/119:
def replace_spaces(s):
    return s.replace(" ", "_")
94/120:
new_columns = {}
for col in base.columns:
    new_columns[col] = replace_spaces(col).upper()
    
base = base.rename(columns=new_columns)
94/121:
base.DATE = base.DATE.str.replace('nov.', '11', regex=False)
base.DATE = base.DATE.str.replace('DEC', '12', regex=False)
base.DATE = base.DATE.str.replace(' ', '', regex=False)

base.POSSESSION_TEAM1 = base.POSSESSION_TEAM1.str.replace('%', '').astype(int)
base.POSSESSION_TEAM2 = base.POSSESSION_TEAM2.str.replace('%', '').astype(int)

base
94/122:
base.DATE = base.DATE.str.replace('nov.', '11', regex=False)
base.DATE = base.DATE.str.replace('DEC', '12', regex=False)
base.DATE = base.DATE.str.replace(' ', '', regex=False)

base.POSSESSION_TEAM1 = base.POSSESSION_TEAM1.str.replace('%', '').astype(int)
base.POSSESSION_TEAM2 = base.POSSESSION_TEAM2.str.replace('%', '').astype(int)
94/123:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/124: base = pd.read_csv("../dataset/matches.csv", header=0)
94/125: base.head(2)
94/126:
def replace_spaces(s):
    return s.replace(" ", "_")
94/127:
new_columns = {}
for col in base.columns:
    new_columns[col] = replace_spaces(col).upper()
    
base = base.rename(columns=new_columns)
94/128:
base.DATE = base.DATE.str.replace('nov.', '11', regex=False)
base.DATE = base.DATE.str.replace('DEC', '12', regex=False)
base.DATE = base.DATE.str.replace(' ', '', regex=False)

base.POSSESSION_TEAM1 = base.POSSESSION_TEAM1.str.replace('%', '').astype(int)
base.POSSESSION_TEAM2 = base.POSSESSION_TEAM2.str.replace('%', '').astype(int)
94/129:
base.DATE = pd.to_datetime(base.DATE, format='%d%m%Y')
base
94/130:
base.DATE = pd.to_datetime(base.DATE, format='%d%m%Y')
base.head()
94/131:
base.TEAM1.value_counts().reset_index()
base.TEAM1
94/132:
base.TEAM1.value_counts().reset_index()
base.TEAM1.reset_index()
94/133: base.TEAM1.value_counts().reset_index()
94/134: matches = pd.read_csv("../dataset/matches.csv", header=0)
94/135: matches.head(2)
94/136:
def replace_spaces(s):
    return s.replace(" ", "_")
94/137:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
94/138:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
94/139:
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y')
matches.head()
94/140:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
94/141: argentina.sort_values(by='DATE', ascending=True)
94/142:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.CATEGORY, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.CATEGORY, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/143:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
home
94/144:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
94/145:
home.columns = ['Countries', 'Goals']
home
94/146:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
94/147:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
94/148:
goals = pd.concat([home, away], axis=0)
goals
94/149:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals
94/150:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals.shape
94/151:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals
94/152:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
94/153:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals
94/154:
plot = sns.catplot(data=goals, kind='bar', x='Countries', y='Goals', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8)
plot.despine(left=True)
plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/155: goals.plot()
94/156: goals.plot.bar()
94/157:
goals.plot.bar()
plt.xticks(fontsize=8)
94/158:
plt.xticks(fontsize=8)
goals.plot.bar()
94/159:
goals.plot.bar()
plt.xticks(fontsize=8)
94/160:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.show()
94/161:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.set_title("Countries number of goals")
plt.show()
94/162:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.show()
94/163:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.x_label('')
plt.show()
94/164:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('')
plt.show()
94/165:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.show()
94/166:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(6)
94/167:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(7)
94/168:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
94/169:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.show()
94/170: matches_goals = matches.NUMBER_OF_GOALS_TEAM1 + matches.NUMBER_OF_GOALS_TEAM2
94/171:
matches_goals = matches.NUMBER_OF_GOALS_TEAM1 + matches.NUMBER_OF_GOALS_TEAM2
matches_goals
94/172:
matches_goals = matches.NUMBER_OF_GOALS_TEAM1 + matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([matches.CATEGORY, matches_goals])
groups_goals
94/173:
matches_goals = matches.NUMBER_OF_GOALS_TEAM1 + matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([matches.CATEGORY, matches_goals], axis=1)
groups_goals
94/174:
matches_goals = matches.NUMBER_OF_GOALS_TEAM1 + matches.NUMBER_OF_GOALS_TEAM2
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
groups_matches
94/175:
matches_goals = matches.NUMBER_OF_GOALS_TEAM1 + matches.NUMBER_OF_GOALS_TEAM2
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
groups_goals = pd.concat([goups_matches, matches_goals], axis=1)
groups_goals
94/176:
matches_goals = matches.NUMBER_OF_GOALS_TEAM1 + matches.NUMBER_OF_GOALS_TEAM2
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
groups_goals = pd.concat([groups_matches, matches_goals], axis=1)
groups_goals
94/177:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals
94/178:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals
94/179:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
94/180:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
94/181: groups_goals.plot.bar()
94/182: groups_goals.plot.barh()
94/183:
groups_goals.plot.barh()
plt.xticks(range(22))
plt.show()
94/184:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.show()
94/185:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.show()
94/186:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/187:
plot = sns.catplot(data=groups_goals, kind='bar', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/188:
plot = sns.catplot(data=groups_goals, kind='bar', aspect=1, height=3.5, palette='coolwarm', alpha=.8, orient='h')
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/189:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/190:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data
94/191:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)

data.plot.barh()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/192:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data
data.plot.barh()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/193:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh()
data

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/194:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='POSSESSION')
data

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/195:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x=['POSSESSION', 'OPPONENT'])
data

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/196:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh()
data

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/197:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT')
data

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/198:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y='POSSESSION')
data

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/199:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'])
data

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/200:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
data

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/201:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.yticks(range(101))

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/202:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.yticks(range(101))
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/203:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(101))
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/204:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(101, 5))
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/205:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/206:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'])
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/207:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'])
plt.yticks(fontsize=8)
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/208:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'])
plt.yticks(fontsize=10)
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/209:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'])
plt.yticks(fontsize=9)
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/210:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'])
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/211:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'], loc=best)
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/212:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'], loc='best')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/213:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'], loc='upper left')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/214:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'], loc='upper left')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/215:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/216:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Possession (%)', 'Defense (%)'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/217:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Argentina', 'Defense (%)'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/218:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['POSSESSION', 'DEFENDING'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Argentina', 'Opponent'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/219:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Argentina', 'Opponent'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/220:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/221:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True, color={"DEFENDING": "red", "POSSESSION": "aqua"})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/222:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True, color={"DEFENDING": "red", "POSSESSION": "lightskyblue"})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/223:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue"})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()

#plot = sns.catplot(data=data, kind='bar', x='POSSESSION', y='OPPONENT', aspect=1.8, height=3.5, palette='coolwarm', alpha=.8, orient='h')
#plot.despine(left=True)
#plot.set_axis_labels("Argentina ball possession (%)", "Opponents")
94/224:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue"})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/225:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
94/226:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/227: matches = pd.read_csv("../dataset/matches.csv", header=0)
94/228: matches.head(2)
94/229:
def replace_spaces(s):
    return s.replace(" ", "_")
94/230:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
94/231:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
94/232:
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y')
matches.head()
94/233:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
94/234:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
94/235:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
94/236:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.show()
94/237:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
94/238:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
94/239:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/240:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
94/241: argentina.sort_values(by='DATE', ascending=True)
94/242:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue"})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/243:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'In contest'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue"})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/244:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'In contest'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue"})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/245:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue"})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/246:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION', 'IN CONTEST'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue"})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/247:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'POSSESSION', 'IN CONTEST'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/248:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/249:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue", 'IN CONTEST': 'lightgrey'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/250:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue", 'IN CONTEST': 'lemonchiffron'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/251:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue", 'IN CONTEST': 'lemonchiffon'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/252:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={"DEFENDING": "orangered", "POSSESSION": "lightskyblue", 'IN CONTEST': 'lemonchiffon'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/253:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})

data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data
94/254:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})
data
94/255:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/256:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True)
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/257:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'orange', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/258:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'white', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/259:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'black', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/260:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/261:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Argentina ball possession throughout all seven matches')
plt.show()
94/262:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/263:
playoffs = matches[!matches.CATEGORY.str.startswith('Group')]
playoffs
94/264:
playoffs = matches[!(matches.CATEGORY.str.startswith('Group'))]
playoffs
94/265:
playoffs = matches[not matches.CATEGORY.str.startswith('Group')]
playoffs
94/266:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
playoffs
94/267:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals
94/268:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals
94/269:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index
94/270:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
94/271:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals
94/272:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals
94/273:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals
94/274:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False].sort_values(by='DATE', ascending=False)
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals.sort_values(by='Goals', ascendin)
94/275:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False].sort_values(by='DATE', ascending=False)
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals.sort_values(by='Goals')
94/276:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False].sort_values(by='DATE', ascending=False)
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals
94/277:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False].sort_values(by='DATE', ascending=False)
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals.iloc[4], playoffs_goals.iloc[2] = playoffs_goals.iloc[2], playoffs_goals.iloc[4]
94/278:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False].sort_values(by='DATE', ascending=False)
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals.iloc[4], playoffs_goals.iloc[2] = playoffs_goals.iloc[2], playoffs_goals.iloc[4]
playoffs_goals
94/279:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False].sort_values(by='DATE', ascending=False)
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals
94/280:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False].sort_values(by='DATE', ascending=True)
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals
94/281:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals
94/282:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals
94/283:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round', sort=False).sum()
playoffs_goals.index = ['Finals', 'Third place', 'Quarter-final', 'Round of 16', 'Semi-final']
playoffs_goals
94/284:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round', sort=False).sum()
playoffs_goals
94/285:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round', sort=False).sum()
playoffs_goals.index = ['Round of 16', 'Quarter-final', 'Semi-final', 'Third place', 'Finals']
playoffs_goals
94/286:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round', sort=False).sum()
playoffs_goals
94/287:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round', sort=False).sum()
playoffs_goals.index = ['Round of 16', 'Quarter-final', 'Semi-final', 'Third place', 'Finals']
playoffs_goals
94/288:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Goals']
playoffs_goals = playoffs_goals.groupby('Round', sort=False).sum()
playoffs_goals.index = ['Round of 16', 'Quarter-final', 'Semi-final', 'Third place', 'Finals']
playoffs_goals
94/289:
playoffs_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/290:
playoffs_goals.plot.barh()
plt.xticks(range(29))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/291:
playoffs_goals.plot.barh()
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/292:
playoffs_goals.plot.bar()
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/293:
playoffs_goals.plot.bar()
plt.yticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/294:
playoffs_goals.plot.barh()
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/295: argentina.sort_values(by='DATE', ascending=True)
94/296:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})
data
94/297:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/298:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals, playoffs.TEAM1, playoffs.TEAM2], axis=1)

playoffs_goals
94/299:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals, playoffs.TEAM1, playoffs.TEAM2], axis=1)
playoffs_goals.columns = ['Round', 'Goals', 'Home', 'Away']

playoffs_goals
94/300:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals, playoffs.TEAM1, playoffs.TEAM2], axis=1)
playoffs_goals.columns = ['Round', 'Goals', 'Home', 'Away']
playoffs_goals = playoffs_goals.groupby('Round', sort=False).sum()
playoffs_goals.index = ['Round of 16', 'Quarter-final', 'Semi-final', 'Third place', 'Finals']
playoffs_goals
94/301:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, matches_goals, playoffs.TEAM1, playoffs.TEAM2], axis=1)
playoffs_goals.columns = ['Round', 'Goals', 'Home', 'Away']
playoffs_goals
94/302:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Goals']
playoffs_goals
94/303:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals']
playoffs_goals
94/304:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals']
playoffs_goals = playoffs_goals.groupby('Total goals').sum()
playoffs_goals
94/305:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals']
playoffs_goals = playoffs_goals.groupby('Round').sum()
playoffs_goals
94/306:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals']
playoffs_goals
94/307:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals']
playoffs_goals

total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals
94/308:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals']
playoffs_goals

total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round').sum()
total_playoffs_goals
94/309:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals']
playoffs_goals

total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals
94/310:
total_playoffs_goals.plot.barh()
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/311:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals']
playoffs_goals
94/312:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals
94/313: arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA' || playoff_goals.Away == 'ARGENTINA']
94/314: arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA' | playoff_goals.Away == 'ARGENTINA']
94/315: arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA' | playoffs_goals.Away == 'ARGENTINA']
94/316: arg_playoffs_goals = playoffs_goals.query('Home == ARGENTINA || Away == ARGENTINA')
94/317: arg_playoffs_goals = playoffs_goals.query('Home == ARGENTINA or Away == ARGENTINA')
94/318: arg_playoffs_goals = playoffs_goals.query('Home == `ARGENTINA` or Away == `ARGENTINA`')
94/319: arg_playoffs_goals = playoffs_goals[(playoffs_goals.Home == 'ARGENTINA') || (playoffs_goals.Away == 'ARGENTINA')]
94/320: arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
94/321:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals
94/322:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals']
playoffs_goals
94/323:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals', 'Date']
playoffs_goals
94/324:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals
94/325:
total_playoffs_goals.plot.barh()
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/326:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals =
94/327:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals
94/328:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals = pd.concat([arg_playoffs_goals, playoff_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='DATE', ascending=True)
94/329:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals = pd.concat([arg_playoffs_goals, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='DATE', ascending=True)
94/330:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals = pd.concat([arg_playoffs_goals, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
94/331:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals = pd.concat([arg_playoffs_goals, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs_goals
94/332:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals = pd.concat([arg_playoffs_goals, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals home', 'Goals away']]
94/333:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals = pd.concat([arg_playoffs_goals, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals home', 'Goals away']]
arg_playoffs_goals
94/334:
arg_playoffs_goals = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_goals = pd.concat([arg_playoffs_goals, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
#arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals home', 'Goals away']]
arg_playoffs_goals
94/335:
arg_playoffs = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs = pd.concat([arg_playoffs, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
#arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals home', 'Goals away']]
arg_playoffs
94/336:
arg_playoffs = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs = pd.concat([arg_playoffs, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs
94/337:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home
94/338:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home']]
arg_playoffs_home
94/339:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home']]
arg_playoffs_home.columns = ['Round', 'Goals']
arg_playoffs_home
94/340:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away']]
arg_playoffs_away.columns = ['Round', 'Goals']
arg_playoffs_away
94/341:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
94/342:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
94/343:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals
94/344:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_home
94/345:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals
94/346:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Argentina']]
arg_playoffs_goals
94/347:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Argentina goals']]
arg_playoffs_goals
94/348:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Argentina goals', 'Date']
arg_playoffs_away
94/349:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Argentina goals']]
arg_playoffs_goals
94/350:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Argentina goals', 'Date']
arg_playoffs_home
94/351:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Argentina goals', 'Date']
arg_playoffs_away
94/352:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Argentina goals']]
arg_playoffs_goals
94/353:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Argentina home goals', 'Date']
arg_playoffs_home
94/354:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Argentina away goals', 'Date']
arg_playoffs_away
94/355:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Argentina goals']]
arg_playoffs_goals
94/356:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
94/357:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
94/358:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals
94/359:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals', 'Date']
94/360:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
94/361:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
94/362:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals', 'Date']
94/363:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
94/364:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals
94/365:
fra_playoffs = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs = pd.concat([fra_playoffs, playoffs_goals[playoffs_goals.Away == 'FRANCE']], axis=0).sort_values(by='Date', ascending=True)
fra_playoffs
94/366:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
94/367:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
94/368:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'Argentina goals']
fra_playoffs_goals
94/369:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'French goals']
fra_playoffs_goals
94/370:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals
94/371: finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals']])
94/372:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals']])
finalists_goals
94/373:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals']], axis=1)
finalists_goals
94/374:
finalists_goals = pd.concat([arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals']], axis=1)
finalists_goals
94/375:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals']], axis=1)
finalists_goals
94/376:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals.reset_index()
94/377:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
94/378:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
94/379:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index()[['Round', 'Argentina goals']]
arg_playoffs_goals
94/380:
fra_playoffs = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs = pd.concat([fra_playoffs, playoffs_goals[playoffs_goals.Away == 'FRANCE']], axis=0).sort_values(by='Date', ascending=True)
fra_playoffs
94/381:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
94/382:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
94/383:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
94/384:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals']], axis=1)
finalists_goals
94/385: playoffs_goals
94/386:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals.reset_index()
94/387:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
94/388:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
94/389:
total_playoffs_goals.plot.barh()
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/390:
total_playoffs_goals.plot.barh(x='Total goals', y='Round')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/391:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/392:
arg_playoffs = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs = pd.concat([arg_playoffs, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs
94/393:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
94/394:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
94/395:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index()[['Round', 'Argentina goals']]
arg_playoffs_goals
94/396:
fra_playoffs = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs = pd.concat([fra_playoffs, playoffs_goals[playoffs_goals.Away == 'FRANCE']], axis=0).sort_values(by='Date', ascending=True)
fra_playoffs
94/397:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
94/398:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
94/399:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
94/400:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], total_playoffs_goals['Total goals']], axis=1)
finalists_goals
94/401:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], total_playoffs_goals['Total goals']], axis=1)
finalists_goals.dropna()
94/402:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], total_playoffs_goals['Total goals']], axis=1)
finalists_goals.dropna()
finalists_goals
94/403:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], total_playoffs_goals['Total goals']], axis=1)
finalists_goals = finalists_goals.dropna()
finalists_goals
94/404:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], total_playoffs_goals['Total goals']], axis=1)
finalists_goals = finalists_goals.dropna()
finalists_goals['Argentina goals'] = finalists_goals['Argentina goals'].astype(int)
finalists_goals['France goals'] = finalists_goals['France goals'].astype(int)
94/405:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], total_playoffs_goals['Total goals']], axis=1)
finalists_goals = finalists_goals.dropna()
finalists_goals['Argentina goals'] = finalists_goals['Argentina goals'].astype(int)
finalists_goals['France goals'] = finalists_goals['France goals'].astype(int)
finalists_goals
94/406:
finalists_goals.plot.barh(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/407:
finalists_goals.plot.barh(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
#plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/408:
finalists_goals.plot.barh(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
#plt.xticks(range(0, 29, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/409:
finalists_goals.plot.barh(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.xticks(range(0, 29, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/410:
finalists_goals.plot.barh(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.xticks(range(29))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/411:
finalists_goals.plot.barh(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.xticks(range(0, 29, 2))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/412:
finalists_goals.plot.barh(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.xticks(range(0, 29, 4))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/413:
finalists_goals.plot.barh(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.xticks(range(0, 29, 2))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/414:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.xticks(range(0, 29, 2))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/415:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/416:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2))
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/417:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2), rotation=90)
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/418:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2))
plt.xticks(rotation=90)
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/419:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2))
plt.xticks(rotation=0)
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/420:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
94/421:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
94/422:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
94/423:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
94/424:
arg_playoffs = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs = pd.concat([arg_playoffs, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs
94/425:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
94/426:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
94/427:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index()[['Round', 'Argentina goals']]
arg_playoffs_goals
94/428:
fra_playoffs = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs = pd.concat([fra_playoffs, playoffs_goals[playoffs_goals.Away == 'FRANCE']], axis=0).sort_values(by='Date', ascending=True)
fra_playoffs
94/429:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
94/430:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
94/431:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
94/432:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], total_playoffs_goals['Total goals']], axis=1)
finalists_goals = finalists_goals.dropna()
finalists_goals['Argentina goals'] = finalists_goals['Argentina goals'].astype(int)
finalists_goals['France goals'] = finalists_goals['France goals'].astype(int)
finalists_goals
94/433:
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], total_playoffs_goals['Total goals']], axis=1)
finalists_goals
94/434:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
94/435:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Third place'].reset_index()
finalists_games
94/436:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals = total_playoffs_goals.rename(columns={'Play-off for third place': 'Third place'})
total_playoffs_goals
94/437:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals = total_playoffs_goals.rename(column={'Play-off for third place': 'Third place'})
total_playoffs_goals
94/438:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals = total_playoffs_goals.rename(columns={'Play-off for third place': 'Third place'})
total_playoffs_goals
94/439:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
94/440:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_games
94/441:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
94/442:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
94/443:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/444: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
94/445:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.head()
94/446:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgrey')
94/447:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='yellow')
94/448:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgrey')
94/449:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
gpd.set_theme(style='lightgrid')
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/450:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/451:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgrey')
94/452:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightskyblue')
94/453:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgray')
94/454:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries[countries.name == 'Brazil'].plot(color='lightgray')
94/455:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgray')
94/456:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgray', figsize=(10, 6))
94/457:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgray', figsize=(7, 6))
94/458:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgray', figsize=(10, 6))
94/459:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgray', figsize=(10, 8))
94/460:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgray', figsize=(10, 8))

wc_participants = matches.TEAM1
countries
94/461:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgray', figsize=(10, 8))

wc_participants = matches.TEAM1.str.capitalize()
wc_participants
94/462:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.plot(color='lightgray', figsize=(10, 8))

wc_participants = matches.TEAM1.str.capitalize()
countries
94/463:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))

c = countries[countries.name == 'Canada']
var = c.__geo_interface__['features']
var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
plt.show()

wc_participants = matches.TEAM1.str.capitalize()
countries
94/464:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))

c = countries[countries.name == 'Canada']
var = c.__geo_interface__['features']
var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
plt.show()

wc_participants = matches.TEAM1.str.capitalize()
countries
94/465:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/466:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
94/467:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))

c = countries[countries.name == 'Canada']
var = c.__geo_interface__['features']
var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
plt.show()

wc_participants = matches.TEAM1.str.capitalize()
countries
94/468:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/469:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    c
    
plt.show()
94/470:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    print(c)
    
plt.show()
94/471:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/472:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    print(c.name)
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/473:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    print(var)
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/474:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    print(c)
    var = c.__geo_interface__['features']
    print(var)
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/475:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var)
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/476:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/477:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/478:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    print(country)
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/479:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    print(country)
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/480:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants:
    print(country)
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
countries
94/481:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

countries
94/482:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

countries[countries.name == 'England']
94/483:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

countries[countries.name == 'United Kingdom']
94/484:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

countries[countries.name == 'Ireland']
94/485:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants.tail():
    print(country)
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/486:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()

for country in wc_participants.head():
    print(country)
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/487:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()
wc_participants
94/488:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()
pd.unique(wc_participants)
94/489:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()
wc_participants = pd.unique(wc_participants)
wc_participants
94/490:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = matches.TEAM1.str.capitalize()
wc_participants = pd.unique(wc_participants)
len(wc_participants)
94/491:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())

for country in wc_participants.head():
    print(country)
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/492:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)

for country in wc_participants.head():
    print(country)
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/493:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)

for country in wc_participants.tail():
    print(country)
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
    
plt.show()
94/494:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)

wc_participants
94/495:
def patch(ax, country):
    c = countries[countries.name == country]
    print(c.name)
    var = c.__geo_interface__['features']
    print(var[0])
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
94/496:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)

patch(ax, 'Argentina')

plt.show()
94/497:
def patch(ax, country):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
94/498:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)

patch(ax, 'Argentina')

plt.show()
94/499:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)

patch(ax, 'Argentina')
patch(ax, 'Brazil')

plt.show()
94/500:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)

patch(ax, 'Argentina')
patch(ax, 'Brazil')
patch(ax, 'Morroco')

plt.show()
94/501:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)

patch(ax, 'Argentina')
patch(ax, 'Brazil')
patch(ax, 'Morrocco')

plt.show()
94/502:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)

patch(ax, 'Argentina')
patch(ax, 'Brazil')
patch(ax, 'Morocco')

plt.show()
94/503:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants
94/504:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.loc[1,0] = 'United Kingdom'
wc_participants
94/505:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.loc[1,0] = 'United Kingdom'
wc_participants['0']
94/506:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.loc[1,0] = 'United Kingdom'
wc_participants.reset_index()
94/507:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.loc[1,0] = 'United Kingdom'
wc_participants.reset_index()['0']
94/508:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.loc[1,0] = 'United Kingdom'
wc_participants.columns
94/509:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.loc[1,0] = 'United Kingdom'
wc_participants.columns = ['Countries']
94/510:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.loc[1,0] = 'United Kingdom'
wc_participants.columns = ['Countries']
wc_participants
94/511:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1,'Countries'] = 'United Kingdom'
wc_participants
94/512:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1,'Countries'] = 'United Kingdom'
wc_participants.Countries.str.title()
wc_participants
94/513:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1,'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants
94/514:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1,'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()

for country in wc_participants:
    patch(ax, country)

plt.show()
94/515:
def patch(ax, country):
    print(country)
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
94/516:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1,'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()

for country in wc_participants:
    patch(ax, country)

plt.show()
94/517:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1,'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()

for country in wc_participants.Countries:
    patch(ax, country)

plt.show()
94/518:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries
94/519:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries[countries.continent == 'America']
94/520:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries
94/521:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries[countries.continent == 'North America']
94/522: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
94/523:
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants.Countries = wc_participants.Countries.str.title()

for country in wc_participants.Countries:
    patch(ax, country)

plt.show()
94/524:
ax = countries.plot(color='lightgray', figsize=(10, 8))


wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'

for country in wc_participants.Countries:
    patch(ax, country)

plt.show()
94/525:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.continent.str.startswith('Wales')
94/526:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries.continent.str.startswith('Wales').value_counts()
94/527:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries[countries.continent == 'Europe']
94/528:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants = wc_participants.drop(['Wales'], axis=0)
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
94/529: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
94/530:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants
94/531:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants.drop(index=16)
94/532:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants.drop(index=16)
wc_participants
94/533:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16)
94/534:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16)
94/535:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16)
wc_participants
94/536:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16).reset_index()
wc_participants
94/537:
ax = countries.plot(color='lightgray', figsize=(10, 8))

for country in wc_participants.Countries:
    patch(ax, country)

plt.show()
94/538:
countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
countries[countries.continent == 'Asia']
94/539: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
94/540:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16).reset_index().Countries
wc_participants
94/541:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16).reset_index()
wc_participants
94/542:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16).reset_index()['Countries']
wc_participants
94/543:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16).reset_index()['Countries']
wc_participants.loc[22] = 'South Korea'
94/544:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16).reset_index()['Countries']
wc_participants.loc[22] = 'South Korea'
wc_participants
94/545:
ax = countries.plot(color='lightgray', figsize=(10, 8))

for country in wc_participants.Countries:
    patch(ax, country)

plt.show()
94/546:
ax = countries.plot(color='lightgray', figsize=(10, 8))

for country in wc_participants:
    patch(ax, country)

plt.show()
94/547:
def patch(ax, country):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
94/548:
def patch(ax, country):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='red', ec='black', alpha=.85, zorder=2))
94/549: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
94/550:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants.loc[1, 'Countries'] = 'United Kingdom'
wc_participants.Countries = wc_participants.Countries.str.title()
wc_participants.loc[3, 'Countries'] = 'United States of America'
wc_participants = wc_participants.drop(index=16).reset_index()['Countries']
wc_participants.loc[22] = 'South Korea'
wc_participants
94/551:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
wc_participants
94/552:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
94/553:
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_participants.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_participants.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'
wc_map
94/554:
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_participants.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'
wc_map
94/555:
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'
wc_map
94/556:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'
wc_map
94/557:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'
wc_participants
94/558:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'
wc_map
94/559:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'
94/560:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_participants:
    patch(ax, country)

plt.show()
94/561:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    patch(ax, country)

plt.show()
94/562: ax = countries[countries.continent == 'South America'].plot(color='lightgray', figsize=(10, 8))
94/563:
def patch(ax, country):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc='orangered', ec='black', alpha=.85, zorder=2))
94/564: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
94/565:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
94/566:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    patch(ax, country)

plt.show()
94/567:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    patch(ax, country)

plt.legend('a')
plt.show()
94/568:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    patch(ax, country)

plt.legend('')
plt.show()
94/569:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'Brazil':
        patch(ax, country, 'green')
    else:
        patch(ax, country, 'orangered')

plt.legend('')
plt.show()
94/570:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
94/571: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
94/572:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
94/573: legend =
94/574:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'Brazil':
        patch(ax, country, 'green')
    else:
        patch(ax, country, 'orangered')

plt.legend('')
plt.show()
94/575:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'Brazil':
        patch(ax, country, 'green')
    else:
        patch(ax, country, 'orangered')

plt.legend('')
plt.show()
94/576:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'Brazil':
        patch(ax, country, 'green')
    else:
        patch(ax, country, 'orangered')

plt.legend(['Brazil', 'ROW'])
plt.show()
94/577:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    else:
        patch(ax, country, 'orangered')

plt.legend(['Brazil'])
plt.show()
94/578:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lighskyblue')

plt.legend(['Brazil'])
plt.show()
94/579:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')

plt.legend(['Brazil'])
plt.show()
94/580:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'lightyellow')

plt.legend(['Brazil'])
plt.show()
94/581:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'lightsamon')

plt.legend(['Brazil'])
plt.show()
94/582:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'lightsalmon')

plt.legend(['Brazil'])
plt.show()
94/583:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.legend(['Brazil'])
plt.show()
94/584:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.legend(['Argentina', 'France'])
plt.show()
94/585:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.legend(['Argentina', 'France'], ['mediumblue', 'lightskyblue'])
plt.show()
94/586:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.legend(['Argentina', 'France'])
plt.show()
94/587:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')


plt.legend(['Argentina', 'France'])
plt.show()
94/588:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.legend(['Argentina', 'France', 'ROW'])
plt.show()
94/589:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.legend({'mediumblue': 'France'})
plt.show()
94/590:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.legend({'mediumblue': 'France', 'lightskyblue': 'Argentina', 'sandybrown': 'ROW'})
plt.show()
94/591:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.legend(labelcolor=['mediumblue', 'lightskyblue', 'sandybrown'], labels=['France', 'Argentina', 'ROW'])
plt.show()
94/592:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')

plt.legend(['France', 'Argentina'])
plt.show()
94/593:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.legend(['France', 'Argentina'])
plt.show()
94/594:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'sandybrown')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
94/595:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'mediumblue')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'bisque')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
94/596:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'bisque')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
94/597:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'ghostwhite')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
94/598:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'mistyrose')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
94/599:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'orangered')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
94/600:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'darkorange')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
94/601:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
94/602: matches
94/603:
wc_countries = pd.unique(matches.TEAM1.str.title())
wc_countries
94/604:
wc_countries = pd.unique(matches.TEAM1.str.title())
wc_countries = pd.DataFrame(wc_countries)
94/605:
wc_countries = pd.unique(matches.TEAM1.str.title())
wc_countries = pd.DataFrame(wc_countries)
wc_countries
94/606:
wc_countries = pd.unique(matches.TEAM1.str.title())
wc_countries = pd.DataFrame(wc_countries)
wc_countries.columns = ['Countries']
94/607:
wc_countries = pd.unique(matches.TEAM1.str.title())
wc_countries = pd.DataFrame(wc_countries)
wc_countries.columns = ['Countries']
wc_countries
94/608:
wc_countries = pd.unique(matches.TEAM1.str.title())
wc_countries = pd.DataFrame(wc_countries)
wc_countries.columns = ['Countries']
94/609:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2])
on_target
94/610:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target
94/611:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home
94/612:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_gome
94/613:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home
94/614:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum()
on_target_home
94/615:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True)
94/616:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True)
on_target_home
94/617: matches[matches.TEAM1 == 'Uruguay']
94/618: matches[matches.TEAM1 == 'URUGUAY']
94/619:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True)
on_target_home
94/620:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()
on_target_home
94/621:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()['Country', 'On target attempts']
on_target_home
94/622:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]
on_target_home
94/623:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]
94/624:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']
on_target_away = on_target_away.groupby('Country').sum().reset_index()
on_target_away = on_target_away.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]
on_target_away
94/625:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']
on_target_away = on_target_away.groupby('Country').sum().reset_index()
on_target_away = on_target_away.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]
on_target_away

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target
94/626:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']
on_target_away = on_target_away.groupby('Country').sum().reset_index()
on_target_away = on_target_away.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]
on_target_away

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index()
94/627:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']
on_target_away = on_target_away.groupby('Country').sum().reset_index()
on_target_away = on_target_away.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index()
on_target
94/628:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']
on_target_away = on_target_away.groupby('Country').sum().reset_index()
on_target_away = on_target_away.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
on_target
94/629:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']
on_target_away = on_target_away.groupby('Country').sum().reset_index()
on_target_away = on_target_away.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
94/630:
on_target.plot.barh()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
94/631:
on_target.plot.barh()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(50))
plt.show()
94/632:
on_target.plot.barh(x='On target attemps', y='Country')
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(50))
plt.show()
94/633:
on_target.plot.barh(x='On target attempts', y='Country')
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(50))
plt.show()
94/634:
on_target.plot.barh(y='On target attempts', x='Country')
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(50))
plt.show()
94/635:
on_target.plot.barh(x='Country', y='On target attempts')
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(49))
plt.show()
94/636:
on_target.plot.barh(x='Country', y='On target attempts')
plt.yticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(49))
plt.show()
94/637:
on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,6))
plt.yticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(49))
plt.show()
94/638:
on_target.plot.barh(x='Country', y='On target attempts', figsize=(15,6))
plt.yticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(49))
plt.show()
94/639:
on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,8))
plt.yticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(49))
plt.show()
94/640:
on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(49))
plt.show()
94/641:
on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/642:
on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/643:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='Country', y='On target attempts', 
    kind="bar", height=4, aspect=.6,
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/644:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='Country', y='On target attempts', 
    kind="bar", height=4, aspect=.6, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/645:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=4, aspect=.6, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/646:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=4, aspect=2, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/647:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=4, aspect=3, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/648:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=4, aspect=2.5, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/649:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=4, aspect=2, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/650:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=6, aspect=2, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/651:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/652:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, width=6, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/653:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, width=2, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/654:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, width=.5, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/655:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, width=1, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/656:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2.2, width=1, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/657:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, width=1, orient='h'
)

on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/658:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, width=1, orient='h'
)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=8)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/659:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, width=1, orient='h'
)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/660:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, width=1, orient='h'
)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=10)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/661:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5, aspect=2, width=1, orient='h'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=10)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/662:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=10)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/663:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/664:
sns.color_palette("rocket", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/665:
sns.color_palette("flare", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/666:
palette = sns.color_palette("flare", as_cmap=True)
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette=palette
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/667:

plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette=sns.color_palette("flare", as_cmap=True)
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/668:

plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/669:  palette=sns.color_palette("flare", as_cmap=True)
94/670:  sns.color_palette("flare", as_cmap=True)
94/671:

plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/672:

plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='mako'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/673:

plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='rocket'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/674:

plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='rocket_r'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/675:

plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='rocket'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/676:

plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/677:

plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
plot.despine(left=True)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/678:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

#on_target.plot.barh(x='Country', y='On target attempts', figsize=(10,7))
plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/679:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/680:
top10_on_target = on_target[on_target['On target attempts'] >= 16]
top10_on_target
94/681:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
94/682: matches
94/683:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g
94/684:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM_1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g
94/685:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g
94/686:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKSTEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g
95/1:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
95/2: matches = pd.read_csv("../dataset/matches.csv", header=0)
95/3: matches.head(2)
95/4:
def replace_spaces(s):
    return s.replace(" ", "_")
95/5:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
95/6:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
95/7:
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y')
matches.head()
95/8:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
95/9: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
95/10:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
95/11:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
95/12:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
95/13:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
95/14:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
95/15:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
95/16:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
95/17:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
95/18:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals', 'Date']
playoffs_goals
95/19:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
95/20:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
95/21:
arg_playoffs = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs = pd.concat([arg_playoffs, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs
95/22:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
95/23:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
95/24:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index()[['Round', 'Argentina goals']]
arg_playoffs_goals
95/25:
fra_playoffs = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs = pd.concat([fra_playoffs, playoffs_goals[playoffs_goals.Away == 'FRANCE']], axis=0).sort_values(by='Date', ascending=True)
fra_playoffs
95/26:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
95/27:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
95/28:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
95/29:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
95/30:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
95/31:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
95/32: argentina.sort_values(by='DATE', ascending=True)
95/33:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})
data
95/34:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
95/35: matches
95/36:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']
on_target_away = on_target_away.groupby('Country').sum().reset_index()
on_target_away = on_target_away.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
95/37:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
95/38:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKSTEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g
95/39:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
96/1:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
96/2: matches = pd.read_csv("../dataset/matches.csv", header=0)
96/3: matches.head(2)
96/4:
def replace_spaces(s):
    return s.replace(" ", "_")
96/5:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
96/6:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
96/7:
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y')
matches.head()
96/8:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
96/9: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
96/10:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
96/11:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
96/12:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
96/13:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
96/14:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
96/15:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
96/16:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
96/17:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
96/18:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals', 'Date']
playoffs_goals
96/19:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
96/20:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
96/21:
arg_playoffs = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs = pd.concat([arg_playoffs, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs
96/22:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
96/23:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
96/24:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index()[['Round', 'Argentina goals']]
arg_playoffs_goals
96/25:
fra_playoffs = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs = pd.concat([fra_playoffs, playoffs_goals[playoffs_goals.Away == 'FRANCE']], axis=0).sort_values(by='Date', ascending=True)
fra_playoffs
96/26:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
96/27:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
96/28:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
96/29:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
96/30:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
96/31:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
96/32: argentina.sort_values(by='DATE', ascending=True)
96/33:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})
data
96/34:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
96/35: matches
96/36:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']
on_target_home = on_target_home.groupby('Country').sum().reset_index()
on_target_home = on_target_home.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']
on_target_away = on_target_away.groupby('Country').sum().reset_index()
on_target_away = on_target_away.sort_values(by='On target attempts', ascending=True).reset_index()[['Country', 'On target attempts']]

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
96/37:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
96/38:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKSTEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g
96/39:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g
96/40:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g
96/41:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g
96/42:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clg_b], axis=0)
clb_g
96/43:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g
96/44:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g.value_counts()
96/45:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g.Country.value_counts()
96/46:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g
96/47:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0).reset_index()
clb_g
96/48:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0).reset_index()[['Country', 'Completed line breaks', 'Total attempts']]
clb_g
96/49: sns.scatterplot(data=clb_g, x="Completed line breaks", y="Total attempts")
96/50: sns.scatterplot(data=clb_g, x="Completed line breaks", y="Total attempts", hue='Country')
96/51:
sns.scatterplot(data=clb_g, x="Completed line breaks", y="Total attempts")
sns.scatterplot(data=clb_g[clb_g.Country == 'ARGENTINA'], x="Completed line breaks", y="Total attempts", hue='Country')
96/52:
sns.scatterplot(data=clb_g, x="Completed line breaks", y="Total attempts")

test = clb_g[clb_g.Country == 'ARGENTINA']
test = pd.concat([test, clb_g[clb_g.Country == 'BRAZIL']], axis=0)
sns.scatterplot(data=teste, x="Completed line breaks", y="Total attempts", hue='Country')
96/53:
sns.scatterplot(data=clb_g, x="Completed line breaks", y="Total attempts")

test = clb_g[clb_g.Country == 'ARGENTINA']
test = pd.concat([test, clb_g[clb_g.Country == 'BRAZIL']], axis=0)
sns.scatterplot(data=test, x="Completed line breaks", y="Total attempts", hue='Country')
96/54:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0).reset_index()[['Country', 'Completed line breaks', 'Total attempts']]
clb_g = clb_g.sort_values(by=['Total attempts', 'Completed line breaks'], ascending=False)
clb_g
96/55:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g = clb_g.sort_values(by=['Total attempts', 'Completed line breaks'], ascending=False).reset_index()
clb_g
96/56:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g = clb_g.sort_values(by=['Total attempts', 'Completed line breaks'], ascending=False).reset_index()[['Country', 'Completed line breaks', 'Total attempts']]
clb_g
96/57:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g = clb_g.sort_values(by=['Total attempts', 'Completed line breaks'], ascending=False).reset_index()[['Country', 'Completed line breaks', 'Total attempts']]
clb_g.head()
96/58:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g = clb_g.sort_values(by=['Total attempts', 'Completed line breaks'], ascending=False).reset_index()[['Country', 'Completed line breaks', 'Total attempts']]
clb_g.head()
96/59:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g = clb_g.sort_values(by=['Total attempts', 'Completed line breaks'], ascending=False).reset_index()[['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']]
clb_g.head()
96/60:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g = clb_g.sort_values(by=['Total attempts', 'Completed line breaks'], ascending=False).reset_index()[['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']]
clb_g.groupby('Country').sum()
96/61:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g.groupby('Country').sum().sort_values(by='Goals scored')
96/62:
home_clb_g = matches[['TEAM1', 'COMPLETED_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed line breaks', 'Total attempts', 'Goals scored']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g.groupby('Country').sum().sort_values(by='Goals scored', ascending=False)
96/63:
home_clb_g = matches[['TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1', 'TOTAL_ATTEMPTS_TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
home_clb_g.columns = ['Country', 'Completed defensive line breaks', 'Total attempts', 'Goals scored']
home_clb_g

away_clb_g = matches[['TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2', 'TOTAL_ATTEMPTS_TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
away_clb_g.columns = ['Country', 'Completed defensive line breaks', 'Total attempts', 'Goals scored']
away_clb_g

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g.groupby('Country').sum().sort_values(by='Goals scored', ascending=False)
96/64:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']


on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']


on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
96/65:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
96/66:
home_clb_g = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_clb_g.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_clb_g = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_clb_g.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g.groupby('Country').sum().sort_values(by='Goals scored', ascending=False)
96/67:
home_clb_g = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_clb_g.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_clb_g = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_clb_g.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

clb_g = pd.concat([home_clb_g, away_clb_g], axis=0)
clb_g.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=False)
96/68:
sns.scatterplot(data=clb_g, x="Completed defensive line breaks", y="Receptions between midfield and defensive lines")

test = clb_g[clb_g.Country == 'ARGENTINA']
test = pd.concat([test, clb_g[clb_g.Country == 'BRAZIL']], axis=0)
sns.scatterplot(data=test, x="Completed line breaks", y="Total attempts", hue='Country')
96/69:
sns.scatterplot(data=clb_g, x="Completed defensive line breaks", y="Receptions between midfield and defensive lines")

#test = clb_g[clb_g.Country == 'ARGENTINA']
#test = pd.concat([test, clb_g[clb_g.Country == 'BRAZIL']], axis=0)
#sns.scatterplot(data=test, x="Completed line breaks", y="Total attempts", hue='Country')
96/70:
sns.scatterplot(data=clb_g, x="Completed defensive line breaks", y="Receptions between midfield and defensive lines")
clb_g
#test = clb_g[clb_g.Country == 'ARGENTINA']
#test = pd.concat([test, clb_g[clb_g.Country == 'BRAZIL']], axis=0)
#sns.scatterplot(data=test, x="Completed line breaks", y="Total attempts", hue='Country')
96/71:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=False)
96/72:
sns.catplot(
    data=rbm_clb, x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'],
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
96/73:
sns.catplot(
    data=rbm_clb, x='Country', y='Receptions between midfield and defensive lines',
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
96/74:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=False)
96/75:
sns.catplot(
    data=rbm_clb, x='Country', y='Receptions between midfield and defensive lines',
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
96/76:
sns.catplot(
    data=rbm_clb, y='Country', x='Receptions between midfield and defensive lines',
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
96/77:
sns.catplot(
    data=rbm_clb, y='Country', x='Receptions between midfield and defensive lines',
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
96/78:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=False)
rbm_clb
96/79:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=False).reset_index()
rbm_clb
96/80:
sns.catplot(
    data=rbm_clb, y='Country', x='Receptions between midfield and defensive lines',
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
96/81:
sns.catplot(
    data=rbm_clb, y='Country', x=['Receptions between midfield and defensive lines'],
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
96/82: rbm_clb.plot.bar(x='Country', y='Receptions between midfield and defensive lines')
96/83: rbm_clb.plot.bar(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'])
96/84: rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'])
96/85:
rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 6))
plt.
96/86: rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 6))
96/87: rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 10))
96/88:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=True).reset_index()
rbm_clb
96/89: rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 10))
96/90: rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 12))
96/91: rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 8))
96/92: sns.heatmap(rbm_clb)
96/93: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 8))
96/94:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=True).reset_index()

hits_rbm_clb = rbm_clb[rbm_clb['Receptions between midfield and defensive lines'] >= 52]
96/95: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 8))
96/96: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(8, 8))
96/97: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 8))
96/98: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(8, 8))
96/99: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 6))
96/100: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 7))
96/101: matches[matches.TEAM1 == 'BRAZIL']
96/102: matches[matches.TEAM2 == 'BRAZIL']
96/103:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
96/104:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
96/105:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=True).reset_index()

hits_rbm_clb = rbm_clb[rbm_clb['Receptions between midfield and defensive lines'] >= 52]
96/106: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 7))
96/107:
quarter_finals = matches[matches.CATEGORY == 'Quarter-finals']
quarter_finals
96/108:
quarter_finals = matches[matches.CATEGORY == 'Quarter finals']
quarter_finals
96/109:
quarter_finals = matches[matches.CATEGORY == 'Quarter Finals']
quarter_finals
96/110:
quarter_finals = matches[matches.CATEGORY == 'Quarter-Finals']
quarter_finals
96/111:
quarter_finals = matches[matches.CATEGORY == 'Quarter final']
quarter_finals
96/112:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']
quarter_finals
96/113:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']
qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1']]
qf_passes_home
96/114:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']
qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1']]
qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2']]
qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0)
qf_passes
96/115:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0)
qf_passes
96/116:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes')
qf_passes
96/117:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=False)
qf_passes
96/118:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=False)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completes']
qf_passes
96/119:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=False)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes
96/120:
plot = sns.catplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
96/121:
plot = sns.catplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=1, width=1, orient='h', palette='flare_r'
)
96/122:
plot = sns.catplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)
96/123:
plot = sns.catplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.5, orient='h', palette='flare_r'
)
96/124:
plot = sns.catplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.75, orient='h', palette='flare_r'
)
96/125:
plot = sns.catplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.5, orient='h', palette='flare_r'
)
96/126:
plot = sns.catplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.5, orient='h', palette='flare_r'
)

plot = sns.catplot(
    data=qf_passes, x='Passes not completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.5, orient='h', palette='flare_r'
)
96/127:
plot = sns.catplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.5, orient='h', palette='flare_r'
)

sns.catplot(
    data=qf_passes, x='Passes not completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.5, orient='h', palette='flare_r', ax=plot
)
96/128:
plot = sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.5, orient='h', palette='flare_r'
)

sns.barplot(
    data=qf_passes, x='Passes not completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.5, orient='h', palette='flare_r', ax=plot
)
96/129:
plot = sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", height=5.5, aspect=2, width=.5, orient='h', palette='flare_r'
)
96/130:
plot = sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', 
    kind="bar", aspect=2, width=.5, orient='h', palette='flare_r'
)
96/131:
plot = sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', aspect=2, width=.5, orient='h', palette='flare_r'
)
96/132:
plot = sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', width=.5, orient='h', palette='flare_r'
)
96/133:
plot = sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', width=.5, orient='h', palette='flare_r'
)

plot = sns.barplot(
    data=qf_passes, x='Passes not completed', y='Country', width=.5, orient='h', palette='flare_r'
)
96/134:
plot = sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', width=.5, orient='h', palette='flare_r'
)

sns.barplot(
    data=qf_passes, x='Passes not completed', y='Country', width=.5, orient='h', palette='flare_r'
)
96/135:
plot = sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', width=.5, orient='h', palette='flare_r'
)
96/136:
fig, ax = plt.subplots()

plot = sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', width=.5, orient='h', palette='flare_r', ax=ax
)
96/137:
fig, ax = plt.subplots()

sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', width=.5, orient='h', palette='flare_r', ax=ax
)

sns.barplot(
    data=qf_passes, x='Passes not completed', y='Country', width=.5, orient='h', palette='flare_r', ax=ax
)
96/138:
fig, ax = plt.subplots()

sns.barplot(
    data=qf_passes, x='Passes completed', y='Country', width=.5, orient='h', palette='flare_r', ax=ax
)
96/139: qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(10, 7))
96/140: qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(10, 7), stacked=True)
96/141:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes
96/142: qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(10, 7), stacked=True)
96/143: qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 7), stacked=True)
96/144: qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
96/145:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 10))
96/146:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 10))
plt.show()
96/147:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.show()
96/148:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes
96/149:
matches.DATE = matches.DATE + matches.HOUR
#matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y')
matches.head()
96/150:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
96/151: matches = pd.read_csv("../dataset/matches.csv", header=0)
96/152: matches.head(2)
96/153:
def replace_spaces(s):
    return s.replace(" ", "_")
96/154:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
96/155:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
96/156:
matches.DATE = matches.DATE + matches.HOUR
#matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y')
matches.head()
96/157:
matches.DATE = matches.DATE + ' ' + matches.HOUR
#matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y')
matches.head()
96/158:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
96/159: matches = pd.read_csv("../dataset/matches.csv", header=0)
96/160: matches.head(2)
96/161:
def replace_spaces(s):
    return s.replace(" ", "_")
96/162:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
96/163:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
96/164:
matches.DATE = matches.DATE + ' ' + matches.HOUR
#matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y')
matches.head()
96/165:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
96/166:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
96/167: matches = pd.read_csv("../dataset/matches.csv", header=0)
96/168: matches.head(2)
96/169:
def replace_spaces(s):
    return s.replace(" ", "_")
96/170:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
96/171:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
96/172:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
96/173:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
96/174:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
96/175: matches = pd.read_csv("../dataset/matches.csv", header=0)
96/176: matches.head(2)
96/177:
def replace_spaces(s):
    return s.replace(" ", "_")
96/178:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
96/179:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
96/180:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
96/181:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
96/182: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
96/183:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
96/184:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
96/185:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
96/186:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
96/187:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
96/188:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
96/189:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
96/190:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
96/191:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals', 'Date']
playoffs_goals
96/192:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
96/193:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
96/194:
arg_playoffs = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs = pd.concat([arg_playoffs, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs
96/195:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
96/196:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
96/197:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index()[['Round', 'Argentina goals']]
arg_playoffs_goals
96/198:
fra_playoffs = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs = pd.concat([fra_playoffs, playoffs_goals[playoffs_goals.Away == 'FRANCE']], axis=0).sort_values(by='Date', ascending=True)
fra_playoffs
96/199:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
96/200:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
96/201:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
96/202:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
96/203:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
96/204:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
96/205: argentina.sort_values(by='DATE', ascending=True)
96/206:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})
data
96/207:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
96/208:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
96/209:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
96/210:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=True).reset_index()

hits_rbm_clb = rbm_clb[rbm_clb['Receptions between midfield and defensive lines'] >= 52]
96/211: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 7))
96/212:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes
96/213:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.show()
96/214:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100
qf_passes
96/215:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100
date = qf_passes.pop('Date')
date
96/216:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100
date = qf_passes.pop('Date')
qf_passes['Date'] = date
96/217:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100
date = qf_passes.pop('Date')
qf_passes['Date'] = date
qf_passes
96/218:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date
qf_passes
96/219:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.show()
96/220:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.6,
)
96/221:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1,
)
96/222:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1, sharex=False
)
96/223:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1, sharex=False, palette='mako'
)
96/224:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1, sharex=False, palette='mako_r'
)
96/225:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1, sharex=False, palette='flare_r'
)
96/226:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1, sharex=False, palette='flare'
)
96/227:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1, sharex=False, color=".9"
)
96/228:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1, sharex=False, color="lightskyblue"
)
96/229:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1, sharex=False, color=["lightskyblue", 'red']
)
96/230:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=1, sharex=False,
)
96/231:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=2, sharex=False,
)
96/232:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.5, sharex=False,
)
96/233:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.76, sharex=False,
)
96/234:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.75, sharex=False,
)
96/235:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, sharex=False,
)
96/236:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=2, sharex=False,
)
96/237:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=1, sharex=False,
)
96/238:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False,
)
96/239:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False,
)

plt.yticks(range(0, 90, 5))
96/240:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False,
)

plt.yticks(range(0, 90, 5))
plt.show()
96/241:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False,
)

plt.yticks(range(0, 90, 5))
plt.xlabel('')
plt.show()
96/242:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False,
)

plt.yticks(range(0, 90, 5))
plt.ylabel('')
plt.show()
96/243:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False,
)

plt.yticks(range(0, 90, 5))
plt.xlabel('')
plt.show()
96/244:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

plt.yticks(range(0, 90, 5))
plt.xlabel('')
plt.show()
96/245:
sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

plt.yticks(range(0, 91, 5))
plt.xlabel('')
plt.show()
96/246:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

g.set_axis_labels("", "Pass accuracy (%)")


plt.yticks(range(0, 91, 5))
plt.show()
96/247:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

plot.set_axis_labels("", "Pass accuracy (%)")


plt.yticks(range(0, 91, 5))
plt.show()
96/248:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("a")
g.set(ylim=(0, 1))
g.despine(left=True)

plt.yticks(range(0, 91, 5))
plt.show()
96/249:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("{x_name}")
g.set(ylim=(0, 1))
g.despine(left=True)

plt.yticks(range(0, 91, 5))
plt.show()
96/250:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("{col_name}")
g.set(ylim=(0, 1))
g.despine(left=True)

plt.yticks(range(0, 91, 5))
plt.show()
96/251:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes
96/252:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date[:16]
qf_passes
96/253:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[:16]
qf_passes
96/254:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
96/255:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:5]
qf_passes
96/256:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:10]
qf_passes
96/257:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:12]
qf_passes
96/258:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:13]
qf_passes
96/259:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:14]
qf_passes
96/260:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:15]
qf_passes
96/261:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
96/262:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.show()
96/263:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("{col_name}")
g.set(ylim=(0, 1))
g.despine(left=True)

plt.yticks(range(0, 91, 5))
plt.show()
96/264:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")
g.set(ylim=(0, 1))
g.despine(left=True)

plt.yticks(range(0, 91, 5))
plt.show()
96/265:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, margin_titles=False
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/266:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='orangered'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/267:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='orangered blue'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/268:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color=['orangered', 'blue']
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/269:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color={'orangered'}
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/270:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color={'Argentina': 'orangered'}
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/271:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='orangered'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/272:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='mako'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/273:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/274:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='mako'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/275:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='flare'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/276:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='flare_r'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/277:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='rocket'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/278:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='rocket_r'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/279:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='crest'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/280:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='crest_r'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/281:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='magma'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/282:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/283:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis_r'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/284:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/285:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes = qf_passes.sort_values(by='Passes accuracy')
qf_passes
96/286:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.show()
96/287:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
96/288:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.show()
96/289:
qf_passes = qf_passes.sort_values(by='Passes accuracy')
qf_passes
96/290:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
96/291:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.show()
96/292:
qf_passes = qf_passes.sort_values(by='Passes accuracy')
qf_passes
96/293:
qf_passes = qf_passes.sort_values(by='Passes accuracy', ascending=False)
qf_passes
96/294:
plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/295:
qf_passes = qf_passes.sort_values(by='Date', ascending=False)

plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/296:
qf_passes = qf_passes.sort_values(by='Date', ascending=True)

plot = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

plot.set_axis_labels("", "Pass accuracy (%)")
plot.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/297:
qf_passes = qf_passes.sort_values(by='Date', ascending=True)

g = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

g.set_axis_labels("", "Pass accuracy (%)")
g.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
96/298:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.title('Total passes during quarter finals')
plt.show()
96/299:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.ylabel('')
plt.title('Total passes during quarter finals')
plt.show()
96/300:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
96/301:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.ylabel('')
plt.title('Total passes during quarter finals')
plt.show()
96/302:
qf_passes = qf_passes.sort_values(by='Passes accuracy', ascending=False)
qf_passes
96/303:
qf_passes = qf_passes.sort_values(by='Date', ascending=True)

g = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

g.set_axis_labels("", "Pass accuracy (%)")
g.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
97/1:
goals.plot.scatter()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
97/2:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
97/3:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
97/4:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
97/5: matches = pd.read_csv("../dataset/matches.csv", header=0)
97/6: matches.head(2)
97/7:
def replace_spaces(s):
    return s.replace(" ", "_")
97/8:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
97/9:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
97/10:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
97/11:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
97/12: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
97/13:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
97/14:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
97/15:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
97/16:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
97/17:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
97/18:
goals.plot.scatter()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
97/19:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
97/20:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
playoffs
98/1:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
98/2: matches = pd.read_csv("../dataset/matches.csv", header=0)
98/3: matches.head(2)
98/4:
def replace_spaces(s):
    return s.replace(" ", "_")
98/5:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
98/6:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
98/7:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
98/8:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
98/9: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
98/10:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
98/11:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
98/12:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
98/13:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
98/14:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
98/15:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
98/16:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
98/17:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
98/18:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals', 'Date']
playoffs_goals
98/19:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
98/20:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
98/21:
arg_playoffs = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs = pd.concat([arg_playoffs, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs
98/22:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
98/23:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
98/24:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index()[['Round', 'Argentina goals']]
arg_playoffs_goals
98/25:
fra_playoffs = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs = pd.concat([fra_playoffs, playoffs_goals[playoffs_goals.Away == 'FRANCE']], axis=0).sort_values(by='Date', ascending=True)
fra_playoffs
98/26:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
98/27:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
98/28:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
98/29:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
98/30:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
98/31:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
98/32: argentina.sort_values(by='DATE', ascending=True)
98/33:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})
data
98/34:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
98/35:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
98/36:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
98/37:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=True).reset_index()

hits_rbm_clb = rbm_clb[rbm_clb['Receptions between midfield and defensive lines'] >= 52]
98/38: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 7))
98/39:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
playoffs = playoffs[['']]
98/40:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.ylabel('')
plt.title('Total passes during quarter finals')
plt.show()
98/41:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
98/42:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.ylabel('')
plt.title('Total passes during quarter finals')
plt.show()
98/43:
qf_passes = qf_passes.sort_values(by='Passes accuracy', ascending=False)
qf_passes
98/44:
qf_passes = qf_passes.sort_values(by='Date', ascending=True)

g = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

g.set_axis_labels("", "Pass accuracy (%)")
g.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
98/45:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
playoffs
98/46:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/47:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes.Round.value_counts()
98/48:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/49:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index()
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/50:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index().drop('index')
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/51:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index()
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/52:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index()
po_passes = po_passes.drop('index')
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/53:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index()

po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/54:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index()
po_passes = po_passes[col for col in po_passes.columns col != 'index']
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/55:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index()
po_passes = po_passes[col if col != 'index' for col in po_passes.columns]
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/56:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index()
po_passes = po_passes[col if col != 'index' else continue for col in po_passes.columns]
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes
98/57:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index()
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes.columns
98/58:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True).reset_index()
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

po_passes.columns = po_passes.columns[1:]
98/59:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100
98/60:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100
po_passes
98/61:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
po_passes
98/62:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=True)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes
98/63:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0).sort_values(by='Passes', ascending=False)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes
98/64:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes
98/65:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100
po_passes = po_passes.sort_values(by='Passes accuracy', ascending=False)

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes
98/66: po_passes.plot.scatter(x='Passes', y='Passes completed')
98/67: po_passes.plot.scatter(x='Passes', y='Passes accuracy')
98/68: sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy")
98/69: sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round')
98/70: sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', pallete='mako')
98/71: sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='mako')
98/72: sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='flare')
98/73: sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='flare_r')
98/74: sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='viridis')
98/75:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='viridis')
plt.xticks(range(0, 1042, 50))
98/76:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='viridis')
plt.xticks(range(0, 1042, 50))
plt.show()
98/77:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='viridis')
plt.xticks(range(0, 1042, 100))
plt.show()
98/78: sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='viridis')
98/79:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='viridis')
plt.ylabel('Passes accuracy (%)')
98/80:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='viridis')
plt.ylabel('Passes accuracy (%)')
plt.title('Country pass accuracy per match throughout playoffs')
98/81:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='viridis')
plt.ylabel('Passes accuracy (%)')
plt.title('Country pass accuracy per match round throughout playoffs')
98/82:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round', palette='viridis')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/83:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/84:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100
po_passes = po_passes.sort_values(by='Passes accuracy', ascending=False)

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes.head(10)
98/85:
passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.head(10)
98/86:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.head()
98/87:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes
98/88:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.head(20)
98/89:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.head()
98/90:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts_passes = passes

for first in first_places:
    firsts_passes = passes[passes.TEAM1 == first]
    firsts_passes = pd.contat([firsts_passes, passes[passes.TEAM2 == first]], axis=0)

firsts_passes
98/91:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts_passes = matches

for first in first_places:
    firsts_passes = passes[passes.TEAM1 == first]
    firsts_passes = pd.contat([firsts_passes, passes[passes.TEAM2 == first]], axis=0)

firsts_passes
98/92:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts_passes = matches

for first in first_places:
    firsts_passes = matches[matches.TEAM1 == first]
    firsts_passes = pd.contat([firsts_passes, matches[matches.TEAM2 == first]], axis=0)

firsts_passes
98/93:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts_passes = matches

for first in first_places:
    firsts_passes = matches[matches.TEAM1 == first]
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM2 == first]], axis=0)

firsts_passes
98/94:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts_passes = matches

for first in first_places:
    firsts_passes = matches[matches.TEAM1 == first]
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM2 == first]], axis=0)

firsts_passes.shape
98/95:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts_passes = matches

for first in first_places:
    firsts_passes = matches[matches.TEAM1 == first]
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM2 == first]], axis=0)

firsts_passes
98/96:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']

for first in first_places:
    firsts_passes = matches[matches.TEAM1 == first]
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM2 == first]], axis=0)

firsts_passes
98/97:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']

for first in first_places:
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM1 == first], axis=0)
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM2 == first]], axis=0)

firsts_passes
98/98:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']

for first in first_places:
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM1 == first]], axis=0)
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM2 == first]], axis=0)

firsts_passes
98/99:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']

for first in first_places:
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM1 == first]], axis=0)
    firsts_passes = pd.concat([firsts_passes, matches[matches.TEAM2 == first]], axis=0)

firsts_passes.TEAM1.value_counts()
98/100:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']

for country in first_places:
    first_home = matches[matches.TEAM1 == country]]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first, first_home, first_away], axis=0)


first
98/101:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first, first_home, first_away], axis=0)


first
98/102:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first, first_home, first_away], axis=0)

first
98/103:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)

first
98/104:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)

firsts
98/105:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)

firsts.shape
98/106:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts
98/107:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts.Country.value_counts()
98/108:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts
98/109:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

firsts
98/110:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts
98/111:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts.reset_index()
98/112:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts.reset_index(drop=True)
98/113:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts
98/114:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts
98/115:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts
98/116:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts
98/117:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.tail()
98/118:
sns.scatterplot(data=firsts, x="Passes", y="Passes accuracy", hue='Country')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/119:
sns.lineplot(data=firsts, x="Passes", y="Passes accuracy", hue='Country')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/120:
sns.distlpot(data=firsts, x="Passes", y="Passes accuracy", hue='Country')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/121:
sns.distplot(data=firsts, x="Passes", y="Passes accuracy", hue='Country')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/122:
sns.displot(data=firsts, x="Passes", y="Passes accuracy", hue='Country')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/123:
sns.displot(data=firsts, x="Passes", y="Passes accuracy", hue='Country', palette='mako')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/124:
sns.displot(data=firsts, x="Passes", y="Passes accuracy", hue='Country', palette='flare')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/125:
sns.displot(data=firsts, x="Passes", y="Passes accuracy", hue='Country', palette='flare_r')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
98/126:
sns.displot(data=firsts, x="Passes", y="Passes accuracy", hue='Country', palette='flare_r')
plt.ylabel('Passes accuracy (%)')
plt.xticks(range(900))
plt.title('Countries pass accuracy per match round throughout playoffs')
98/127:
sns.displot(data=firsts, x="Passes", y="Passes accuracy", hue='Country', palette='flare_r')
plt.ylabel('Passes accuracy (%)')
plt.xticks(range(0, 900, 100))
plt.title('Countries pass accuracy per match round throughout playoffs')
98/128:
sns.displot(data=firsts, x="Passes", y="Passes accuracy", hue='Country', palette='flare_r')
plt.ylabel('Passes accuracy (%)')
plt.xticks(range(0, 900, 100))
plt.yticks(range(0, 95, 5))
plt.title('Countries pass accuracy per match round throughout playoffs')
98/129:
sns.displot(data=firsts, x="Passes", y="Passes accuracy", hue='Country', palette='flare_r')
plt.ylabel('Passes accuracy (%)')
plt.xticks(range(0, 900, 100))
plt.yticks(range(0, 95, 10))
plt.title('Countries pass accuracy per match round throughout playoffs')
98/130:
sns.displot(data=firsts, x="Passes", y="Passes accuracy", hue='Country', palette='flare_r')
plt.ylabel('Passes accuracy (%)')
plt.xticks(range(0, 900, 100))
plt.title('Countries pass accuracy per match round throughout playoffs')
98/131:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts
98/132: sns.heatmap(data=firsts)
98/133:
firsts_heatmap = firsts.pivot('Country', 'Round', 'Passea accuracy')
firsts_heatmap
98/134:
firsts_heatmap = firsts.pivot('Country', 'Round', 'Passes accuracy')
firsts_heatmap
98/135:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts
98/136:
firsts_heatmap = firsts.pivot('Country', 'Round', 'Passes accuracy')
firsts_heatmap
98/137:
firsts_heatmap = firsts.pivot(index='Country', columns='Round', values='Passes accuracy')
firsts_heatmap
98/138:
firsts_heatmap = firsts.pivot(index='Round', columns='Country', values='Passes accuracy')
firsts_heatmap
98/139: firsts
98/140:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap
98/141:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot('Round', 'Country', 'Passes accuracy')
firsts_heatmap
98/142:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot(index='Round', columns='Country', values='Passes accuracy')
firsts_heatmap
98/143:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap
98/144:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.groupby('Round').sum()
firsts_heatmap/
98/145:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.groupby('Round').sum()
firsts_heatmap
98/146:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap
98/147:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap)
firsts_heatmap
98/148:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts[firsts.Round.str.startswith('Group')] = 'Group'
98/149:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts[firsts.Round.str.startswith('Group')] = 'Group'
firsts
98/150:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts[firsts.Round.str.startswith('Group')].Round = 'Group'
firsts
98/151:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts[firsts.Round.str.startswith('Group')].Round = firsts.Round.str[1:]
firsts
98/152:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts[firsts.Round.str.startswith('Group')] = 'Group'
firsts
98/153:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts[firsts.Round.str.startswith('Group')].loc[:, 'Round'] = 'Group'
firsts
98/154:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts[firsts.Round.str.startswith('Group')].loc[:, 'Round'] = '222222'
firsts
98/155:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts[firsts.Round.str.startswith('Group')].Round = '222222'
firsts
98/156:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.loc[firsts.Round.str.startswith('Group'), 'Round'] = 'Group'
firsts
98/157:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.loc[firsts.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
firsts
98/158:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap)
firsts_heatmap
98/159:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.loc[firsts.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
firsts.head()
98/160:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.loc[firsts.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
firsts.head()
98/161:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.loc[firsts.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
firsts.head()
98/162:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap)
98/163:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f")
plt.
98/164:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f")
98/165:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.loc[firsts.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
firsts.head()
98/166:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f")
98/167:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Date', ascending=True)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.loc[firsts.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
firsts.head()
98/168:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.loc[firsts.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
firsts.head()
98/169:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f")
98/170:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")
98/171:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest_f")
98/172:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")
98/173:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")
98/174:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

firsts_heatmap
98/175:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

cols = firsts_heatmap.columns
cols
98/176:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

cols = firsts_heatmap.columns.to_list()
cols
98/177:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

new_cols = ['Groups Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Playoff for third place', 'Final']
firsts_heatmap = firsts_heatmap[new_cols]
98/178:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

new_cols = ['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']
firsts_heatmap = firsts_heatmap[new_cols]
98/179:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

new_cols = ['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']
firsts_heatmap = firsts_heatmap[new_cols]
firsts_heatmap
98/180:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table('Country', 'Round', 'Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")


firsts_heatmap
98/181:
firsts_heatmap = firsts[['Country', 'Round', 'Passes accuracy']]
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")


firsts_heatmap
98/182:
firsts_heatmap = firsts_heatmap.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")


firsts_heatmap
98/183:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")


firsts_heatmap
98/184:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")


firsts_heatmap = firsts_heatmap.fillna(0)
firsts_heatmap
98/185:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']].fillna(0)
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

firsts_heatmap
98/186:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']].fillna(50)
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

firsts_heatmap
98/187:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']])
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

firsts_heatmap
98/188:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")

firsts_heatmap
98/189:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")
plt.title("Pass accuracy for group stages' first places throughout their campaign")
98/190:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")
plt.title("Pass accuracy for group stages' first places throughout their campaign")
plt.xlabel('')
plt.ylabel('')
98/191:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")
plt.title("Pass accuracy for group stages' first places throughout their campaign")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/192:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=".1f", cmap="crest")
plt.title("Pass accuracy for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/193:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt=''.1f', cmap='crest', linewidth=.5)
plt.title("Pass accuracy for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/194:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5)
plt.title("Pass accuracy for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/195:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/196:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']].fillna('NaN')
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/197:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/198:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.legend('(%)')
plt.show()
98/199:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/200:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/201:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.head()
98/202: passes.plot.scatter(x='Passes', y='Passes accuracy')
98/203: passes.plot.scatter(x='Passes', y='Passes accuracy', hue='Round')
98/204: sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
98/205:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.loc[passes.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
passes.head()
98/206: sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
98/207:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first_home = matches[matches.TEAM1 == country]
    first_home = first_home[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
    first_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first_away = matches[matches.TEAM2 == country]
    first_away = first_away[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
    first_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']
    
    first = pd.concat([first_home, first_away], axis=0)
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.head()
98/208:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/209:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first = passes[passes.Country == country]
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.head()
98/210:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first = passes[passes.Country == country]
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts
98/211:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first = passes[passes.Country == country]
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.Country.value_counts()
98/212:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first = passes[passes.Country == country]
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.head()
98/213:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Play-off for third place', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/214:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Final']]
sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/215:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True)
firsts_lineplot
98/216:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot
98/217:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy')
98/218:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
98/219:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country', height=4)
98/220:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
firsts_lineplot
98/221:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lieplot.Date.str[0:10]
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
firsts_lineplot
98/222:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[0:10]
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
firsts_lineplot
98/223:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[4:10]
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
firsts_lineplot
98/224:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
firsts_lineplot
98/225:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
g = sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
g.fig.set_figwidth(12)
g.fig.set_figheight(10)
98/226:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
g = sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
g.set_figwidth(12)
g.fig.set_figheight(10)
98/227:
firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
g = sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
g.set_figwidth(12)
g.set_figheight(10)
98/228:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
98/229:
fig, ax = plt.subplots(figsize=(12, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
98/230:
fig, ax = plt.subplots(figsize=(14, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Date', y='Passes accuracy', hue='Country')
98/231:
fig, ax = plt.subplots(figsize=(14, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
98/232:
fig, ax = plt.subplots(figsize=(14, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='upper right')
98/233:
fig, ax = plt.subplots(figsize=(14, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='down right')
98/234:
fig, ax = plt.subplots(figsize=(14, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
98/235:
fig, ax = plt.subplots(figsize=(14, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Final']]
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
98/236:
fig, ax = plt.subplots(figsize=(14, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
98/237:
fig, ax = plt.subplots(figsize=(14, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
98/238:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
98/239:
fig, ax = plt.subplots(figsize=(8, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
98/240:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
98/241:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
plt.yticks(range(70, 92))
98/242:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
plt.yticks(range(70, 92))
plt.show()
98/243:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
plt.show()
98/244:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='mako')
plt.legend(loc='lower right')
plt.show()
98/245:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='flare')
plt.legend(loc='lower right')
plt.show()
98/246:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='magma')
plt.legend(loc='lower right')
plt.show()
98/247:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='vidridis')
plt.legend(loc='lower right')
plt.show()
98/248:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='rocket')
plt.legend(loc='lower right')
plt.show()
98/249:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='rocket')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.show()
98/250:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='rocket')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel(1)
plt.show()
98/251:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']
firsts_lineplot.Date = firsts_lineplot.Date.str[5:10]
sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='rocket')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.show()
98/252:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='rocket')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title
plt.show()
98/253:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='rocket')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/254:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/255:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', legend='brief')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/256:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', legend='full')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/257:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', legend='full', style='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/258:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', palette='mako', style='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/259:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', style='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/260:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', style='Country', units='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/261:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', style='Country', size='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/262:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', size='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/263:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=False).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', size='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/264:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', size='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/265:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/266:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round', palette='mako')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/267:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round', palette='mako_r')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/268:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round', palette='flare')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/269:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round', palette='magma')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/270:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round', palette='magma_r')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/271:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round', palette='mako_r')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/272:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round', palette='rocket')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/273:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round', palette='rocket_r')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/274:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
98/275:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
98/276:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', size='Country', dashes=0)
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/277:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', style='Country', dashes=True)
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/278:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', style='Country', dashes=True, markers=['o','o','o'])
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/279:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', size='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/280:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first = passes[passes.Country == country]
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.head()
98/281:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Final']]

sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/282:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', size='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/283:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Final']]

sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
98/284:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', size='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
98/285:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100
po_passes = po_passes.sort_values(by='Passes accuracy', ascending=False)

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes.head(10)
99/1:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
99/2: matches = pd.read_csv("../dataset/matches.csv", header=0)
99/3: matches.head(2)
99/4:
def replace_spaces(s):
    return s.replace(" ", "_")
99/5:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/6:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
99/7:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
99/8:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
99/9: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
99/10:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
99/11:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
99/12:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
99/13:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
99/14:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
99/15:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
99/16:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
99/17:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
99/18:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals', 'Date']
playoffs_goals
99/19:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
99/20:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
99/21:
arg_playoffs = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs = pd.concat([arg_playoffs, playoffs_goals[playoffs_goals.Away == 'ARGENTINA']], axis=0).sort_values(by='Date', ascending=True)
arg_playoffs
99/22:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
99/23:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
99/24:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index()[['Round', 'Argentina goals']]
arg_playoffs_goals
99/25:
fra_playoffs = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs = pd.concat([fra_playoffs, playoffs_goals[playoffs_goals.Away == 'FRANCE']], axis=0).sort_values(by='Date', ascending=True)
fra_playoffs
99/26:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
99/27:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
99/28:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
99/29:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
99/30:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)
plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
99/31:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
99/32: argentina.sort_values(by='DATE', ascending=True)
99/33:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home = argentina_home.rename(columns={'TEAM2': 'OPPONENT', 'POSSESSION_TEAM1': 'POSSESSION', 'POSSESSION_TEAM2': 'DEFENDING'})

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away = argentina_away.rename(columns={'TEAM1': 'OPPONENT', 'POSSESSION_TEAM2': 'POSSESSION', 'POSSESSION_TEAM1': 'DEFENDING'})

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})
data
99/34:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
99/35:
on_target = pd.concat([matches.TEAM1, matches.TEAM2, matches.ON_TARGET_ATTEMPTS_TEAM1, matches.ON_TARGET_ATTEMPTS_TEAM2], axis=1)
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
99/36:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
99/37:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=True).reset_index()

hits_rbm_clb = rbm_clb[rbm_clb['Receptions between midfield and defensive lines'] >= 52]
99/38: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 7))
99/39:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.loc[passes.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
passes.head()
99/40:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
99/41:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first = passes[passes.Country == country]
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.head()
99/42:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Final']]

sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
99/43:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', size='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
99/44:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100
po_passes = po_passes.sort_values(by='Passes accuracy', ascending=False)

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes.head(10)
99/45:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
99/46:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
99/47:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.ylabel('')
plt.title('Total passes during quarter finals')
plt.show()
99/48:
qf_passes = qf_passes.sort_values(by='Passes accuracy', ascending=False)
qf_passes
99/49:
qf_passes = qf_passes.sort_values(by='Date', ascending=True)

g = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

g.set_axis_labels("", "Pass accuracy (%)")
g.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
99/50:
def country_playoff_matches(playoffs_goals, country):
    country_playoffs = playoffs_goals[playoffs_goals.Home == country]
    country_playoffs = pd.concat([country_playoffs, playoffs_goals[playoffs_goals.Away == country]], axis=0).sort_values(by='Date', ascending=True)
    
    return country_playoffs
99/51:
arg_playoffs = country_playoff_matches(playoffs_goals, 'ARGENTINA')
arg_playoffs
99/52:
def get_country_matches(data, country, situation, columns):
    country_matches = data[data['situation'] == country]
    country_matches = country_matches[columns]
    return country_matches
99/53:
arg_playoffs = country_playoff_matches(playoffs_goals, 'ARGENTINA')
arg_playoffs
99/54:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']
arg_playoffs_home
99/55:
arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
arg_playoffs_away
99/56:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index()[['Round', 'Argentina goals']]
arg_playoffs_goals
99/57:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index(drop=True)
arg_playoffs_goals
99/58:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']
fra_playoffs_home
99/59:
fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
fra_playoffs_away
99/60:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index()[['Round', 'France goals']]
fra_playoffs_goals
99/61:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index(drop=True)
fra_playoffs_goals
99/62:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
99/63:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)

plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
99/64:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING']

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING']

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})
data
99/65:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data = data.rename(columns={'POSSESSION_IN_CONTEST': 'IN CONTEST'})
data
99/66:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=False)
data
99/67:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

data = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
data
99/68:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})
plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
99/69:
data.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})

plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
99/70: sns.lineplot(data=data, x='DATE', y='POSSESSION')
99/71:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

arg_poss = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True).reset_index(drop=True)
arg_poss
99/72:
arg_poss.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})

plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
99/73:
arg_poss.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})

plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
99/74: sns.lineplot(data=data, y='POSSESSION')
99/75: sns.lineplot(data=arg_poss, y='POSSESSION')
99/76:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

arg_poss = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True).reset_index(drop=True)
arg_poss.index
99/77:
argentina_home = argentina.loc[argentina.TEAM1 == 'ARGENTINA', :]
argentina_home = pd.concat([argentina_home.TEAM2, argentina_home.POSSESSION_TEAM1, argentina_home.POSSESSION_TEAM2, argentina_home.POSSESSION_IN_CONTEST, argentina_home.DATE], axis=1)
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina.loc[argentina.TEAM2 == 'ARGENTINA', :]
argentina_away = pd.concat([argentina_away.TEAM1, argentina_away.POSSESSION_TEAM2, argentina_away.POSSESSION_TEAM1, argentina_away.POSSESSION_IN_CONTEST, argentina_away.DATE], axis=1)
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

arg_poss = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True).reset_index(drop=True)
arg_poss
99/78:
arg_poss.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})

plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
99/79: sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
99/80:
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
99/81:
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['a'])
99/82:
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
99/83:
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/84:
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'In contest'])
plt.show()
99/85:
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/86:
sns.set_theme('whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/87:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/88:
sns.set_theme(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/89:
sns.set_theme(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', ax=ax)
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/90:
sns.set_theme(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', ax=ax)
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING', ax=ax)
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/91:
sns.set_theme(style='whitegrid')
fig, ax = plt.subplots(figsize=(10, 6))
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', ax=ax)
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING', ax=ax)
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST', ax=ax)
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/92:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/93:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'], labelcolor=['Blue', 'Green', 'Red'])
plt.show()
99/94:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', palette='mako')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'], labelcolor=['Blue', 'Green', 'Red'])
plt.show()
99/95:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', legend='auto')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'], labelcolor=['Blue', 'Green', 'Red'])
plt.show()
99/96:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', legend='auto')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/97:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', legend='full')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/98:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', legend='brief')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/99:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', legend='brief')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'In contest', 'Defending'])
plt.show()
99/100:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', legend='brief')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')

plt.legend(['Possession', 'In contest', 'Defending'])
plt.show()
99/101:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', legend='brief')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'In contest', 'Defending'])
plt.show()
99/102:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', legend='brief')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending'])
plt.show()
99/103:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION', legend='brief')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['POSSESSION', 'IN CONTEST', 'DEFENDING'])
plt.show()
99/104:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/105:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/106:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')

plt.ylabel("(%)")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/107:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT' ,y='POSSESSION')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='IN CONTEST')
sns.lineplot(data=arg_poss, x=arg_poss.index ,y='DEFENDING')

plt.ylabel("(%)")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/108:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT' ,y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT ,y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT ,y='DEFENDING')

plt.ylabel("(%)")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/109:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/110:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
PLT.xticks(rotation=45)
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/111:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=45)
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/112:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=0)
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/113:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=90)
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/114:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=180)
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/115:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=360)
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/116:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=270)
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/117:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315)
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/118:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315)
plt.xlabel("")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/119:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/120:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=10)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/121:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=8)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/122:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/123:
fig, ax = plt.subplots(4, 4, sharex=True, sharey=True)

sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/124:
fig, ax = plt.subplots(sharex=True, sharey=True)

sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/125:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/126:
arg_poss.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})

plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
99/127:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/128:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION', markers=['o'])
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/129:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION', dashes=True, markers=['o'])
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/130:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/131:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']

arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
99/132:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index(drop=True)
arg_playoffs_goals
99/133:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']

fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
99/134:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index(drop=True)
fra_playoffs_goals
99/135:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
99/136:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)

plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
99/137:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0)
99/138: argentina.sort_values(by='DATE', ascending=True)
99/139: argentina = argentina.sort_values(by='DATE', ascending=True)
99/140:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
99/141:
argentina_home = argentina_home[['TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina_away[['TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

arg_poss = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True).reset_index(drop=True)
arg_poss
99/142:
argentina_home = argentina_home[['TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina_away[['TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

arg_poss = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True).reset_index(drop=True)
arg_poss
99/143:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
99/144:
argentina_home = argentina_home[['TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina_away[['TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

arg_poss = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True).reset_index(drop=True)
arg_poss
99/145:
arg_poss.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})

plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
99/146:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
99/147:
on_target = matches[['TEAM1', 'TEAM2', 'ON_TARGET_ATTEMPTS_TEAM1', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
99/148:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
99/149:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=True).reset_index()

hits_rbm_clb = rbm_clb[rbm_clb['Receptions between midfield and defensive lines'] >= 52]
99/150: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 7))
99/151:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.loc[passes.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
passes.head()
99/152:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
99/153:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
99/154:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
99/155:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
99/156:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
99/157:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
99/158:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
99/159:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first = passes[passes.Country == country]
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.head()
99/160:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Final']]

sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
99/161:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country', size='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
99/162:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
99/163:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100
po_passes = po_passes.sort_values(by='Passes accuracy', ascending=False)

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes.head(10)
99/164:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
99/165:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
99/166:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.ylabel('')
plt.title('Total passes during quarter finals')
plt.show()
99/167:
qf_passes = qf_passes.sort_values(by='Passes accuracy', ascending=False)
qf_passes
99/168:
qf_passes = qf_passes.sort_values(by='Date', ascending=True)

g = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

g.set_axis_labels("", "Pass accuracy (%)")
g.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
99/169:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
99/170: matches = pd.read_csv("../dataset/matches.csv", header=0)
99/171: matches.head(2)
99/172:
def replace_spaces(s):
    return s.replace(" ", "_")
99/173:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/174:
def replace_spaces(s):
    return s.replace(" ", "_")
99/175:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/176:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
99/177:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
99/178:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
99/179:
def replace_spaces(s):
    return s.replace(" ", "_")
99/180:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/181:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
99/182:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
99/183: matches = pd.read_csv("../dataset/matches.csv", header=0)
99/184: matches.head(2)
99/185:
def replace_spaces(s):
    return s.replace(" ", "_")
99/186:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/187:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
99/188:
matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.head()
99/189: matches = pd.read_csv("../dataset/matches.csv", header=0)
99/190: matches.head(2)
99/191:
def replace_spaces(s):
    return s.replace(" ", "_")
99/192:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/193:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
99/194:
matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
matches.head()
99/195:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches = matches.drop('HOUR')
99/196: matches = pd.read_csv("../dataset/matches.csv", header=0)
99/197: matches.head(2)
99/198:
def replace_spaces(s):
    return s.replace(" ", "_")
99/199:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/200:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches = matches.drop('HOUR')
99/201:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches = matches.drop('HOUR', axis=1)
99/202: matches = pd.read_csv("../dataset/matches.csv", header=0)
99/203: matches.head(2)
99/204:
def replace_spaces(s):
    return s.replace(" ", "_")
99/205:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/206:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches = matches.drop('HOUR')
99/207:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.HOUR
99/208: matches = pd.read_csv("../dataset/matches.csv", header=0)
99/209: matches.head(2)
99/210:
def replace_spaces(s):
    return s.replace(" ", "_")
99/211:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/212:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.HOUR
99/213: matches = pd.read_csv("../dataset/matches.csv", header=0)
99/214: matches.head(2)
99/215:
def replace_spaces(s):
    return s.replace(" ", "_")
99/216:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/217:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
matches.drop('HOUR')
99/218: matches = pd.read_csv("../dataset/matches.csv", header=0)
99/219: matches.head(2)
99/220:
def replace_spaces(s):
    return s.replace(" ", "_")
99/221:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
99/222:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
99/223:
matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
matches.head()
99/224:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
99/225: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
99/226:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
99/227:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
99/228:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
99/229:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
99/230:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
99/231:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
99/232:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
99/233:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
99/234:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals', 'Date']
playoffs_goals
99/235:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
99/236:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
100/1:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
100/2: matches = pd.read_csv("../dataset/matches.csv", header=0)
100/3: matches.head(2)
100/4:
def replace_spaces(s):
    return s.replace(" ", "_")
100/5:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
100/6:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
100/7:
matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
matches.head()
100/8:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
100/9: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
100/10:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
100/11:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
100/12:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
100/13:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
100/14:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
100/15:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
100/16:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
100/17:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
100/18:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals', 'Date']
playoffs_goals
100/19:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
100/20:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
100/21:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']

arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
100/22:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index(drop=True)
arg_playoffs_goals
100/23:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']

fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
100/24:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index(drop=True)
fra_playoffs_goals
100/25:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
100/26:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)

plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
100/27:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
100/28:
argentina_home = argentina_home[['TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina_away[['TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

arg_poss = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True).reset_index(drop=True)
arg_poss
100/29:
arg_poss.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})

plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
100/30:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
100/31:
on_target = matches[['TEAM1', 'TEAM2', 'ON_TARGET_ATTEMPTS_TEAM1', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
100/32:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
100/33:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=True).reset_index()

hits_rbm_clb = rbm_clb[rbm_clb['Receptions between midfield and defensive lines'] >= 52]
100/34: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 7))
100/35:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.loc[passes.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
passes.head()
100/36:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
100/37:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first = passes[passes.Country == country]
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.head()
100/38:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Final']]

sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
100/39:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
100/40:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100
po_passes = po_passes.sort_values(by='Passes accuracy', ascending=False)

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes.head(10)
100/41:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
100/42:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
100/43:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.ylabel('')
plt.title('Total passes during quarter finals')
plt.show()
100/44:
qf_passes = qf_passes.sort_values(by='Passes accuracy', ascending=False)
qf_passes
100/45:
qf_passes = qf_passes.sort_values(by='Date', ascending=True)

g = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

g.set_axis_labels("", "Pass accuracy (%)")
g.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
100/46:
"""
Created on 06/02/2023

@author: wssantiago
"""
101/1: import pandas as pd
101/2: food = pd.read_csv("../data/Production_Crops_Livestock_E_All_Data.csv")
101/3: food = pd.read_csv("../data/Production_Crops_Livestock_E_All_Data.csv", encoding='latin-1')
101/4: food
103/1:
import pandas as pd
pd.set_option('display.max_columns', None)
103/2: food = pd.read_csv("../data/Production_Crops_Livestock_E_All_Data.csv", encoding='latin-1')
103/3: food
103/4: food.head(10)
103/5: food = pd.read_csv("../data/Production_Crops_Livestock_E_Flags.csv", encoding='latin-1')
103/6: food.head(10)
103/7: food = pd.read_csv("../data/Production_Crops_Livestock_E_AreaCodes.csv", encoding='latin-1')
103/8: food.head(10)
103/9: food = pd.read_csv("../data/Production_Crops_Livestock_E_ItemCodes.csv", encoding='latin-1')
103/10: food.head(10)
103/11: food
103/12: food = pd.read_csv("../data/Production_Crops_Livestock_E_AreaCodes.csv", encoding='latin-1')
103/13: food
103/14: food = pd.read_csv("../data/Production_Crops_Livestock_E_All.csv", encoding='latin-1')
103/15: food = pd.read_csv("../data/Production_Crops_Livestock_E_All_Data.csv", encoding='latin-1')
103/16: food
104/1: food = pd.read_csv("../data/Inputs_LandUse_E_All_Data.csv", encoding='latin-1')
104/2:
import pandas as pd
pd.set_option('display.max_columns', None)
104/3: food = pd.read_csv("../data/Inputs_LandUse_E_All_Data.csv", encoding='latin-1')
104/4: food
104/5: food.Item.value_counts()
104/6: food
104/7: food.Area.value_counts()
104/8: food.Area.value_counts().shape
104/9: food.Area.value_counts()
104/10: food[food.Area == 'Brazil']
104/11: food[food.Area == 'Brazil'].value_counts()
104/12: food[food.Area == 'Brazil'].Area.value_counts()
104/13: food[food.Area == 'Brazil'].Item.value_counts()
106/1:
import os
import cv2 # OpenCV ou cv2 para tratamento de imagens;
import numpy as np # Numpy para trabalharmos com matrizes n-dimensionais
from keras.models import Sequential # Importando modelo sequencial
from keras.layers.convolutional import Conv2D, MaxPooling2D # Camada de convolução e max pooling
from keras.layers.core import Activation, Flatten, Dense # Camada da função de ativação, flatten, entre outros
from keras.layers import Rescaling # Camada de escalonamento
from keras.optimizers import Adam # optimizador Adam
from keras.callbacks import ModelCheckpoint # Classe utilizada para acompanhamento durante o treinamento onde definimos os atributos que serão considerados para avaliação
from tensorflow.data import AUTOTUNE
from tensorflow.keras.utils import image_dataset_from_directory # Função que carrega o dataset de um diretório
106/2:
def create_lenet(input_shape):
    """
    Cria uma mini arquitetura lenet

    Args:
        input_shape: Uma lista de três valores inteiros que definem a forma de\
                entrada da rede. Exemplo: [100, 100, 3]

    Returns:
        Um modelo sequencial, seguindo a arquitetura lenet
    """
    # Definimos que estamos criando um modelo sequencial
    model = Sequential()

    # Primeira camada do modelo:
    model.add(Conv2D(20, (5, 5), padding="same", input_shape=input_shape))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

    # Segunda camada do modelo:
    model.add(Conv2D(50, (5, 5), padding="same"))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

    # Primeira camada fully connected
    model.add(Flatten())
    model.add(Dense(500))
    model.add(Activation("relu"))

    # Classificador softmax
    model.add(Dense(classes))
    model.add(Activation("softmax"))
    return model
106/3:
if __name__ == "__main__":
    train_path = "./cats_and_dogs" # Adicione aqui o caminho para chegar no diretório que contém as imagens de treino na sua maquina
    models_path = "models" # Defina aqui onde serão salvos os modelos na sua maquina
    width = 100 # Tamanho da largura da janela que será utilizada pelo modelo
    height = 100 # Tamanho da altura da janela que será utilizada pelo modelo
    depth = 1 # Profundidade das janelas utilizadas pelo modelo, caso seja RGB use 3, caso escala de cinza 1
    classes = 2 # Quantidade de classes que o modelo utilizará
    epochs = 10 # Quantidade de épocas (a quantidade de iterações que o modelo realizará durante o treinamento)
    init_lr = 1e-3 # Taxa de aprendizado a ser utilizado pelo optimizador
    batch_size = 32 # Tamanho dos lotes utilizados por cada epoca
    input_shape = (height, width, depth) # entrada do modelo
    save_model = os.path.join(models_path, "lenet-{epoch:02d}-{accuracy:.3f}-{val_accuracy:.3f}.model")
    color_mode = {1:"grayscale", 3: "rgb"} # Usado para selecionar o colormode em função da variável depth

    os.makedirs(models_path, exist_ok=True)

    train_ds = image_dataset_from_directory(
                            train_path,
                            seed=123,
                            label_mode='categorical',
                            validation_split=0.3,
                            subset="training",
                            color_mode=color_mode[depth],
                            image_size=(height, width),
                            batch_size=batch_size
    )

    val_ds = image_dataset_from_directory(
                            train_path,
                            seed=123,
                            label_mode='categorical',
                            validation_split=0.3,
                            subset="validation",
                            color_mode=color_mode[depth],
                            image_size=(height, width),
                            batch_size=batch_size
    )


    rescaling_layer = Rescaling(1./255)
    # pré-busca em buffer para que você possa produzir dados do disco sem que a E/S se torne um bloqueio
    train_ds = train_ds.map(lambda x, y: (rescaling_layer(x), y), num_parallel_calls=AUTOTUNE)
    val_ds = val_ds.map(lambda x, y: (rescaling_layer(x), y), num_parallel_calls=AUTOTUNE)

    model = create_lenet(input_shape)

    opt = Adam(lr=init_lr, decay=init_lr / epochs)

    model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])
    model.summary()

    print("\n training network")

    checkpoint1 = ModelCheckpoint(save_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
    checkpoint2 = ModelCheckpoint(save_model, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
    callbacks_list = [checkpoint1,checkpoint2]

    H = model.fit(train_ds,
                validation_data=val_ds,
                epochs=epochs, 
                verbose=1,
                callbacks=callbacks_list
    )
106/4:
img = cv2.imread('./cats_and_dogs/class_cat/class1_10.jpg')
plt.imshow(img)
plt.show()
106/5:
import matplotlib.pyplot as plt

img = cv2.imread('./cats_and_dogs/class_cat/class1_10.jpg')
plt.imshow(img)
plt.show()
106/6:
resized = tf.image.resize(img, (256, 256))
plt.imshow(resize.numpy().astype(int))
plt.show()
106/7:
import tensorflow as tf

resized = tf.image.resize(img, (256, 256))
plt.imshow(resize.numpy().astype(int))
plt.show()
106/8:
import tensorflow as tf

resized = tf.image.resize(img, (256, 256))
plt.imshow(resized.numpy().astype(int))
plt.show()
106/9: yhat = model.predict(np.expand_dims(resized/255, 0))
106/10:
import tensorflow as tf

resized = tf.image.resize(img, (100, 100))
plt.imshow(resized.numpy().astype(int))
plt.show()
106/11: yhat = model.predict(np.expand_dims(resized/255, 0))
106/12:
np.expand_dims(resized/255, 0)
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/13:
np.expand_dims(resized/255, 0).shape
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/14:
np.expand_dims(resized/255, 0).shape
yhat = model.predict(np.expand_dims(resized/255, 0))
106/15:
np.expand_dims(resized/255, 0).shape
yhat = model.predict(np.expand_dims(resized/255))
106/16:
np.expand_dims(resized/255, 0).shape
yhat = model.predict(np.expand_dims(resized/255), 1)
106/17:
np.expand_dims(resized/255, 0).shape
yhat = model.predict(np.expand_dims(resized/255), 0)
106/18:
np.expand_dims(resized/255, 0).shape
yhat = model.predict(np.expand_dims(resized/255), 0)
106/19:

yhat = model.predict(np.expand_dims(resized/255), 0)
106/20:

yhat = model.predict(np.expand_dims(resized/255, 1))
106/21:

yhat = model.predict(np.expand_dims(resized/255, 0))
106/22:

yhat = model.predict(np.expand_dims(resized/255, -1))
106/23:

yhat = model.predict(np.expand_dims(resized/255, 0))
106/24:
resized.shape
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/25:
resized.reshape((100, 100, 1))
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/26:
np.reshape(resized, (100, 100, 1))
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/27:
import matplotlib.pyplot as plt

img = cv2.imread('./cats_and_dogs/class_cat/class1_10.jpg')
plt.imshow(img)
plt.show()
img.shape
106/28:
import tensorflow as tf

resized = tf.image.resize(img, (100, 100, 1))
plt.imshow(resized.numpy().astype(int))
plt.show()
106/29:
import tensorflow as tf

resized = tf.image.resize(img, (100, 100))
plt.imshow(resized.numpy().astype(int))
plt.show()
106/30:
np.reshape(resized, (100, 100, 1))
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/31:
np.reshape(resized, (100, 100))
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/32: yhat = model.predict(np.expand_dims(resized/255, 0))
106/33:
import matplotlib.pyplot as plt

img = cv2.imread('./cats_and_dogs/class_cat/class1_123.jpg')
plt.imshow(img)
plt.show()
img.shape
106/34:
import tensorflow as tf

resized = tf.image.resize(img, (100, 100))
plt.imshow(resized.numpy().astype(int))
plt.show()
106/35: gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
106/36: gray = cv2.cvtColor(np.float32(resized), cv2.COLOR_BGR2GRAY)
106/37:
gray = cv2.cvtColor(np.float32(resized), cv2.COLOR_BGR2GRAY)
plt.imshow(gray.numpy().astype(int))
plt.show()
106/38:
gray = cv2.cvtColor(np.float32(resized), cv2.COLOR_BGR2GRAY)
plt.imshow(gray.astype(int))
plt.show()
106/39:
gray = cv2.cvtColor(np.float32(resized), cv2.COLOR_BGR2GRAY)
plt.imshow(gray.astype(int))
plt.show()
gray.shape
106/40:
np.expand_dims(resized/255, 0)
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/41:
np.expand_dims(resized/255, 0).shape
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/42:
np.expand_dims(gray/255, 0).shape
#yhat = model.predict(np.expand_dims(resized/255, 0))
106/43:
np.expand_dims(gray/255, 0).shape
yhat = model.predict(np.expand_dims(gray/255, 0))
106/44: yhat
106/45:
import matplotlib.pyplot as plt

img = cv2.imread('./cats_and_dogs/class_cat/class1_13.jpg')
plt.imshow(img)
plt.show()
img.shape
106/46:
import tensorflow as tf

resized = tf.image.resize(img, (100, 100))
plt.imshow(resized.numpy().astype(int))
plt.show()
106/47:
gray = cv2.cvtColor(np.float32(resized), cv2.COLOR_BGR2GRAY)
plt.imshow(gray.astype(int))
plt.show()
gray.shape
106/48: yhat = model.predict(np.expand_dims(gray/255, 0))
106/49: yhat
106/50:
import matplotlib.pyplot as plt

img = cv2.imread('./cats_and_dogs/class_cat/class1_1983.jpg')
plt.imshow(img)
plt.show()
img.shape
106/51:
import tensorflow as tf

resized = tf.image.resize(img, (100, 100))
plt.imshow(resized.numpy().astype(int))
plt.show()
106/52:
gray = cv2.cvtColor(np.float32(resized), cv2.COLOR_BGR2GRAY)
plt.imshow(gray.astype(int))
plt.show()
gray.shape
106/53: yhat = model.predict(np.expand_dims(gray/255, 0))
106/54: yhat
106/55:
import matplotlib.pyplot as plt

img = cv2.imread('./cats_and_dogs/class_dog/class2_1983.jpg')
plt.imshow(img)
plt.show()
img.shape
106/56:
import tensorflow as tf

resized = tf.image.resize(img, (100, 100))
plt.imshow(resized.numpy().astype(int))
plt.show()
106/57:
gray = cv2.cvtColor(np.float32(resized), cv2.COLOR_BGR2GRAY)
plt.imshow(gray.astype(int))
plt.show()
gray.shape
106/58: yhat = model.predict(np.expand_dims(gray/255, 0))
106/59: yhat
109/1:
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
from descartes import PolygonPatch
sns.set_theme(style="darkgrid")
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
109/2: matches = pd.read_csv("../dataset/matches.csv", header=0)
109/3: matches.head(2)
109/4:
def replace_spaces(s):
    return s.replace(" ", "_")
109/5:
new_columns = {}
for col in matches.columns:
    new_columns[col] = replace_spaces(col).upper()
    
matches = matches.rename(columns=new_columns)
matches = matches.rename(columns={'COMPLETED_LINE_BREAKSTEAM1': 'COMPLETED_LINE_BREAKS_TEAM1'})
109/6:
matches.DATE = matches.DATE.str.replace('nov.', '11', regex=False)
matches.DATE = matches.DATE.str.replace('DEC', '12', regex=False)
matches.DATE = matches.DATE.str.replace(' ', '', regex=False)

matches.HOUR = matches.HOUR.str.replace(' ', '', regex=False)

matches.DATE = matches.DATE + ' ' + matches.HOUR
matches.DATE = pd.to_datetime(matches.DATE, format='%d%m%Y %H:%M')
109/7:
matches.POSSESSION_TEAM1 = matches.POSSESSION_TEAM1.str.replace('%', '').astype(int)
matches.POSSESSION_TEAM2 = matches.POSSESSION_TEAM2.str.replace('%', '').astype(int)
matches.POSSESSION_IN_CONTEST = matches.POSSESSION_IN_CONTEST.str.replace('%', '').astype(int)
matches.head()
109/8:
def patch(ax, country, color):
    c = countries[countries.name == country]
    var = c.__geo_interface__['features']
    var0 = {'type': var[0]['geometry']['type'], 'coordinates': var[0]['geometry']['coordinates']}
    ax.add_patch(PolygonPatch(var0, fc=color, ec='black', alpha=.85, zorder=2))
109/9: countries = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
109/10:
wc_participants = pd.unique(matches.TEAM1.str.capitalize())
wc_participants = pd.DataFrame(wc_participants)
wc_participants.columns = ['Countries']
109/11:
wc_map = wc_participants
wc_map.loc[1, 'Countries'] = 'United Kingdom'
wc_map.Countries = wc_map.Countries.str.title()
wc_map.loc[3, 'Countries'] = 'United States of America'
wc_map = wc_map.drop(index=16).reset_index()['Countries']
wc_map.loc[22] = 'South Korea'

ax = countries.plot(color='lightgray', figsize=(10, 8))
for country in wc_map:
    if country == 'France':
        patch(ax, country, 'navy')
    elif country == 'Argentina':
        patch(ax, country, 'lightskyblue')
    else:
        patch(ax, country, 'moccasin')

plt.title("2022 FIFA World Cup countries (Argentina and France highlighted)")
plt.show()
109/12:
home = matches[['TEAM1', 'NUMBER_OF_GOALS_TEAM1']]
away = matches[['TEAM2', 'NUMBER_OF_GOALS_TEAM2']]
109/13:
home.columns = ['Countries', 'Goals']
away.columns = home.columns
109/14:
goals = pd.concat([home, away], axis=0)
goals = goals.groupby('Countries').sum()
goals = goals.sort_values(by='Goals', ascending=False)
goals.head(10)
109/15:
goals.plot.bar()
plt.xticks(fontsize=8)
plt.title("Countries number of goals")
plt.xlabel('Country names')
plt.yticks(range(17))
plt.show()
109/16:
groups_matches = matches[matches.CATEGORY.str.startswith('Group')]
matches_goals = groups_matches.NUMBER_OF_GOALS_TEAM1 + groups_matches.NUMBER_OF_GOALS_TEAM2
groups_goals = pd.concat([groups_matches.CATEGORY, matches_goals], axis=1)
groups_goals.columns = ['Group', 'Goals']
groups_goals = groups_goals.groupby('Group').sum()
groups_goals
109/17:
groups_goals.plot.barh()
plt.xticks(range(23))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
109/18:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]
matches_goals = playoffs.NUMBER_OF_GOALS_TEAM1 + playoffs.NUMBER_OF_GOALS_TEAM2
playoffs_goals = pd.concat([playoffs.CATEGORY, playoffs.TEAM1, playoffs.TEAM2, playoffs.NUMBER_OF_GOALS_TEAM1, playoffs.NUMBER_OF_GOALS_TEAM2, matches_goals, playoffs.DATE], axis=1)
playoffs_goals.columns = ['Round', 'Home', 'Away', 'Goals home', 'Goals away', 'Total goals', 'Date']
playoffs_goals
109/19:
total_playoffs_goals = playoffs_goals[['Round', 'Total goals']]
total_playoffs_goals = total_playoffs_goals.groupby('Round', sort=False).sum()
total_playoffs_goals = total_playoffs_goals.reset_index()
total_playoffs_goals
109/20:
total_playoffs_goals.plot.barh(x='Round', y='Total goals')
plt.xticks(range(0, 29, 2))
plt.ylabel('')
plt.title('Number of goals scored per group')
plt.show()
109/21:
arg_playoffs_home = playoffs_goals[playoffs_goals.Home == 'ARGENTINA']
arg_playoffs_home = arg_playoffs_home[['Round', 'Goals home', 'Date']]
arg_playoffs_home.columns = ['Round', 'Goals', 'Date']

arg_playoffs_away = playoffs_goals[playoffs_goals.Away == 'ARGENTINA']
arg_playoffs_away = arg_playoffs_away[['Round', 'Goals away', 'Date']]
arg_playoffs_away.columns = ['Round', 'Goals', 'Date']
109/22:
arg_playoffs_goals = pd.concat([arg_playoffs_home, arg_playoffs_away], axis=0).sort_values(by='Date')
arg_playoffs_goals = arg_playoffs_goals[['Round', 'Goals']]
arg_playoffs_goals.columns = ['Round', 'Argentina goals']
arg_playoffs_goals = arg_playoffs_goals.reset_index(drop=True)
arg_playoffs_goals
109/23:
fra_playoffs_home = playoffs_goals[playoffs_goals.Home == 'FRANCE']
fra_playoffs_home = fra_playoffs_home[['Round', 'Goals home', 'Date']]
fra_playoffs_home.columns = ['Round', 'Goals', 'Date']

fra_playoffs_away = playoffs_goals[playoffs_goals.Away == 'FRANCE']
fra_playoffs_away = fra_playoffs_away[['Round', 'Goals away', 'Date']]
fra_playoffs_away.columns = ['Round', 'Goals', 'Date']
109/24:
fra_playoffs_goals = pd.concat([fra_playoffs_home, fra_playoffs_away], axis=0).sort_values(by='Date')
fra_playoffs_goals = fra_playoffs_goals[['Round', 'Goals']]
fra_playoffs_goals.columns = ['Round', 'France goals']
fra_playoffs_goals = fra_playoffs_goals.reset_index(drop=True)
fra_playoffs_goals
109/25:
finalists_games = total_playoffs_goals[total_playoffs_goals.Round != 'Play-off for third place'].reset_index()
finalists_goals = pd.concat([arg_playoffs_goals.Round, arg_playoffs_goals['Argentina goals'], fra_playoffs_goals['France goals'], finalists_games['Total goals']], axis=1)
finalists_goals
109/26:
finalists_goals.plot.bar(x='Round', y=['Argentina goals', 'France goals', 'Total goals'], stacked=False)

plt.yticks(range(0, 29, 2), fontsize=9)
plt.xticks(rotation=0)
plt.xlabel('')
plt.legend(['Argentina', 'France', 'Total'], loc='upper right')
plt.title('Goals per playoff rounds')
plt.show()
109/27:
argentina_home = matches.loc[matches.TEAM1 == 'ARGENTINA']
argentina_away = matches.loc[matches.TEAM2 == 'ARGENTINA']
argentina = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True)
109/28:
argentina_home = argentina_home[['TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_home.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

argentina_away = argentina_away[['TEAM1', 'POSSESSION_TEAM2', 'POSSESSION_TEAM1', 'POSSESSION_IN_CONTEST', 'DATE']]
argentina_away.columns = ['OPPONENT', 'POSSESSION', 'DEFENDING', 'IN CONTEST', 'DATE']

arg_poss = pd.concat([argentina_home, argentina_away], axis=0).sort_values(by='DATE', ascending=True).reset_index(drop=True)
arg_poss
109/29:
arg_poss.plot.barh(x='OPPONENT', y=['DEFENDING', 'IN CONTEST', 'POSSESSION'], stacked=True, color={'DEFENDING': 'tomato', 'IN CONTEST': 'gray', 'POSSESSION': 'lightskyblue'})

plt.xticks(range(0, 101, 5))
plt.legend(['Opponent', 'In contest', 'Argentina'], loc='upper right')
plt.yticks(fontsize=9)
plt.title('Ball possession throughout all Argentina seven matches')
plt.show()
109/30:
sns.set_theme(style='whitegrid')
sns.lineplot(data=arg_poss, x='OPPONENT', y='POSSESSION')
sns.lineplot(data=arg_poss, x='OPPONENT', y='IN CONTEST')
sns.lineplot(data=arg_poss, x='OPPONENT', y='DEFENDING')

plt.ylabel("(%)")
plt.xticks(rotation=315, fontsize=9)
plt.xlabel("")
plt.title("Argentina ball situation throughout world cup campaign")
plt.legend(['Possession', 'Defending', 'In contest'])
plt.show()
109/31:
on_target = matches[['TEAM1', 'TEAM2', 'ON_TARGET_ATTEMPTS_TEAM1', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_home = on_target[['TEAM1', 'ON_TARGET_ATTEMPTS_TEAM1']]
on_target_home.columns = ['Country', 'On target attempts']

on_target_away = on_target[['TEAM2', 'ON_TARGET_ATTEMPTS_TEAM2']]
on_target_away.columns = ['Country', 'On target attempts']

on_target = pd.concat([on_target_home, on_target_away], axis=0)
on_target = on_target.groupby('Country').sum().reset_index().sort_values(by='On target attempts', ascending=False)
109/32:
plot = sns.catplot(
    data=on_target, x='On target attempts', y='Country', 
    kind="bar", height=5.5, aspect=2, width=1, orient='h', palette='flare_r'
)

plt.yticks(fontsize=9.5)
plt.title("Total of on target attempts by each country")
plt.ylabel('Country names')
plt.xticks(range(0, 49, 2))
plt.show()
109/33:
home_rbm_clb = matches[['TEAM1', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM1', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM1']]
home_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

away_rbm_clb = matches[['TEAM2', 'RECEPTIONS_BETWEEN_MIDFIELD_AND_DEFENSIVE_LINES_TEAM2', 'COMPLETED_DEFENSIVE_LINE_BREAKS_TEAM2']]
away_rbm_clb.columns = ['Country', 'Receptions between midfield and defensive lines', 'Completed defensive line breaks']

rbm_clb = pd.concat([home_rbm_clb, away_rbm_clb], axis=0)
rbm_clb = rbm_clb.groupby('Country').sum().sort_values(by='Receptions between midfield and defensive lines', ascending=True).reset_index()

hits_rbm_clb = rbm_clb[rbm_clb['Receptions between midfield and defensive lines'] >= 52]
109/34: hits_rbm_clb.plot.barh(x='Country', y=['Receptions between midfield and defensive lines', 'Completed defensive line breaks'], figsize=(10, 7))
109/35:
passes_home = matches[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes_away = matches[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

passes = pd.concat([passes_home, passes_away], axis=0)
passes['Passes not completed'] = passes.Passes - passes['Passes completed']
passes['Passes accuracy'] = (passes['Passes completed'] / passes.Passes) * 100
passes = passes.sort_values(by='Passes accuracy', ascending=False)

date = passes.pop('Date')
passes['Date'] = date.astype(str)
passes.Date = passes.Date.str[0:16]
passes.loc[passes.Round.str.startswith('Group'), 'Round'] = 'Group Stage'
passes.head()
109/36:
sns.scatterplot(data=passes, x='Passes', y='Passes accuracy', hue='Round')
plt.ylabel("Pass accuracy (%)")
plt.title("Pass accuracy per match round")
plt.show()
109/37:
first_places = ['NETHERLANDS', 'ENGLAND', 'ARGENTINA', 'FRANCE', 'JAPAN', 'MOROCCO', 'BRAZIL', 'PORTUGAL']
firsts = pd.DataFrame()

for country in first_places:
    first = passes[passes.Country == country]
    firsts = pd.concat([firsts, first], axis=0)

firsts['Passes not completed'] = firsts.Passes - firsts['Passes completed']
firsts['Passes accuracy'] = (firsts['Passes completed'] / firsts.Passes) * 100
firsts = firsts.sort_values(by='Passes accuracy', ascending=False)

date = firsts.pop('Date')
firsts['Date'] = date.astype(str)
firsts.Date = firsts.Date.str[0:16]

firsts = firsts.reset_index(drop=True)
firsts.head()
109/38:
firsts_heatmap = firsts.pivot_table(index='Country', columns='Round', values='Passes accuracy', aggfunc='mean')
firsts_heatmap = firsts_heatmap[['Group Stage', 'Round of 16', 'Quarter-final', 'Semi-final', 'Final']]

sns.heatmap(firsts_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
109/39:
fig, ax = plt.subplots(figsize=(10, 5))

firsts_lineplot = firsts.sort_values(by='Date', ascending=True).reset_index(drop=True)
firsts_lineplot = firsts_lineplot[firsts_lineplot.Round != 'Play-off for third place']

sns.lineplot(data=firsts_lineplot, x='Round', y='Passes accuracy', hue='Country')
plt.legend(loc='lower right')
plt.ylabel('Passes accuracy (%)')
plt.xlabel('')
plt.title("Group stages' first places pass accuracy evolution")
plt.show()
109/40:
playoffs = matches[matches.CATEGORY.str.startswith('Group') == False]

po_passes_home = playoffs[['TEAM1', 'CATEGORY', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
po_passes_home.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes_away = playoffs[['TEAM2', 'CATEGORY', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
po_passes_away.columns = ['Country', 'Round', 'Passes', 'Passes completed', 'Date']

po_passes = pd.concat([po_passes_home, po_passes_away], axis=0)
po_passes['Passes not completed'] = po_passes.Passes - po_passes['Passes completed']
po_passes['Passes accuracy'] = (po_passes['Passes completed'] / po_passes.Passes) * 100
po_passes = po_passes.sort_values(by='Passes accuracy', ascending=False)

date = po_passes.pop('Date')
po_passes['Date'] = date.astype(str)
po_passes.Date = po_passes.Date.str[0:16]
po_passes.head(10)
109/41:
sns.scatterplot(data=po_passes, x="Passes", y="Passes accuracy", hue='Round')
plt.ylabel('Passes accuracy (%)')
plt.title('Countries pass accuracy per match round throughout playoffs')
109/42:
quarter_finals = matches[matches.CATEGORY == 'Quarter-final']

qf_passes_home = quarter_finals[['TEAM1', 'PASSES_TEAM1', 'PASSES_COMPLETED_TEAM1', 'DATE']]
qf_passes_home.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes_away = quarter_finals[['TEAM2', 'PASSES_TEAM2', 'PASSES_COMPLETED_TEAM2', 'DATE']]
qf_passes_away.columns = ['Country', 'Passes', 'Passes completed', 'Date']

qf_passes = pd.concat([qf_passes_home, qf_passes_away], axis=0).sort_values(by='Passes', ascending=True)
qf_passes['Passes not completed'] = qf_passes.Passes - qf_passes['Passes completed']
qf_passes['Passes accuracy'] = (qf_passes['Passes completed'] / qf_passes.Passes) * 100

date = qf_passes.pop('Date')
qf_passes['Date'] = date.astype(str)
qf_passes.Date = qf_passes.Date.str[0:16]
qf_passes
109/43:
qf_passes.plot.barh(x='Country', y=['Passes completed', 'Passes not completed'], figsize=(8, 6), stacked=True)
plt.xticks(range(0, 716, 50))
plt.ylabel('')
plt.title('Total passes during quarter finals')
plt.show()
109/44:
qf_passes = qf_passes.sort_values(by='Passes accuracy', ascending=False)
qf_passes
109/45:
qf_passes = qf_passes.sort_values(by='Date', ascending=True)

g = sns.catplot(
    data=qf_passes, x="Country", y="Passes accuracy", col="Date",
    kind="bar", height=4, aspect=.8, width=.5, sharex=False, color='salmon', palette='viridis'
)

g.set_axis_labels("", "Pass accuracy (%)")
g.set_titles("Match from {col_name}")

plt.yticks(range(0, 91, 5))
plt.show()
110/1:
! mkdir dataset
! unzip dataset.zip -d dataset
110/2:
!pip install easyocr
!pip install texttable
import numpy as np
from google.colab.patches import cv2_imshow
import cv2
import glob
import imutils
import easyocr
from prettytable import PrettyTable
from keras.preprocessing import image
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.model_selection import GridSearchCV, train_test_split
from skimage.io import imread
import skimage.filters
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', 30*30 + 1)
110/3:
#! mkdir dataset
#! unzip dataset.zip -d dataset
110/4:
#! mkdir dataset
#! unzip dataset.zip -d dataset
110/5:
!pip install easyocr
!pip install texttable
import numpy as np
from google.colab.patches import cv2_imshow
import cv2
import glob
import imutils
import easyocr
from prettytable import PrettyTable
from keras.preprocessing import image
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.model_selection import GridSearchCV, train_test_split
from skimage.io import imread
import skimage.filters
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', 30*30 + 1)
112/1: import pandas as pd
112/2: base = pd.read_csv('sisu-2022-1.csv')
112/3: base = pd.read_csv('./sisu-2022-1.csv')
112/4: base = pd.read_csv('./sisu-2022-1.csv', header=0)
112/5: base = pd.read_csv("./sisu-2022-1.csv", header=0)
112/6: base = pd.read_csv("./sisu-2022-1.csv", header=0, encoding="utf-8")
112/7: base = pd.read_csv("./sisu-2022-1.csv", header=0, encoding="ISO-8859-1")
112/8: base = pd.read_csv("./sisu-2022-1.csv", header=0, encoding="utf-8")
115/1: import pandas as pd
115/2: base = pd.read_csv("./sisu-2022-1.csv", header=0, encoding="utf-8")
115/3: base = pd.read_csv("./sisu-2022-1.csv", header=0)
115/4: base = pd.read_csv("./sisu-2022-1.csv", header=0, engine='python')
115/5: base = pd.read_csv("./sisu-2022-1.csv", header=0, encoding="ISO-8859-1")
115/6: base = pd.read_csv("./sisu-2022-1.csv", header=0, encoding="cp1252")
115/7: base = pd.read_csv("./sisu-2022-1.csv", header=0, encoding="latin1")
118/1: import tensorflow as tf
118/2: gpus = tf.config.experimental.list_physical_devices('GPU')
118/3: gpus
118/4: gpus
118/5:
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
118/6:
for gpu in gpus:
    print(gpu)
118/7:
for gpu in gpus:
    print(gpu)
118/8: tf.test.is_gpu_available()
118/9: tf.test.is_built_with_cuda()
119/1: tf.test.is_built_with_cuda()
119/2: import tensorflow as tf
119/3: gpus = tf.config.experimental.list_physical_devices('GPU')
119/4:
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
119/5:
for gpu in gpus:
    print(gpu)
119/6: tf.test.is_built_with_cuda()
120/1: import tensorflow as tf
120/2: gpus = tf.config.experimental.list_physical_devices('GPU')
120/3:
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
120/4:
for gpu in gpus:
    print(gpu)
120/5: tf.test.is_built_with_cuda()
121/1: import tensorflow as tf
121/2: gpus = tf.config.experimental.list_physical_devices('GPU')
121/3:
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
121/4:
for gpu in gpus:
    print(gpu)
121/5: tf.test.is_built_with_cuda()
121/6:
import tensorflow_datasets as tfds
from matplotlib import pyplot as plt
121/7: ds = tfds.load('fashion_mnist', split='train')
121/8: ds
121/9: ds.as_numpy_iterator().next()
121/10: ds.as_numpy_iterator().next().keys()
121/11: ds.as_numpy_iterator().next()['image']
121/12: ds.as_numpy_iterator().next()['label']
121/13: ds
121/14: import numpy as np
121/15: dataiterator = ds.as_numpy_iterator()
121/16: dataiterator = ds.as_numpy_iterator()
121/17: dataiterator.next()
121/18: dataiterator.next()
121/19: dataiterator.next()
121/20: dataiterator.next()
121/21: dataiterator.next()
121/22: dataiterator.next()
121/23: dataiterator.next()
121/24: dataiterator.next()
121/25: dataiterator.next()
121/26: dataiterator.next()
121/27: dataiterator.next()
121/28: dataiterator.next()
121/29: dataiterator = ds.as_numpy_iterator()
121/30:
# getting data from the pipeline
dataiterator.next()
121/31:
fig, ax = plt.subplots(ncols=4, figsize(20, 20))
for i in range(4):
    batch = dataiterator.next()
    ax[i].imshow(np.squeeze(batch['image']))
    ax[i].title.set_text(batch['label'])
121/32:
fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i in range(4):
    batch = dataiterator.next()
    ax[i].imshow(np.squeeze(batch['image']))
    ax[i].title.set_text(batch['label'])
121/33:
# setup the subplot formatting
fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i in range(4):
    batch = dataiterator.next()
    ax[i].imshow(np.squeeze(batch['image']))
    ax[i].title.set_text(batch['label'])
121/34:
# setup the subplot formatting
fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i in range(4):
    batch = dataiterator.next()
    ax[i].imshow(np.squeeze(batch['image']))
    ax[i].title.set_text(batch['label'])
121/35:
# setup the subplot formatting
fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i in range(4):
    batch = dataiterator.next()
    ax[i].imshow(np.squeeze(batch['image']))
    ax[i].title.set_text(batch['label'])
121/36:
# scaling image pixels
def scaler(data):
    image = data['image']
    return image / 255
121/37:
# common steps for building tf pipeline
ds = ds.map(scaler)
ds = ds.cache()
ds = ds.shuffle(60000)
# batches of 128 images per sample
ds = ds.batch(128)
# reduces bottlenecking
ds = ds.prefetch(64)
121/38: ds.as_numpy_iterator().next()
121/39: ds.as_numpy_iterator().next().shape
121/40:
def viz_images(iterator):
    fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
    for i in range(4):
        batch = iterator.next()
        ax[i].imshow(np.squeeze(batch['image']))
        ax[i].title.set_text(batch['label'])
121/41:
# setup the subplot formatting
viz_images(dataiterator)
121/42:
# setup the subplot formatting
viz_images(dataiterator)
121/43:
transfiterator = ds.as_numpy_iterator()
transfiterator.next().shape
121/44: viz_images(transfiterator)
121/45: transfiterator
121/46: dataiterator = ds.as_numpy_iterator()
121/47:
# getting data from the pipeline
dataiterator.next()
121/48:
def viz_images(iterator):
    fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
    for i in range(4):
        batch = iterator.next()
        ax[i].imshow(np.squeeze(batch['image']))
        ax[i].title.set_text(batch['label'])
121/49:
# setup the subplot formatting
viz_images(dataiterator)
121/50: ds = tfds.load('fashion_mnist', split='train')
121/51: ds
121/52: import numpy as np
121/53: dataiterator = ds.as_numpy_iterator()
121/54:
# getting data from the pipeline
dataiterator.next()
121/55:
def viz_images(iterator):
    fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
    for i in range(4):
        batch = iterator.next()
        ax[i].imshow(np.squeeze(batch['image']))
        ax[i].title.set_text(batch['label'])
121/56:
# setup the subplot formatting
viz_images(dataiterator)
121/57:
# scaling image pixels
def scaler(data):
    image = data['image']
    return image / 255
121/58:
# common steps for building tf pipeline
ds = ds.map(scaler)
ds = ds.cache()
ds = ds.shuffle(60000)
# batches of 128 images per sample
ds = ds.batch(128)
# reduces bottlenecking
ds = ds.prefetch(64)
121/59:
transfiterator = ds.as_numpy_iterator()
transfiterator.next().shape
121/60: transfiterator.
121/61: viz_images(transfiterator)
121/62: viz_images(transfiterator[1])
121/63: viz_images(transfiterator.next()[1])
121/64: transfiterator.next(?)
121/65: transfiterator.next()
121/66: transfiterator.next()[1]
121/67: transfiterator.next()[1].shape
121/68:
transfiterator = ds.as_numpy_iterator()
transfiterator.next().shape
121/69: # build neural network
121/70:
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2d, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling
121/71:
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling
121/72:
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D
121/73:
from tensorflow.keras.models import Sequential
# the neural network layers
from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D
121/74:
from tensorflow.keras.models import Sequential
# the neural network layers and activation functions
from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D
121/75:
def build_generator():
    model = Sequential()
    
    model.add(Dense(7*7*128, input_dim=128))
    model.add(LeakyReLU(0.2))
    model.Reshape((7,7,128))
    
    return model
121/76:
test_model = build_generator()
test_model.summary()
121/77:
def build_generator():
    model = Sequential()
    
    model.add(Dense(7*7*128, input_dim=128))
    model.add(LeakyReLU(0.2))
    model.add(Reshape((7,7,128))
    
    return model
121/78:
def build_generator():
    model = Sequential()
    
    model.add(Dense(7*7*128, input_dim=128))
    model.add(LeakyReLU(0.2))
    model.add(Reshape((7,7,128)))
    
    return model
121/79:
test_model = build_generator()
test_model.summary()
121/80:
def build_generator():
    model = Sequential()
    
    # 128 random numbers as input to the nn
    model.add(Dense(7*7*128, input_dim=128))
    # 0.2 is a parameter to define leakiness?
    model.add(LeakyReLU(0.2))
    # reshaping as a piece of the image shape
    model.add(Reshape((7,7,128)))
    
    # upsampling block 1
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    return model
121/81:
test_model = build_generator()
test_model.summary()
121/82:
def build_generator():
    model = Sequential()
    
    # 128 random numbers as input to the nn
    model.add(Dense(7*7*128, input_dim=128))
    # 0.2 is a parameter to define leakiness?
    model.add(LeakyReLU(0.2))
    # reshaping as a piece of the image shape
    model.add(Reshape((7,7,128)))
    
    # upsampling block 1
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    model.add(UpSampling2D())
    model.add(Conv2D(1, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    return model
121/83:
test_model = build_generator()
test_model.summary()
121/84:
def build_generator():
    model = Sequential()
    
    # 128 random numbers as input to the nn
    model.add(Dense(7*7*128, input_dim=128))
    # 0.2 is a parameter to define leakiness?
    model.add(LeakyReLU(0.2))
    # reshaping as a piece of the image shape
    model.add(Reshape((7,7,128)))
    
    # upsampling block 1 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    
    return model
121/85:
test_model = build_generator()
test_model.summary()
121/86:
def build_generator():
    model = Sequential()
    
    # 128 random numbers as input to the nn
    model.add(Dense(7*7*128, input_dim=128))
    # 0.2 is a parameter to define leakiness?
    model.add(LeakyReLU(0.2))
    # reshaping as a piece of the image shape
    model.add(Reshape((7,7,128)))
    
    # upsampling block 1 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # upsampling block 2 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # downsampling block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # downsampling block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    
    return model
121/87:
test_model = build_generator()
test_model.summary()
121/88:
def build_generator():
    model = Sequential()
    
    # 128 random numbers as input to the nn
    model.add(Dense(7*7*128, input_dim=128))
    # 0.2 is a parameter to define leakiness?
    model.add(LeakyReLU(0.2))
    # reshaping as a piece of the image shape
    model.add(Reshape((7,7,128)))
    
    # upsampling block 1 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # upsampling block 2 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # downsampling block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # downsampling block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # convolutional layer to get to one channel
    model.add(Conv2D(1, 4, padding='same', activation='sigmoid'))
    
    return model
121/89:
test_model = build_generator()
test_model.summary()
121/90:
def build_generator():
    model = Sequential()
    
    # 128 random numbers as input to the nn
    model.add(Dense(7*7*128, input_dim=128))
    # 0.2 is a parameter to define leakiness?
    model.add(LeakyReLU(0.2))
    # reshaping as a piece of the image shape
    model.add(Reshape((7,7,128)))
    
    # upsampling block 1 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # upsampling block 2 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # downsampling block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # downsampling block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # convolutional layer to get to one channel
    # desired image shape (28,28,1)
    model.add(Conv2D(1, 4, padding='same', activation='sigmoid'))
    
    return model
121/91:
test_model = build_generator()
test_model.summary()
121/92:
def build_generator():
    model = Sequential()
    
    # 128 random numbers as input to the nn
    model.add(Dense(7*7*128, input_dim=128))
    model.add(LeakyReLU(0.2))
    model.add(Reshape((7,7,128)))
    
    # upsampling block 1 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # upsampling block 2 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # convolutional block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # convolutional block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # convolutional layer to get to one channel
    # desired image shape (28,28,1)
    model.add(Conv2D(1, 4, padding='same', activation='sigmoid'))
    
    return model
121/93:
test_model = build_generator()
test_model.summary()
121/94:
def build_generator():
    model = Sequential()
    
    # 128 random numbers as input to the nn
    model.add(Dense(7*7*128, input_dim=128))
    model.add(LeakyReLU(0.2))
    model.add(Reshape((7,7,128)))
    
    # upsampling block 1 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # upsampling block 2 -> doubles size of previous layer output
    model.add(UpSampling2D())
    model.add(Conv2D(128, 5, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # convolutional block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # convolutional block 1
    model.add(Conv2D(128, 4, padding='same'))
    model.add(LeakyReLU(0.2))
    
    # convolutional layer to get to one channel (one kernel)
    # desired image shape (28,28,1)
    model.add(Conv2D(1, 4, padding='same', activation='sigmoid'))
    
    return model
121/95:
test_model = build_generator()
test_model.summary()
121/96:
generator = build_generator()
generator.summary()
121/97:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
img
121/98:
fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/99:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
img.shape
121/100:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
img.shape
121/101:
fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/102:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
img.shape
121/103:
fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/104:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print(img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/105:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape= ', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/106:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape = ', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/107:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape =', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/108:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape =', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/109:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape =', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/110:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape =', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/111:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape =', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/112:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape =', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/113:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape =', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/114:
# randn uses normal distribution
# 4 images with 128 random values
img = generator.predict(np.random.randn(4, 128, 1))
print('img shape =', img.shape)

fig, ax = plt.subplots(ncols=4, figsize=(20, 20))
for i, img in enumerate(img):
    ax[i].imshow(np.squeeze(img))
    ax[i].title.set_text(i)
121/115:
discriminator = build_discriminator()
discriminator.summary()
121/116:
def build_discriminator():
    model = Sequential()
    
    # first convolutional block
    # input shape is the generator output shape
    # dropout applies reguralization
    model.add(Conv2D(32, 5, input_shape=(28,28,1)))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    return model
121/117:
discriminator = build_discriminator()
discriminator.summary()
121/118:
def build_discriminator():
    model = Sequential()
    
    # first convolutional block
    # input shape is the generator output shape
    # dropout applies reguralization
    model.add(Conv2D(32, 5, input_shape=(28,28,1)))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # second convolutional block
    model.add(Conv2D(64, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # second convolutional block
    model.add(Conv2D(128, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    return model
121/119:
discriminator = build_discriminator()
discriminator.summary()
121/120:
def build_discriminator():
    model = Sequential()
    
    # first convolutional block
    # input shape is the generator output shape
    # dropout applies reguralization
    model.add(Conv2D(32, 5, input_shape=(28,28,1)))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # second convolutional block
    model.add(Conv2D(64, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # third convolutional block
    model.add(Conv2D(128, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # fourth convolutional block
    model.add(Conv2D(256, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    return model
121/121:
discriminator = build_discriminator()
discriminator.summary()
121/122:
def build_discriminator():
    model = Sequential()
    
    # first convolutional block
    # input shape is the generator output shape
    # dropout applies reguralization
    model.add(Conv2D(32, 5, input_shape=(28,28,1)))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # second convolutional block
    model.add(Conv2D(64, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # third convolutional block
    model.add(Conv2D(128, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # fourth convolutional block
    model.add(Conv2D(256, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # flatthen then input to dense layer
    model.add(Flatten())
    model.add(Dropout(0.4))
    model.add(Dense(1, activation='sigmoid'))
    
    return model
121/123:
discriminator = build_discriminator()
discriminator.summary()
121/124:
def build_discriminator():
    model = Sequential()
    
    # first convolutional block
    # input shape is the generator output shape
    # dropout applies reguralization
    model.add(Conv2D(32, 5, input_shape=(28,28,1)))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # second convolutional block
    model.add(Conv2D(64, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # third convolutional block
    model.add(Conv2D(128, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # fourth convolutional block
    model.add(Conv2D(256, 5))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.4))
    
    # flatten then input to dense layer
    model.add(Flatten())
    model.add(Dropout(0.4))
    model.add(Dense(1, activation='sigmoid'))
    
    return model
121/125:
discriminator = build_discriminator()
discriminator.summary()
121/126:
img = generator.predict(np.random.randn(4, 128, 1))
discriminator.predict(img)
121/127:
img = generator.predict(np.random.randn(4, 128, 1))
discriminator.predict(img)
121/128:
# Adam is the optimizer for both models, as well BCE
# is their loss function
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossEntropy
121/129:
# Adam is the optimizer for both models, as well BCE
# is their loss function
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
121/130:
# discriminator learns slower than generator
g_opt = Adam(learning_rate=0.0001)
d_opt = Adam(learning_rate=0.00001)

g_loss = BinaryCrossentropy()
d_loss = BinaryCrossentropy()
121/131:
# importing base model class to subclass our training step
from tensorflow.keras.models import Model
121/132:
class FashionGAN(Model):
    def __init__(self, generator, discriminator, *args, **kwargs):
        # pass through args and kwards to base class
        super().__init__(*args, **kwargs)
        
        # creating attr for gen and disc
        self.generator = generator
        self.discriminator = discriminator
        
    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs):
        # compile with base class
        super().compile(*args, **kwargs)
        
        # creating attr for losses and optmizers
        self.g_opt = g_opt
        self.d_opt = d_opt
        self.g_loss = g_loss
        self.d_loss = d_loss
        
    def train_step(self, batch):
        # get data
        real_images = batch
        fake_images = self.generator(tf.random.normal((128, 128, 1)), training=False)
        
        # train the discriminator
        with tf.GradientTape() as d_tape:
            # pass real and fake images to the disc model
            yhat_real = self.discriminator(real_images, training=True)
            yhat_fake = self.discriminator(fake_images, training=True)
            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)
            
            # create labels for real and fake imgs
            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)
            
            # add noise to the outputs (reduces gen learning speed)
            noise_real =  0.15 * tf.random.uniform(tf.shape(yhat_real))
            noise_fake = -0.15 * tf.random.uniform(tf.shape(yhat_fake))
            y_realfake += tf.concat([noise_real, noise_fake], axis=0)
            
            # calculate loss
            total_d_loss = self.d_loss(y_realfake, yhat_realfake)
        
        # apply backpropagation -- nn learn
        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables)
        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))
       
        
        with tf.GradientTape() as g_tape:
            # generate new images
            gen_images = self.generator(tf.random.normal((128, 128, 1)), training=True)
            
            # create predicted labels (disc ouputs 1 for fake images)
            predicted_labels = self.discriminator(gen_images, training=False)
            
            # calculate loss -> the generator is rewarded by faking the discriminator
            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels)
        
        # apply backpropagation
        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)
        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))
        
        
        return {'d_loss':total_d_loss, 'g_loss':total_g_loss}
121/133:
# instance of subclassed model
fashgan = FashionGAN(generator, discriminator)
121/134:
# compile the model
fashgan.compile(g_opt, d_opt, g_loss, d_loss)
121/135:
import os
from tensorflow.keras.preprocessing.image import array_to_img
from tensorflow.keras.calbacks import Callback
121/136:
import os
from tensorflow.keras.preprocessing.image import array_to_img
from tensorflow.keras.callbacks import Callback
121/137:
class ModelMonitor(Callback):
    def __init__(self, num_img=3, latent_dim=128):
        self.num_img = num_img
        self.latent_dim = latent_dim
        
    def on_epoch_end(self, epoch, logs=None):
        random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim, 1))
        generated_images = self.model.generator(random_latent_vectors)
        generated_images *= 255
        generated_images.numpy()
        
        for i in range(self.num_img):
            img = array_to_img(generated_images[i])
            img.save(os.path.join('images', f'generated_img_{epoch}_{i}.png'))
121/138:
# recommend 2000 epochs
hist = fashgan.fit(ds, epochs=20)
121/139: gpus = tf.config.experimental.list_physical_devices('GPU')
121/140:
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
121/141:
for gpu in gpus:
    print(gpu)
121/142:
import tensorflow_datasets as tfds
from matplotlib import pyplot as plt
121/143: gpus = tf.config.experimental.list_physical_devices('GPU')
121/144:
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
121/145:
for gpu in gpus:
    print(gpu)
122/1:
# !pip install torch torchvision torchaudio
# !pip install transformers ipywidgets gradio --upgrade
122/2:
import gradio as gr                 # UI library
from transformers import pipeline   # transformers pipeline
122/3:
import gradio as gr                 # UI library
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM   # transformers pipeline
122/4:
tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
122/5:
# !pip install torch torchvision torchaudio
# !pip install transformers ipywidgets gradio --upgrade
# !pip install sentencepiece
122/6:
import gradio as gr                 # UI library
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM   # transformers pipeline
122/7:
tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
123/1:
# !pip install torch torchvision torchaudio
# !pip install transformers ipywidgets gradio --upgrade
# !pip install sentencepiece
123/2:
import gradio as gr                 # UI library
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM   # transformers pipeline
123/3:
tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
124/1:
# !pip install torch torchvision torchaudio
# !pip install transformers ipywidgets gradio --upgrade
# !pip install sentencepiece sacremoses
124/2:
import gradio as gr                 # UI library
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM   # transformers pipeline
124/3:
tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
124/4:
import gradio as gr                 # UI library
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline   # transformers pipeline
124/5:
#tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
#model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
translation = pipeline('translation_en_to_fr')
124/6:
import gradio as gr                 # UI library
from transformers import pipeline   # transformers pipeline
124/7:
#tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
#model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
translation = pipeline('translation_en_to_fr')
124/8:
#tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
#model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
translation = pipeline('translation_en_to_de')
126/1:
# !pip install torch torchvision torchaudio
# !pip install transformers ipywidgets gradio --upgrade
# !pip install sentencepiece sacremoses
126/2:
import gradio as gr                                             # UI library
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM   # transformers pipeline
126/3:
tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-mul-en')
126/4: model('olá')
126/5: model.predict('olá')
126/6:
input_ids = tokenizer.encode('olá', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
decoded
126/7:
input_ids = tokenizer.encode('olá', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded
126/8:
input_ids = tokenizer.encode('olá', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
type(decoded)
126/9:
input_ids = tokenizer.encode('olá', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded
126/10:
input_ids = tokenizer.encode('olá', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded[3:]
126/11:
input_ids = tokenizer.encode('olá', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded[5:]
126/12:
input_ids = tokenizer.encode('olá', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded[6:]
126/13:
input_ids = tokenizer.encode('olá', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded[6:len(decoded)-1]
126/14:
input_ids = tokenizer.encode('olá', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded[6:len(decoded)-4]
126/15:
input_ids = tokenizer.encode('Meu nome é Williams', return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded[6:len(decoded)-4]
126/16:
input_ids = tokenizer.encode("Je m'appele Williams", return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded[6:len(decoded)-4]
126/17:
input_ids = tokenizer.encode("Comment tu vas?", return_tensors='pt')
outputs = model.generate(input_ids)
decoded = tokenizer.decode(outputs[0])
decoded[6:len(decoded)-4]
126/18:
def translate(input_text, tokenizer, model):
    input_ids = tokenizer.encode("Ich", return_tensors='pt')
    outputs = model.generate(input_ids)
    decoded = tokenizer.decode(outputs[0])
    decoded[6:len(decoded)-4]
126/19: translate('', tokenizer, model)
126/20:
def translate(input_text, tokenizer, model):
    input_ids = tokenizer.encode("Ich", return_tensors='pt')
    outputs = model.generate(input_ids)
    decoded = tokenizer.decode(outputs[0])
    return decoded[6:len(decoded)-4]
126/21: translate('', tokenizer, model)
126/22:
def translate(input_text, tokenizer, model):
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    outputs = model.generate(input_ids)
    decoded = tokenizer.decode(outputs[0])
    return decoded[6:len(decoded)-4]
126/23: translate('', tokenizer, model)
126/24: translate('A', tokenizer, model)
126/25: translate('O', tokenizer, model)
126/26: translate('O homem', tokenizer, model)
126/27: output_text = translate('O homem', tokenizer, model)
126/28: output_text
126/29:
interface = gr.Interface(fn=translate, inputs=gr.inputs.Textbox(lines=2), plaheholder='Text to translate'),
                        outputs='text')
126/30:
interface = gr.Interface(fn=translate, inputs=gr.inputs.Textbox(lines=2), plaheholder='Text to translate'), 
outputs='text')
126/31:
interface = gr.Interface(fn=translate, inputs=gr.inputs.Textbox(lines=2, plaheholder='Text to translate'), 
                        outputs='text')
126/32:
interface = gr.Interface(fn=translate, inputs=gr.inputs.Textbox(lines=2, placeholder='Text to translate'), 
                        outputs='text')
126/33:
interface = gr.Interface(fn=translate, inputs=gr.components.Textbox(lines=2, placeholder='Text to translate'), 
                        outputs='text')
126/34: interface.launch()
126/35:
def translate(input_text):
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    outputs = model.generate(input_ids)
    decoded = tokenizer.decode(outputs[0])
    return decoded[6:len(decoded)-4]
126/36: output_text = translate('O homem', tokenizer, model)
126/37: output_text = translate('O homem')
126/38:
def translate(input_text):
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    outputs = model.generate(input_ids)
    decoded = tokenizer.decode(outputs[0])
    return decoded[6:len(decoded)-4]
126/39: output_text = translate('O homem')
126/40: output_text
126/41:
interface = gr.Interface(fn=translate, inputs=gr.components.Textbox(lines=2, placeholder='Text to translate'), 
                        outputs='text')
126/42: interface.launch()
126/43:
with gr.Blocks() as interface:
    input_text = gr.components.Textbox(lines=2, placeholder='Type to translate from any language...', label="Name")
    output_text = gr.Textbox(label="English translation")
    translate_btn = gr.Button("Translate")
    translate_btn.click(fn=translate, inputs=input_text, outputs=output_text)
126/44: interface.launch()
126/45:
with gr.Blocks() as interface:
    input_text = gr.components.Textbox(lines=2, placeholder='Type to translate from any language...', label="Name")
    output_text = gr.Textbox(label="English translation")
    translate_btn = gr.Button("Translate")
    translate_btn.click(fn=translate, inputs=input_text, outputs=output_text)
    
interface = gr.interface(fn=translate,
                         inputs=gr.components.Textbox(lines=2, placeholder='Type to translate from any language.', label='FROM TEXT'),
                         outputs=gr.components.Textbox(label='TRANSLATED TEXT'))
126/46:
interface = gr.interface(fn=translate,
                         inputs=gr.components.Textbox(lines=2, placeholder='Type to translate from any language.', label='FROM TEXT'),
                         outputs=gr.components.Textbox(label='TRANSLATED TEXT'))
126/47:
interface = gr.Interface(fn=translate,
                         inputs=gr.components.Textbox(lines=2, placeholder='Type to translate from any language.', label='FROM TEXT'),
                         outputs=gr.components.Textbox(label='TRANSLATED TEXT'))
126/48: interface.launch()
126/49:
interface = gr.Interface(fn=translate,
                         inputs=gr.components.Textbox(lines=2, placeholder='Type to translate from any language.', label='FROM TEXT'),
                         outputs='text')
126/50: interface.launch()
126/51:
interface = gr.Interface(fn=translate,
                         inputs=gr.components.Textbox(lines=2, placeholder='Type to translate from any language.', label='FROM TEXT'),
                         outputs=gr.components.Textbox(label='TRANSLATED TEXT'))
126/52: interface.launch()
126/53: interface.launch()
127/1: import pandas as pd
127/2: base = pd.read_csv("./sisu-2022-1.csv", header=0, encoding='mbcs')
127/3: base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs')
127/4: base.head()
127/5: base
127/6: base.shape
127/7: base
127/8: base.head()
127/9: base.columns
127/10: base.DS_PERIODICIDADE
127/11: base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
127/12: base.DS_PERIODICIDADE
127/13: base
127/14: base.groupby('UF_IES')
127/15: base.groupby(by='UF_IES')
127/16: base.UF_IES
127/17: base.groupby(by="UF_IES", dropna=False).sum()
127/18: base.groupby(by="UF_IES", dropna=False).sum(numeric_only=False)
128/1: import pandas as pd
128/2: base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
128/3: base.groupby(by="UF_IES", dropna=False).sum(numeric_only=True)
128/4: base.groupby(by="UF_IES", dropna=False).sum(numeric_only=True).shape
128/5: base.groupby(by="UF_IES", dropna=False).sum(numeric_only=True)
128/6: base.groupby(by="UF_IES", dropna=False).value_counts()
128/7: base[base.UF_IES == 'PE']
128/8: base[base.UF_IES == 'PE'].head()
128/9:
import pandas as pd
pd.set_option('display.max_columns', None)
128/10: base[base.UF_IES == 'PE'].head()
128/11: base[base.NOME_CURSO == 'MEDICINA'].head()
128/12: base[base.NOME_CURSO == 'MEDICINA'].shape
128/13: medicina = base[base.NOME_CURSO == 'MEDICINA']
128/14:
med_por_estado = medicina.groupby(by='UF_IES')
med_por_estado
128/15:
med_por_estado = medicina.UF_IES
med_por_estado
128/16:
med_por_estado = medicina.UF_IES.value_counts()
med_por_estado
128/17: base.head()
128/18:
med_por_estado = medicina.UF_CANDIDATO.value_counts()
med_por_estado
128/19:
# Importing dependencies
import pandas as pd
pd.set_option('display.max_columns', None)
128/20: base.shape
128/21: base.head()
128/22: base.shape
128/23: base.APROVADO.value_counts()
128/24:
aprovados = base[base.APROVADO == 'S']
percentual_aprovados = aprovados.shape[0] / base.shape[0]
percentual_aprovados
128/25:
aprovados = base[base.APROVADO == 'S']
percentual_aprovados = aprovados.shape[0] / base.shape[0]
print(round(percentual_aprovados * 100, 2))
128/26:
aprovados = base[base.APROVADO == 'S']
percentual_aprovados = aprovados.shape[0] / base.shape[0]
print('Percentual de aprovados no SISU 2022: ', round(percentual_aprovados * 100, 2), '%')
128/27:
aprovados = base[base.APROVADO == 'S']
percentual_aprovados = aprovados.shape[0] / base.shape[0]
print('Percentual de aprovados no SISU 2022:', round(percentual_aprovados * 100, 2), '%')
128/28: base.head()
128/29: cortes_cursos = base['NOME_CURSO', 'NOTA_CORTE']
128/30: cortes_cursos = base['UF_IES', 'NOTA_CORTE']
128/31: cortes_cursos = base[['UF_IES', 'NOTA_CORTE']]
128/32: cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
128/33:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos
128/34:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos = cortes_cursos.groupby(by='NOME_CURSO').mean()
128/35:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos = cortes_cursos.groupby(by='NOME_CURSO').mean()
cortes_cursos
128/36:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.dtypes
128/37:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.astype(float)
128/38:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.astype(float)
print(7.8)
128/39:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.astype(float)
print(7,8)
128/40:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.')
128/41:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.loc[:, 'NOTA_CORTE'] = 's'
128/42:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.loc[:, 'NOTA_CORTE'] = 's'
cortes_cursos
128/43:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.loc[:, 'NOTA_CORTE'] = str.replace(',', '.')
cortes_cursos
128/44:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.loc[:, 'NOTA_CORTE'] = 1
cortes_cursos
128/45:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.loc[:, 'NOTA_CORTE'] = cortes_cursos.NOTA_CORTE.str.replace(',', '.')
cortes_cursos
128/46:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.loc[:, 'NOTA_CORTE'].str.replace(',', '.')
cortes_cursos
128/47:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
nota_corte = cortes_cursos.loc[:, 'NOTA_CORTE']
cortes_cursos
128/48:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
nota_corte = cortes_cursos.loc[:, 'NOTA_CORTE']
nota_corte = nota_corte.str.replace(',', '.')
cortes_cursos
128/49:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
nota_corte = cortes_cursos.loc[:, 'NOTA_CORTE']
nota_corte = nota_corte.str.replace(',', '.')
cortes_cursos.NOTA_CORTE = nota_corte
128/50:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
nota_corte = cortes_cursos.loc[:, 'NOTA_CORTE']
nota_corte = nota_corte.str.replace(',', '.')
cortes_cursos.loc[:, 'NOTA_CORTE'] = nota_corte
128/51:
# Importing dependencies
import pandas as pd
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
128/52:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
nota_corte = cortes_cursos.loc[:, 'NOTA_CORTE']
nota_corte = nota_corte.str.replace(',', '.')
cortes_cursos.loc[:, 'NOTA_CORTE'] = nota_corte
128/53:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.')
cortes_curtos
128/54:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.')
cortes_cursos
128/55:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos
128/56:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos.dtypes
128/57:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos
128/58:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by='NOME_CURSO').mean()
128/59:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by='NOME_CURSO').mean()
cortes_cursos
128/60:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos = pd.unique(cortes_cursos)
cortes_cursos
128/61:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by='NOME_CURSO').mean().sort_values(ascending=False)
cortes_cursos
128/62:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by='NOME_CURSO').mean().sort_values(by='NOTA_CORTE', ascending=False)
cortes_cursos
128/63:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by='NOME_CURSO').mean().sort_values(by='NOTA_CORTE', ascending=False)
cortes_cursos
128/64:
cortes_cursos = base[['NOME_CURSO', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by='NOME_CURSO').mean().sort_values(by='NOTA_CORTE', ascending=False)
cortes_cursos.head(10)
128/65:
cortes_cursos = base[['NOME_CURSO', 'UF_IES', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by='NOME_CURSO').mean().sort_values(by='NOTA_CORTE', ascending=False)
cortes_cursos.head(10)
128/66:
cortes_cursos = base[['NOME_CURSO', 'UF_IES', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by=['NOME_CURSO', 'UF_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)
cortes_cursos.head(10)
128/67:
cortes_cursos = base[['NOME_CURSO', 'UF_IES', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by=['NOME_CURSO', 'UF_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)
cortes_cursos.head(20)
128/68:
cortes_cursos = base[['UF_IES', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by='UF_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
cortes_cursos.head(20)
128/69: base.head()
128/70:
cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
cortes_cursos.head(20)
128/71: base.head()
128/72:
aprovados = base[base.APROVADO == 'S']
aprovados

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/73:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/74:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/75:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean()

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/76:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES'], aprovados.NOTA_CORTE).mean()

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/77:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean()

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/78:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/79:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/80:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados.head(10)
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/81:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados.head(20)
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/82:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados = aprovados.drop_duplicates()
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/83:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados = aprovados.drop_duplicates()
aprovados
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/84:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['SIGLA_IES', 'NOME_CURSO', 'NOTA_CORTE']]
aprovados = aprovados.drop_duplicates().reset_index(drop=True)
aprovados
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/85:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['NOME_CURSO', 'SIGLA_IES', 'NOTA_CORTE']]
aprovados = aprovados.drop_duplicates().reset_index(drop=True)
aprovados
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/86:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
aprovados = aprovados[['NOME_CURSO', 'SIGLA_IES', 'NOTA_CORTE']]
aprovados = aprovados.drop_duplicates().reset_index(drop=True)
aprovados.head(10)
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/87:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/88:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/89: base.head()
128/90:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/91:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/92:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/93:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = np.where(corte.SIGLA_IES == 'UFPE' && corte.TIPO_MOD_CONCORRENCIA == 'A')
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/94:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = np.where(corte.SIGLA_IES == 'UFPE' & corte.TIPO_MOD_CONCORRENCIA == 'A')
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/95:
# Importing dependencies
import pandas as pd
import numpy as np
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
128/96:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = np.where(corte.SIGLA_IES == 'UFPE' & corte.TIPO_MOD_CONCORRENCIA == 'A')
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/97:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/98:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/99:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/100:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe[corte.TIPO_MOD_CONCORRENCIA == 'A'].reset_index(drop=True)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/101:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe[corte.TIPO_MOD_CONCORRENCIA == 'A'].reset_index(drop=True)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/102:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe[corte.TIPO_MOD_CONCORRENCIA == 'A'].reset_index(drop=True)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/103:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES']).max().reset_index(drop=True)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/104:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES']).max().reset_index(drop=True)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/105:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES']).max().reset_index(drop=True)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/106:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES']).max()
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/107:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/108: base.head(50)
128/109: base[base.TIPO_MOD]
128/110: base[base.TIPO_MOD_CONCORRENCIA == 'L']
128/111:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/112:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/113:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES']).max(axis=1).sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/114:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/115:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/116:
teste = base[base.TIPO_MOD_CONCORRENCIA == 'A']
teste = teste[teste.SIGLA_IES == 'UFPE']
teste
128/117:
teste = base[base.TIPO_MOD_CONCORRENCIA == 'A']
teste = teste[teste.SIGLA_IES == 'UFPE'].sort_values(by='NOTA_CORTE')
teste
128/118:
teste = base[base.TIPO_MOD_CONCORRENCIA == 'A']
teste = teste[teste.SIGLA_IES == 'UFPE'].sort_values(by='NOTA_CORTE', ascending=False)
teste
128/119:
teste = base[base.TIPO_MOD_CONCORRENCIA == 'A']
teste = teste[teste.SIGLA_IES == 'UFPE'].sort_values(by='NOTA_CORTE', ascending=False)
teste
128/120:
teste = base[base.TIPO_MOD_CONCORRENCIA == 'A']
teste = teste[teste.SIGLA_IES == 'UFPE'].sort_values(by='NOTA_CORTE', ascending=False)
teste.head(20)
128/121:
teste = base[base.TIPO_MOD_CONCORRENCIA == 'A']
teste = teste[teste.SIGLA_IES == 'UFPE'].sort_values(by='NOTA_CORTE', ascending=False)
teste.head(50)
128/122:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/123:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/124:
aprovados = base[base.APROVADO == 'S'].sort_values(by=['NOTA_CORTE', 'TIPO_MOD_CONCORRENCIA'], ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(10)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/125:
aprovados = base[base.APROVADO == 'S'].sort_values(by=['NOTA_CORTE', 'TIPO_MOD_CONCORRENCIA'], ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/126:
aprovados = base[base.APROVADO == 'S'].sort_values(by=['NOTA_CORTE', 'TIPO_MOD_CONCORRENCIA'], ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/127:
aprovados = base[base.APROVADO == 'S'].sort_values(by=['NOTA_CORTE', 'TIPO_MOD_CONCORRENCIA'], ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/128:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/129:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/130:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/131:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/132:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/133:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/134:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = corte[corte.SIGLA_IES == 'UFPE']
teste.head()

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/135:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = corte[corte.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/136:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/137:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste.APROVADO.value_counts()

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/138:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/139:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste.TIPO_MOD_CONCORRENCIA.value_counts()

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/140:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste.TIPO_MOD_CONCORRENCIA.value_counts()

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/141:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/142:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste.MOD_CONCORRENCIA

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/143:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste.loc[1109670, 'MOD_CONCORRENCIA']

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/144:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste = teste[teste.TIPO_MOD_CONCORRENCIA == ('A' or 'B')]
# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/145:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste = teste[teste.TIPO_MOD_CONCORRENCIA == ('A' or 'B')]
teste
# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/146:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste = teste[teste.TIPO_MOD_CONCORRENCIA == 'A']
teste
# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/147:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste = teste[teste.TIPO_MOD_CONCORRENCIA == 'B']
teste
# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/148:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0)

#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

teste = base[base.SIGLA_IES == 'UFPE']
teste = teste[teste.NOME_CURSO == 'MEDICINA']
teste = teste[teste.APROVADO == 'S']
teste = teste[teste.TIPO_MOD_CONCORRENCIA == 'B']
teste
# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/149:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0)

#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
#corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
#corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/150:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0)

#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)

#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/151:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0)

#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)
corte_bonus
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/152:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0)

#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)
corte_bonus[corte_bonus.SIGLA_IES == 'UFPE']
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/153:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0)

#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)
corte[corte.SIGLA_IES == 'UFPE']
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/154:
aprovados = base[base.APROVADO == 'S'].sort_values(by='NOTA_CORTE', ascending=False)
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)

#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)
corte[corte.SIGLA_IES == 'UFPE']
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/155:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)

#corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)
corte[corte.SIGLA_IES == 'UFPE']
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/156:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)
corte[corte.SIGLA_IES == 'UFPE']
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/157:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)
corte = corte.drop_duplicates().reset_index(drop=True)
corte.head(20)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
#corte_ufpe.NOTA_CORTE = corte_ufpe.NOTA_CORTE.str.replace(',', '.').astype(float)
#corte_ufpe = corte_ufpe.groupby(by=['NOME_CURSO', 'SIGLA_IES', 'TIPO_MOD_CONCORRENCIA']).max().sort_values(by='NOTA_CORTE', ascending=False)
corte_ufpe.head(10)
corte[corte.SIGLA_IES == 'UFPE']
#aprovados.NOTA_CORTE = aprovados.NOTA_CORTE.str.replace(',', '.').astype(float)
#aprovados.groupby(by=['NOME_CURSO', 'SIGLA_IES']).mean().sort_values(by='NOTA_CORTE', ascending=False)

#cortes_cursos = base[['NOME_IES', 'NOTA_CORTE']]
#cortes_cursos.NOTA_CORTE = cortes_cursos.NOTA_CORTE.str.replace(',', '.').astype(float)
#cortes_cursos = cortes_cursos.groupby(by='NOME_IES').mean().sort_values(by='NOTA_CORTE', ascending=False)
#cortes_cursos.head(20)
128/158:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)
corte = corte.drop_duplicates().reset_index(drop=True)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência e bonificação
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.head(10)
128/159:
teste = base[base.TIPO_MOD_CONCORRENCIA == 'A']
teste = teste[teste.SIGLA_IES == 'UFPE'].sort_values(by='NOTA_CORTE', ascending=False)
teste[teste.NOME_CURSO == 'ODONTOLOGIA']
128/160:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TURNO', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)
corte = corte.drop_duplicates().reset_index(drop=True)

# USP, UNICAMP, UFPE, UFRJ e UFMG

# UFPE ampla concorrência e bonificação
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.head(10)
128/161:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TURNO', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)
corte = corte.drop_duplicates().reset_index(drop=True)
128/162:
# UFPE ampla concorrência e bonificação

corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.head(10)
128/163:
# UFPE ampla concorrência e bonificação

corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe.head(6)
128/164: corte_ufpe.TIPO_MOD_CONCORRENCIA.value_counts()
128/165: corte_ufpe[corte_ufpe.NOME_CURSO == 'MEDICINA'].TIPO_MOD_CONCORRENCIA.value_counts()
128/166:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TURNO', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)
128/167:
# UFPE ampla concorrência e bonificação

corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe.drop_duplicates().reset_index(drop=True)
corte_ufpe.head(6)
128/168: corte_ufpe[corte_ufpe.NOME_CURSO == 'MEDICINA'].TIPO_MOD_CONCORRENCIA.value_counts()
128/169:
aprovados_ufpe = aprovados[aprovados.SIGLA_IES == 'UFPE']
med_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'MEDICINA']
med_ufpe.TIPO_MOD_CONCORRENCIA.value_counts()
128/170:
# Importing dependencies
import pandas as pd
import numpy as np
import seaborn as sns
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
128/171:
# UFPE ampla concorrência e bonificação

corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe.drop_duplicates().reset_index(drop=True)
corte_ufpe.head(6)
128/172: sns.barplot(data=corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
128/173:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TURNO', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)
corte.NOTA_CORTE = corte.NOTA_CORTE.str.replace(',', '.').astype(float)
128/174:
# UFPE ampla concorrência e bonificação

corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe.drop_duplicates().reset_index(drop=True)
corte_ufpe.head(6)
128/175: sns.barplot(data=corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
128/176:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
128/177:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
128/178:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS", orient='h')
128/179:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOTA_CORTE", y="NOME_CURSO", hue="MUNICIPIO_CAMPUS", orient='h')
128/180:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
128/181:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=90)
plt.show()
128/182:
# Importing dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
128/183:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=90)
plt.show()
128/184:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=270)
plt.show()
128/185:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=90)
plt.show()
128/186:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.show()
128/187:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.xlabel('')
plt.show()
128/188:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
128/189:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45, range(0, 800, 50))
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
128/190:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(range(0, 800, 50))
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
128/191:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
128/192:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.yticks(range(0, 800, 50))
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
128/193:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.yticks(range(0, 850, 50))
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
128/194:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)

plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
128/195:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.yticks(range(0, 950, 50))
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
128/196: base.head()
128/197:
# Importing dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import geopandas as gpd
import seaborn as sns
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
128/198: estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
128/199:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados
128/200:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados.shape
128/201:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados
128/202:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados.geometry
128/203:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados.geometry[2]
128/204:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados.geometry[24]
128/205:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados.geometry[24]
print(estados.nome[24])
128/206:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a', encoding='mbcs')
estados.geometry[24]
print(estados.nome[24])
128/207:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a', encoding='utf-8')
estados.geometry[24]
print(estados.nome[24])
128/208:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a', encoding='mbcs')
estados.geometry[24]
print(estados.nome[24])
128/209:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a', encoding='mbcs')
estados.geometry[2]
print(estados.nome[2])
128/210:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a', encoding='mbcs')
estados.geometry[2]
print(estados.nome[2])
128/211:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a', encoding='mbcs')
estados.geometry[1]
print(estados.nome[2])
128/212:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados.geometry[1]
print(estados.nome[2])
128/213:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados.geometry[24]
print(estados.nome[2])
128/214:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados.geometry[24]
128/215:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layers='lim_unidade_federacao_a')
estados.geometry[2]
128/216:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.geometry[2]
128/217:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
print(estados.nome[2])
estados.geometry[2]
128/218:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
print(estados.nome[24])
estados.geometry[24]
128/219:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot()
128/220:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black')
128/221:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds')
128/222:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)
128/223:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe.TIPO_MOD_CONCORRENCIA.value_counts()
128/224: base
128/225:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe.UF_CANDIDATO.value_counts()
128/226:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
128/227:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']
ec_ufpe
128/228:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.column = ['UF', 'COUNT']
ec_ufpe
128/229:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']
ec_ufpe
128/230:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']
ec_ufpe
128/231:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='COUNT', how='left')
estados
128/232:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

#estados = estados.merge(ec_ufpe, on='COUNT', how='left')
estados
128/233:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados.rename({'sigla': 'UF'})
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

#estados = estados.merge(ec_ufpe, on='COUNT', how='left')
estados
128/234:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'})
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

#estados = estados.merge(ec_ufpe, on='COUNT', how='left')
estados
128/235:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

#estados = estados.merge(ec_ufpe, on='COUNT', how='left')
estados
128/236:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados.plot(edgecolor='black', cmap='Reds', legend=True)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados
128/237:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados.plot(edgecolor='black', cmap='Reds', legend=True)
estados.COUNT = estados.COUNT.fillna(0)

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados
128/238:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados.plot(edgecolor='black', cmap='Reds', legend=True)


ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados
128/239:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados.plot(edgecolor='black', cmap='Reds', legend=True)


ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/240:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
128/241:
ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/242:
cursos = most_corte_ufpe.NOME_CURSO
fig, ax = plt.subplots(nrows=2, ncol=3, figsize=(10, 10))


ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/243:
cursos = most_corte_ufpe.NOME_CURSO
fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 10))


ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/244:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
128/245:
cursos = most_corte_ufpe.NOME_CURSO
fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(10, 10))


ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/246:
cursos = most_corte_ufpe.NOME_CURSO
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))

for i

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/247:
cursos = most_corte_ufpe.NOME_CURSO
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/248:
cursos = most_corte_ufpe.NOME_CURSO
#fig, ax = plt.subplots(ncols=5, figsize=(10, 10))

#for i in range(5):
#    ax[0].plot()

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/249:
cursos = most_corte_ufpe.NOME_CURSO
#fig, ax = plt.subplots(ncols=5, figsize=(10, 10))

for i in range(5):
    ax[0].plt()

ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/250:
cursos = most_corte_ufpe.NOME_CURSO
#fig, ax = plt.subplots(ncols=5, figsize=(10, 10))



ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/251:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
128/252:
cursos = most_corte_ufpe.NOME_CURSO
#fig, ax = plt.subplots(ncols=5, figsize=(10, 10))



ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/253:
cursos = most_corte_ufpe.NOME_CURSO
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))



ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/254:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
128/255:
cursos = most_corte_ufpe.NOME_CURSO
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))



ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/256:
cursos = most_corte_ufpe.NOME_CURSO
print(cursos)
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))



ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/257:
cursos = most_corte_ufpe.NOME_CURSO
print(cursos.index)
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))



ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/258:
cursos = most_corte_ufpe.NOME_CURSO
print(cursos.to_numpy())
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))



ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'ENGENHARIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/259:
cursos = most_corte_ufpe.NOME_CURSO
print(cursos.to_numpy()[1:])
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))

#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == cursos[i]]
    ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
    ec_ufpe.columns = ['UF', 'COUNT']

    estados = estados.merge(ec_ufpe, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/260:
cursos = most_corte_ufpe.NOME_CURSO.to_numpy()[1:]
print(cursos)
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))

#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == cursos[i]]
    ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
    ec_ufpe.columns = ['UF', 'COUNT']

    estados = estados.merge(ec_ufpe, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/261:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
128/262:
cursos = most_corte_ufpe.NOME_CURSO.to_numpy()[1:]
print(cursos)
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))

#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == cursos[i]]
    ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
    ec_ufpe.columns = ['UF', 'COUNT']

    estados = estados.merge(ec_ufpe, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/263:
cursos = most_corte_ufpe.NOME_CURSO.to_numpy()[1:]
print(cursos)
fig, ax = plt.subplots(ncols=5, figsize=(10, 10))

#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == cursos[i]]
    ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
    ec_ufpe.columns = ['UF', 'COUNT']

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(ec_ufpe, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/264:
cursos = most_corte_ufpe.NOME_CURSO.to_numpy()[1:]
print(cursos)
fig, ax = plt.subplots(ncols=5, figsize=(20, 20))

#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == cursos[i]]
    ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
    ec_ufpe.columns = ['UF', 'COUNT']

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(ec_ufpe, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[i])
128/265:
cursos = most_corte_ufpe.NOME_CURSO.to_numpy()[1:]
fig, ax = plt.subplots(ncols=5, figsize=(20, 20))

#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == cursos[i]]
    ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
    ec_ufpe.columns = ['UF', 'COUNT']

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(ec_ufpe, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, legend_kwds={'shrink': 0.3}, ax=ax[i])
128/266:
cursos = most_corte_ufpe.NOME_CURSO.to_numpy()[1:]
fig, ax = plt.subplots(ncols=5, figsize=(30, 30))

#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == cursos[i]]
    ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
    ec_ufpe.columns = ['UF', 'COUNT']

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(ec_ufpe, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, legend_kwds={'shrink': 0.3}, ax=ax[i])
128/267:
cursos = most_corte_ufpe.NOME_CURSO.to_numpy()[1:]
fig, ax = plt.subplots(ncols=5, figsize=(30, 30))

#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

for i in range(5):
    print(cursos[i])
    ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == cursos[i]]
    ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
    ec_ufpe.columns = ['UF', 'COUNT']

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(ec_ufpe, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, legend_kwds={'shrink': 0.3}, ax=ax[i])
128/268:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'CIÊNCIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, legend_kwds={'shrink': 0.3}, ax=ax[i])
128/269:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
ec_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'CIÊNCIA DA COMPUTAÇÃO']
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, legend_kwds={'shrink': 0.3})
128/270:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
ec_ufpe = aprovados_ufpe
ec_ufpe = ec_ufpe.UF_CANDIDATO.value_counts().reset_index()
ec_ufpe.columns = ['UF', 'COUNT']

estados = estados.merge(ec_ufpe, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, legend_kwds={'shrink': 0.3})
128/271:
estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
aprovados_med_count

#estados = estados.merge(ec_ufpe, on='UF', how='left')
#estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, legend_kwds={'shrink': 0.3})
128/272:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
aprovados_med_count

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)
128/273:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
aprovados_med_count

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

plt.title("a")
plt.show()
128/274:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
aprovados_med_count

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

plt.title("Distribuição por estado dos aprovados em Medicina pelo SISU 2022")
plt.show()
128/275:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
aprovados_med_count

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/276:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()

aprovados_med_count
128/277:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

aprovados_med_count

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/278:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
print(
aprovados_med_count)

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/279:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
print(aprovados_med_count.head())

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/280:
fig, ax = plt.subplots(ncols=2, figsize=(20, 20))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1])

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/281:
fig, ax = plt.subplots(ncols=2, figsize=(10, 10))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1])

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/282:
fig, ax = plt.subplots(ncols=2, figsize=(8, 10))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1])

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/283:
fig, ax = plt.subplots(ncols=2, figsize=(6, 10))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1])

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/284:
fig, ax = plt.subplots(ncols=2, figsize=(8, 6))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1])

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/285:
fig, ax = plt.subplots(ncols=2, figsize=(8, 4))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1])

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/286:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1])

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/287:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], figsize=(10,10))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/288:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], figsize=(20,10))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/289:
fig, ax = plt.subplots(ncols=2)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], figsize=(20,10))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/290:
fig, ax = plt.subplots(ncols=2)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1])

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/291:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], figsize=(20,10))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/292:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], figsize=(20,10))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/293:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.3})

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/294:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.4})

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/295:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.5})

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/296:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.6})

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/297:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55})

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/298:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55})

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/299:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 5))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/300:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/301:
fig, ax = plt.subplots(ncols=2)
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/302:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/303:
fig, ax = plt.subplots(ncols=2, figsize=(8, 6))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/304:
fig, ax = plt.subplots(ncols=2, figsize=(8, 10))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/305:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/306:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout(pad=5.0)

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/307:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0])

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/308:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="flare")

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/309:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="r_flare")

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/310:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="flare_r")

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/311:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="rocket_r")

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/312:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/313: base.head(1)
128/314:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DA INFORMAÇÃO']

aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]
aprovados_tech_count = aprovados_tech.UF_CANDIDATO.value_counts().reset_index()
aprovados_tech_count.columns = ['UF', 'COUNT']
aprovados_tech
"""sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()"""
128/315:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DA INFORMAÇÃO']

aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]
aprovados_tech_count = aprovados_tech.UF_CANDIDATO.value_counts().reset_index()
aprovados_tech_count.columns = ['UF', 'COUNT']

#sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

#estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
#estados = estados.rename({'sigla': 'UF'}, axis=1)
#estados = estados.merge(aprovados_med_count, on='UF', how='left')
#estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()"""
128/316:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DA INFORMAÇÃO']

aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]
aprovados_tech_count = aprovados_tech.UF_CANDIDATO.value_counts().reset_index()
aprovados_tech_count.columns = ['UF', 'COUNT']
aprovados_tech
#sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

#estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
#estados = estados.rename({'sigla': 'UF'}, axis=1)
#estados = estados.merge(aprovados_med_count, on='UF', how='left')
#estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
#plt.show()"""
128/317:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DA INFORMAÇÃO']

aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]
aprovados_tech_count = aprovados_tech.UF_CANDIDATO.value_counts().reset_index()
aprovados_tech_count.columns = ['UF', 'COUNT']
aprovados_tech.NOME_CURSO.value_counts()
#sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

#estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
#estados = estados.rename({'sigla': 'UF'}, axis=1)
#estados = estados.merge(aprovados_med_count, on='UF', how='left')
#estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
#plt.show()"""
128/318:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']

aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]
aprovados_tech_count = aprovados_tech.UF_CANDIDATO.value_counts().reset_index()
aprovados_tech_count.columns = ['UF', 'COUNT']
aprovados_tech.NOME_CURSO.value_counts()
#sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

#estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
#estados = estados.rename({'sigla': 'UF'}, axis=1)
#estados = estados.merge(aprovados_med_count, on='UF', how='left')
#estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
#plt.show()"""
128/319:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']

aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]
aprovados_tech_count = aprovados_tech.UF_CANDIDATO.value_counts().reset_index()
aprovados_tech_count.columns = ['UF', 'COUNT']
aprovados_tech_count
#sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

#estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
#estados = estados.rename({'sigla': 'UF'}, axis=1)
#estados = estados.merge(aprovados_med_count, on='UF', how='left')
#estados.COUNT = estados.COUNT.fillna(0)
#estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
#plt.show()"""
128/320:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
aprovados_med_count = aprovados_med.UF_CANDIDATO.value_counts().reset_index()
aprovados_med_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_med_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_med_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/321:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']

aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]
aprovados_tech_count = aprovados_tech.UF_CANDIDATO.value_counts().reset_index()
aprovados_tech_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_tech_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_tech_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em Computação pelo SISU 2022")
plt.show()
128/322:
fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
fig.tight_layout()

cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']

aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]
aprovados_tech_count = aprovados_tech.UF_CANDIDATO.value_counts().reset_index()
aprovados_tech_count.columns = ['UF', 'COUNT']
sns.barplot(data=aprovados_tech_count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
estados = estados.rename({'sigla': 'UF'}, axis=1)
estados = estados.merge(aprovados_tech_count, on='UF', how='left')
estados.COUNT = estados.COUNT.fillna(0)
estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

plt.title("Distribuição por estado dos aprovados\n em cursos de computação pelo SISU 2022")
plt.show()
128/323:
def situacao_por_estado(aprovados_curso):
    fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
    fig.tight_layout()
    
    count = aprovados_curso.UF_CANDIDATO.value_counts().reset_index()
    count.columns = ['UF', 'COUNT']
    sns.barplot(data=count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(count, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))

    plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
    plt.show()
128/324:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
situacao_por_estado(aprovados_med)
128/325:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
situacao_por_estado(aprovados_med)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/326:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
situacao_por_estado(aprovados_med)

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/327:
def situacao_por_estado(aprovados_curso):
    fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
    fig.tight_layout()
    
    count = aprovados_curso.UF_CANDIDATO.value_counts().reset_index()
    count.columns = ['UF', 'COUNT']
    sns.barplot(data=count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(count, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))
128/328:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
situacao_por_estado(aprovados_med)

#plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/329:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
situacao_por_estado(aprovados_med)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/330:
def situacao_por_estado(aprovados_curso):
    fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
    fig.tight_layout()
    
    count = aprovados_curso.UF_CANDIDATO.value_counts().reset_index()
    count.columns = ['UF', 'COUNT']
    sns.barplot(data=count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(count, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))
128/331:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
situacao_por_estado(aprovados_med)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
128/332:
cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']
aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]

plt.title("Distribuição por estado dos aprovados\n em cursos de computação pelo SISU 2022")
plt.show()
128/333:
cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']
aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]
situacao_por_estado(aprovados_tech)

plt.title("Distribuição por estado dos aprovados\n em cursos de computação pelo SISU 2022")
plt.show()
128/334:
cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']
aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]

situacao_por_estado(aprovados_tech)

plt.title("Distribuição por estado dos aprovados\n em cursos de computação pelo SISU 2022")
plt.show()
128/335: base.head(1)
128/336:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas
128/337:
# Reading base dataset
base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
base = base.str.replace(',', '.')
128/338:
# Reading base dataset
base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
128/339:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
for column in notas:
    base[column] = base[column].str.replace(',', '.').astype(float)
128/340:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
for column in notas:
    print(column)
    base[column] = base[column].str.replace(',', '.').astype(float)
128/341:
# Reading base dataset
base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
128/342:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
for column in notas:
    print(column)
    base[column] = base[column].str.replace(',', '.').astype(float)
128/343:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
base['NOTA_CORTE']
for column in notas:
    print(column)
    base[column] = base[column].str.replace(',', '.').astype(float)
128/344:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
base['NOTA_CORTE']
for column in notas:
    print(column)
    #base[column] = base[column].str.replace(',', '.').astype(float)
128/345:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
base['NOTA_CORTE']
for column in notas:
    print(column)
    #base[column] = base[column].str.replace(',', '.').astype(float)

base['NOTA_CORTE']
128/346:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
base['NOTA_CORTE']
for column in notas:
    print(column)
    #base[column] = base[column].str.replace(',', '.').astype(float)

base
128/347:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
base['NOTA_CORTE']
for column in notas:
    print(column)
    #base[column] = base[column].str.replace(',', '.').astype(float)

base.head()
128/348:
# Reading base dataset
base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
128/349:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
for column in notas:
    base[column] = base[column].str.replace(',', '.').astype(float)

base.head()
128/350:
# Reading base dataset
base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
128/351:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']
for column in notas:
    base[column].str.replace(',', '.').astype(float)

base.head()
128/352:
# Reading base dataset
base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
128/353: base.NOTA_L
128/354: base.NOTA_R
128/355: base.NOTA_M
128/356: base.NOTA_CN
128/357: base.NOTA_CH
128/358: base.NOTA_R
128/359: base.NOTA_CORTE
128/360:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M']
for column in notas:
    base[column].str.replace(',', '.').astype(float)

base.head()
128/361:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M']
for column in notas:
    base[column] = base[column].str.replace(',', '.').astype(float)

base.head()
128/362:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M']
for column in notas:
    base[column] = base[column].str.replace(',', '.').astype(float)
128/363:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas
128/364:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]

sns.heatmap(notas, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/365:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_heatmap = notas.pivot_table(index='UF', aggfunc='mean')

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/366:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_heatmap = notas.pivot_table(index='UF_CANDIDATO', aggfunc='mean')

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Pass accuracy (%) for group stages' first places")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/367:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_heatmap = notas.pivot_table(index='UF_CANDIDATO', aggfunc='mean')

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Nota média dos candidatos por UF e áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/368:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_heatmap = notas.pivot_table(index='UF_CANDIDATO', aggfunc='mean')

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/369:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_heatmap = notas.pivot_table(index='UF_CANDIDATO', aggfunc='mean')

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.8, linecolor='black')
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/370:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_heatmap = notas.pivot_table(index='UF_CANDIDATO', aggfunc='mean')

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black')
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/371:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_heatmap = notas.pivot_table(index='UF_CANDIDATO', aggfunc='mean')

fig, ax = plt.subplots(figsize=(10,10))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/372:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_heatmap = notas.pivot_table(index='UF_CANDIDATO', aggfunc='mean')

fig, ax = plt.subplots(figsize=(10,8))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/373:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_heatmap = notas.pivot_table(index='UF_CANDIDATO', aggfunc='mean')

fig, ax = plt.subplots(figsize=(6,8))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/374:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas.columns = ['UF', 'LING', 'HUM', 'NAT', 'MAT', 'RED']
notas_heatmap = notas.pivot_table(index='UF', aggfunc='mean')

fig, ax = plt.subplots(figsize=(6,8))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/375:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas.columns = ['UF', 'LING', 'HUM', 'NAT', 'MAT', 'RED']
notas_heatmap = notas.pivot_table(index='UF', aggfunc='mean')

fig, ax = plt.subplots(figsize=(6,10))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/376:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas.columns = ['UF', 'LING', 'HUM', 'NAT', 'MAT', 'RED']
notas_heatmap = notas.pivot_table(index='UF', aggfunc='mean')
128/377:
fig, ax = plt.subplots(figsize=(6,10))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/378:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas.columns = ['UF', 'LING', 'HUM', 'NAT', 'MAT', 'RED']
notas_heatmap = notas.pivot_table(columns='UF', aggfunc='mean')
128/379:
fig, ax = plt.subplots(figsize=(6,10))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/380:
fig, ax = plt.subplots(figsize=(10,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/381:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/382:
fig, ax = plt.subplots(figsize=(22,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/383:
fig, ax = plt.subplots(figsize=(26,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/384:
fig, ax = plt.subplots(figsize=(16,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/385:
fig, ax = plt.subplots(figsize=(18,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/386:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.show()
128/387:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.yticks(rotation=90)
plt.show()
128/388:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black',ax=ax)
plt.title("Nota média dos candidatos por UF\ne áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.yticks(rotation=0)
plt.show()
128/389:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black', ax=ax)
plt.title("Nota média dos candidatos por UF e áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.yticks(rotation=0)
plt.show()
129/1:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
129/2: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
129/3: train_base.shape
129/4: train_base.describe()
129/5: train_base.head()
129/6:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1) #

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
129/7: cleaned_train = data_cleaning(train_base)
129/8:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
129/9:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
129/10: cleaned_train.head(1)
129/11: cleaned_train.tail(1)
129/12:
idades = cleaned_train.IDADE
idades.plot.hist(idades, color='blue', edgecolor='black')
129/13:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
129/14: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
129/15: train_base.shape
129/16: train_base.describe()
129/17: train_base.head()
129/18:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1) #

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
129/19: cleaned_train = data_cleaning(train_base)
129/20:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
129/21:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
129/22: cleaned_train.head(1)
129/23: cleaned_train.tail(1)
129/24:
idades = cleaned_train.IDADE
idades.plot.hist(idades, color='blue', edgecolor='black')
129/25:
idades = cleaned_train.IDADE
#idades.plot.hist(idades, color='blue', edgecolor='black')
129/26:
idades = cleaned_train.IDADE
idades
#idades.plot.hist(idades, color='blue', edgecolor='black')
129/27:
idades = cleaned_train.IDADE.reset_intex(drop=True)
idades
#idades.plot.hist(idades, color='blue', edgecolor='black')
129/28:
idades = cleaned_train.IDADE.reset_index(drop=True)
idades
#idades.plot.hist(idades, color='blue', edgecolor='black')
129/29:
idades = cleaned_train.IDADE.reset_index(drop=True)
idades.plot.hist(idades, color='blue', edgecolor='black')
129/30:
idades = cleaned_train.IDADE.reset_index(drop=True)
idades.plot.hist(color='blue', edgecolor='black')
129/31:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
129/32: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
129/33: train_base.shape
129/34: train_base.describe()
129/35: train_base.head()
129/36:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1) #

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
129/37: cleaned_train = data_cleaning(train_base)
129/38:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
129/39:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
129/40: cleaned_train.head(1)
129/41: cleaned_train.tail(1)
129/42:
idades = cleaned_train.IDADE.reset_index(drop=True)
idades.plot.hist(color='blue', edgecolor='black')
129/43:
def calcula_percentual_categorias(feature, categoria):
    categoria_df = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = categoria_df[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
129/44:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)*100) + "% do total da base")
129/45:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
129/46:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("\nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
129/47:
percentual_mulheres = calcula_percentual_categorias('VAR2', 'F')
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 3)*100) + "% do total da base")
129/48:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
129/49:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
129/50:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned_train.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
129/51:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'HOMENS'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['HOMENS'] = data['HOMENS'].cumsum()
print("total de homens: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='HOMENS', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MULHERES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MULHERES'] = data['MULHERES'].cumsum()
print("total de mulheres: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='MULHERES', figsize=(10,6), ax=ax)
129/52:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

print("Descrição da feature idade")
cleaned_train.IDADE.describe()
129/53:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
129/54:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
129/55:
categorical_transform(cleaned_train)
cleaned_train.head()
129/56:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
129/57:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
129/58: X_train, y_train = prepare_data_vectors(cleaned_train)
129/59:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
129/60: X_train = scale_features(X_train)
129/61: y_train.value_counts()
129/62: X_train.head()
129/63:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

## saga solver was pointed out as a good fit for a large dataset
lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
129/64:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
129/65:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
129/66: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
129/67: test_base.head()
129/68:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
129/69:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
129/70: y_test.value_counts()
129/71: X_test.head()
129/72:
plot_confusion_matrix(lr, X_test, y_test)
plot_confusion_matrix(rfc, X_test, y_test)
plot_confusion_matrix(dtc, X_test, y_test)
129/73:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
129/74: new_cleaned_train.TARGET.value_counts()
129/75: new_cleaned_test.TARGET.value_counts()
129/76:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)

rfc_over = RandomForestClassifier(n_jobs=-1)
rfc_over.fit(X_train, y_train)
plot_confusion_matrix(rfc_over, X_test, y_test)

lr_over = LogisticRegression(max_iter=100, solver='saga')
lr_over.fit(X_train, y_train)
plot_confusion_matrix(lr_over, X_test, y_test)

dtc_over = DecisionTreeClassifier()
dtc_over.fit(X_train, y_train)
plot_confusion_matrix(dtc_over, X_test, y_test)
129/77:
## dropando features categóricas a fim de address overfitting
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## aplicando feature scaling em ambos datasets
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
129/78:
lr_mc = LogisticRegression(max_iter=100, solver='saga', class_weight='balanced', random_state=42)
lr_mc.fit(X_train, y_train)
129/79:
plot_confusion_matrix(lr_mc, X_train, y_train)
plot_confusion_matrix(lr_mc, X_test, y_test)
129/80:
score = precision_score(y_test, lr_mc.predict(X_test))
score
129/81:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

y_test_proba = pd.DataFrame(lr_mc.predict_proba(X_test))
thresholds = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85]

for threshold in thresholds:
    print ('\n******** For threshold = {} ******'.format(threshold))
    y_pred = y_test_proba.applymap(lambda x: 1 if x>threshold else 0)
    y_pred = y_pred.iloc[:,1].to_numpy().reshape(y_pred.iloc[:,1].to_numpy().size,1)
    y_pred = pd.DataFrame(y_pred)
    
    cm = confusion_matrix(y_test, y_pred, labels=lr_mc.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr_mc.classes_)
    display.plot()
    plt.title(type(lr_mc).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y_test, y_pred))
    print("Recall score: ", recall_score(y_test, y_pred))
    print("F1 score: ", f1_score(y_test, y_pred))
    print("Accuracy score: ", accuracy_score(y_test, y_pred))
129/82: score
129/83:
## tamanho credito = total emprestado aprovado pela política
## divida total = total aprovado referente aos mau pagadores
129/84: ## cross-validation set??
129/85:
as_is = cleaned_test[cleaned_test.IDADE > 28.0]
as_is.shape
129/86:
total_emprestado = as_is.shape[0] * 1000
total_emprestado
129/87:
mas_pagadoras = as_is[as_is.TARGET == 0]
print(mas_pagadoras.shape)
mas_pagadoras.TARGET.value_counts()
129/88:
divida_total = mas_pagadoras.shape[0] * 1000
divida_total
129/89:
percentual_negado = (cleaned_test.shape[0] - as_is.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
round(percentual_negado, 3)
129/90:
y_pred_proba = lr_mc.predict_proba(X_test)
y_pred_proba = pd.DataFrame(y_pred_proba)
y_pred_proba = y_pred_proba.iloc[:,1].to_numpy().reshape(y_pred_proba.iloc[:,1].to_numpy().size,1)
y_pred_proba = pd.DataFrame(y_pred_proba)

# recreating target column
cleaned_test['SCORE'] = y_pred_proba
cleaned_test.head()
129/91:
to_be_negado = 0.19321

to_be_shape = cleaned_test.shape[0] - (cleaned_test.shape[0] * to_be_negado)

# quantidade de empréstimos aprovado no cleaned test set
to_be_shape
129/92:
to_be = cleaned_test[cleaned_test.SCORE > 0.3266]
to_be.shape
129/93:
total_emprestado_tobe = to_be.shape[0] * 1000
total_emprestado_tobe
129/94:
mas_pagadoras_tobe = to_be[to_be.TARGET == 0]
print(mas_pagadoras_tobe.shape)
mas_pagadoras_tobe.TARGET.value_counts()
129/95:
divida_total = mas_pagadoras_tobe.shape[0] * 1000
divida_total
129/96:
percentual_negado_tobe = (cleaned_test.shape[0] - to_be.shape[0]) / cleaned_test.shape[0]

## este será o ponto de corte da política TO_BE
print(round(percentual_negado, 3) == round(percentual_negado_tobe, 3))
print("Percentual de solicitações negadas: ", + round(percentual_negado_tobe, 3))
129/97:
from keras.models import Sequential
from keras.layers import Dense
129/98:
from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=29))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['precision']) 
model.summary()
129/99: X_train.shape
129/100:
from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=19))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['precision']) 
model.summary()
129/101: y_train.shape
129/102:
from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=19))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['precision']) 
model.summary()
129/103: hist = model.fit(X_train, y_train, epochs=10, batch_size=100)
129/104:
from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['precision']) 
model.summary()
129/105: hist = model.fit(X_train, y_train, epochs=10, batch_size=100)
129/106:
from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 
model.summary()
129/107: hist = model.fit(X_train, y_train, epochs=10, batch_size=100)
129/108:
from sklearn.metrics import confusion_matrix
 
y_predicted = model.predict(X_train) > 0.5
mat = confusion_matrix(y_train, y_predicted)
labels = ['Good', 'Bad']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/109:
from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_train) > 0.5
mat = confusion_matrix(y_train, y_predicted)
labels = ['Good', 'Bad']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/110: hist = model.fit(X_train, y_train, epochs=20, batch_size=100)
129/111:
from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_train) > 0.5
mat = confusion_matrix(y_train, y_predicted)
labels = ['Good', 'Bad']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/112: hist = model.fit(X_train, y_train, epochs=20, batch_size=10)
129/113: hist = model.fit(X_train, y_train, epochs=20, batch_size=200)
129/114:
from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_train) > 0.5
mat = confusion_matrix(y_train, y_predicted)
labels = ['Good', 'Bad']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/115:
from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Good', 'Bad']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/116:
from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/117:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/118:
from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['f1']) 
model.summary()
129/119:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/120:
from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['recall']) 
model.summary()
129/121: hist = model.fit(X_train, y_train, epochs=20, batch_size=200)
129/122:
from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 
model.summary()
129/123: hist = model.fit(X_train, y_train, epochs=20, batch_size=200)
129/124:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/125: X_test.head()
129/126:
neg, pos = np.bincount(cleaned_train.TARGET)
total = neg + pos
print('Examples:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, pos, 100 * pos / total))
129/127:
neg, pos = np.bincount(cleaned_train.TARGET)
total = neg + pos
print('Examples:\n    Total: {}\n    Bad payer: {} ({:.2f}% of total)\n'.format(
    total, neg, 100 * neg / total))
129/128:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
129/129:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/130:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=200, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/131:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=100, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/132:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/133:
from keras.models import Sequential
from keras.layers import Dense

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/134:
from keras.models import Sequential
from keras.layers import Dense
from tensorflow import keras

model = Sequential() 
model.add(Dense(128, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/135:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/136:
from keras.models import Sequential
from keras.layers import Dense
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/137:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/138:
from keras.models import Sequential
from keras.layers import Dense
from tensorflow import keras

model = Sequential() 
model.add(Dense(512, activation='relu', input_dim=18))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/139:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/140:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/141:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/142:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=200, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/143:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=100, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/144:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=100, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/145:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=300, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/146:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/147: new_cleaned_train.TARGET.value_counts()
129/148: new_cleaned_test.TARGET.value_counts()
129/149:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
129/150: new_cleaned_train.TARGET.value_counts()
129/151: new_cleaned_test.TARGET.value_counts()
129/152:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
129/153: new_cleaned_train.TARGET.value_counts()
129/154: new_cleaned_test.TARGET.value_counts()
129/155:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
129/156:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/157:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=100, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/158:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=400, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/159:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=40, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/160:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=40)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/161:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/162:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
129/163: new_cleaned_train.TARGET.value_counts()
129/164: new_cleaned_test.TARGET.value_counts()
129/165:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
129/166:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=40, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/167:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=40, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/168:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/169:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=40, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/170:
new_cleaned_train.TARGET.value_counts()
new_cleaned_train.shape
129/171: new_cleaned_train.TARGET.value_counts()
129/172:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=19))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/173:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
129/174:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
129/175: new_cleaned_train.TARGET.value_counts()
129/176: new_cleaned_test.TARGET.value_counts()
129/177:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
129/178:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=40, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/179:
new_cleaned_test.TARGET.value_counts()
new_cleaned_test.shape
129/180: new_cleaned_test.TARGET.value_counts()
129/181:
new_cleaned_train.TARGET.value_counts()
cleaned_train.shape
129/182:
X_train.head()
X_train.shape
129/183:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
129/184: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
129/185: train_base.shape
129/186: train_base.describe()
129/187: train_base.head()
129/188:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1) #

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
129/189: cleaned_train = data_cleaning(train_base)
129/190:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
129/191:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
129/192: cleaned_train.head(1)
129/193: cleaned_train.tail(1)
129/194:
idades = cleaned_train.IDADE.reset_index(drop=True)
idades.plot.hist(color='blue', edgecolor='black')
129/195:
def calcula_percentual_categorias(feature, categoria):
    categoria_df = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = categoria_df[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
129/196:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)*100) + "% do total da base")
129/197:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
129/198:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("\nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
129/199:
percentual_mulheres = calcula_percentual_categorias('VAR2', 'F')
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 3)*100) + "% do total da base")
129/200:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
129/201:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
129/202:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned_train.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
129/203:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'HOMENS'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['HOMENS'] = data['HOMENS'].cumsum()
print("total de homens: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='HOMENS', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MULHERES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MULHERES'] = data['MULHERES'].cumsum()
print("total de mulheres: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='MULHERES', figsize=(10,6), ax=ax)
129/204:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

print("Descrição da feature idade")
cleaned_train.IDADE.describe()
129/205:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
129/206:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
129/207:
categorical_transform(cleaned_train)
cleaned_train.head()
129/208:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
129/209:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
129/210: X_train, y_train = prepare_data_vectors(cleaned_train)
129/211:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
129/212: X_train = scale_features(X_train)
129/213: y_train.value_counts()
129/214:
X_train.head()
X_train.shape
129/215: X_train.shape
129/216: y_train.shape
129/217:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=19))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/218:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

## saga solver was pointed out as a good fit for a large dataset
lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
129/219:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
129/220:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
129/221: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
129/222: test_base.head()
129/223:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
129/224:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
129/225: y_test.value_counts()
129/226: X_test.head()
129/227:
neg, pos = np.bincount(cleaned_train.TARGET)
total = neg + pos
print('Examples:\n    Total: {}\n    Bad payer: {} ({:.2f}% of total)\n'.format(
    total, neg, 100 * neg / total))
129/228:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
129/229:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/230:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=110))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/231:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/232:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=30))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/233:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/234:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=18))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/235:
overfitting_features = ['VAR' + str(x) for x in range(49, 141, 1)]

## removendo categorical features
new_cleaned_train = cleaned_train.drop(columns=overfitting_features, axis=1)
new_cleaned_test = cleaned_test.drop(columns=overfitting_features, axis=1)

## resampling os bons pagadores e recriando o training set
even_good_train = new_cleaned_train[new_cleaned_train.TARGET == 1.0].sample(frac = 0.25)
bad_train = new_cleaned_train[new_cleaned_train.TARGET == 0]
new_cleaned_train = pd.concat([even_good_train, bad_train], axis=0)

## resampling os maus pagadores e recriando test set
even_good_test = new_cleaned_test[new_cleaned_test.TARGET == 1.0].sample(frac = 0.25)
bad_test = new_cleaned_test[new_cleaned_test.TARGET == 0]
new_cleaned_test = pd.concat([even_good_test, bad_test], axis=0)
129/236:
new_cleaned_train.TARGET.value_counts()
cleaned_train.shape
129/237: new_cleaned_test.TARGET.value_counts()
129/238: new_cleaned_train.TARGET.value_counts()
129/239: new_cleaned_test.TARGET.value_counts()
129/240:
X_train, y_train = prepare_data_vectors(new_cleaned_train)
X_train = scale_features(X_train)

X_test, y_test = prepare_data_vectors(new_cleaned_test)
X_test = scale_features(X_test)
129/241:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=40, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/242:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=40)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/243:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/244: X_train, y_train = prepare_data_vectors(cleaned_train)
129/245:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
129/246: X_train = scale_features(X_train)
129/247: y_train.value_counts()
129/248:
X_train.head()
X_train.shape
129/249: X_train.shape
129/250: X_train.head()
129/251: X_train.shape
129/252: y_train.shape
129/253:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=110))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/254:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
129/255:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
129/256: y_test.value_counts()
129/257: X_test.head()
129/258:
neg, pos = np.bincount(cleaned_train.TARGET)
total = neg + pos
print('Examples:\n    Total: {}\n    Bad payer: {} ({:.2f}% of total)\n'.format(
    total, neg, 100 * neg / total))
129/259:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
129/260:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/261:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=10))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/262:
X_train_under = X_train[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']]
X_test_under  = X_test[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']]
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/263:
X_train_under = X_train[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']]
X_test_under  = X_test[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']]
129/264: X_train_under.columns
129/265: X_train.columns
129/266: X_train_under = X_train[RangeIndex(0, 10)]
129/267: X_train_under = X_train.iloc[:, [0,1,3]]
129/268:
X_train_under = X_train.iloc[:, [0,1,3]]
X_train_under
129/269:
X_train_under = X_train.iloc[:, range(11)]
X_train_under
129/270:
X_train_under = X_train.iloc[:, range(11)]
X_test_under = X_test.iloc[:, range(11)]
X_test_under
129/271:
X_train_under = X_train.iloc[:, range(11)]
X_test_under = X_test.iloc[:, range(11)]
129/272:
hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/273:
X_train_under = X_train.iloc[:, range(11)]
X_test_under = X_test.iloc[:, range(11)]
129/274:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/275:
X_train_under = X_train.iloc[:, range(10)]
X_test_under = X_test.iloc[:, range(10)]
129/276:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
129/277:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf
tf.config.run_functions_eagerly(True)

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=10))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
129/278:
X_train_under = X_train.iloc[:, range(10)]
X_test_under = X_test.iloc[:, range(10)]
129/279:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/1:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=10))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
130/2:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/3:
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
pd.set_option('display.max_columns', None)
130/4: train_base = pd.read_csv('../datasets/credit_01/train.gz', compression='gzip', header=0)
130/5: train_base.shape
130/6: train_base.describe()
130/7: train_base.head()
130/8:
def data_cleaning(base):
    ## dataframe with total of null values and their percentage value per column from the base
    miss = base.isnull().sum().sort_values(ascending=False)
    percent = base.isnull().mean().sort_values(ascending=False)
    missing = pd.concat([miss, percent], axis=1, keys=['Total', 'Percent'])
    
    ### removing columns with null percentual > 15%
    ### here, sex and age represent have approximately 11% of null values but I assume they are essencial to this dataset
    columns_to_drop = missing[missing['Percent'] > 0.15].index
    cleaned = base.drop(columns=columns_to_drop, axis=1)
    
    # in this project, it was considered data with null values irrelevant. 
    
    # select rows with null sex, age or national state to drop, 
    # assuming this is important for identifying payers and targeting them
    rows_to_drop = cleaned[(cleaned.VAR2.isnull() == True) | 
                       (cleaned.IDADE.isnull() == True) |
                       (cleaned.VAR5.isnull() == True)].index
    cleaned = cleaned.drop(index=rows_to_drop, axis=1) #

    # dropping column associated to VAR149 once its value for all rows are equal
    cleaned = cleaned.drop(columns='VAR149', axis=1)

    # removing the very few rows remaining with null values
    cleaned = cleaned.dropna()
    
    print("cleaned_base shape =", cleaned.shape)
    print("base shape =", base.shape)
    
    return cleaned
130/9: cleaned_train = data_cleaning(train_base)
130/10:
def sort_ref_date(cleaned_base):
    date_sorted = cleaned_base.sort_values(by=['REF_DATE'], ascending=True)
    date_sorted.REF_DATE = date_sorted.REF_DATE.str[0:10]
    
    return date_sorted
130/11:
cleaned_train = sort_ref_date(cleaned_train)
cleaned_train.isnull().sum()
130/12: cleaned_train.head(1)
130/13: cleaned_train.tail(1)
130/14:
idades = cleaned_train.IDADE.reset_index(drop=True)
idades.plot.hist(color='blue', edgecolor='black')
130/15:
def calcula_percentual_categorias(feature, categoria):
    categoria_df = cleaned_train[cleaned_train[feature] == categoria]
    total_categoria = categoria_df[feature].value_counts().sum()
    percentual_categoria = total_categoria / cleaned_train[feature].value_counts().sum()
    return percentual_categoria
130/16:
percentual_paulistas = calcula_percentual_categorias('VAR5', 'SP')
print("Os paulistas representam cerca de " + str(round(percentual_paulistas, 3)*100) + "% do total da base")
130/17:
estados = cleaned_train.VAR5.value_counts(normalize=True)
estados.plot(kind='bar')
130/18:
print("Base de treinamento")
print(train_base.VAR2.value_counts())
print("\nBase de treinamento limpa")
print(cleaned_train.VAR2.value_counts())
130/19:
percentual_mulheres = calcula_percentual_categorias('VAR2', 'F')
print("As mulheres representam cerca de " + str(round(percentual_mulheres, 3)*100) + "% do total da base")
130/20:
sexos = cleaned_train.VAR2.value_counts(normalize=True)
yticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
sexos.plot(kind='bar', yticks=yticks)
130/21:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR5], axis=1)
data.boxplot(by='VAR5', figsize=(10,6))
130/22:
data = pd.DataFrame(cleaned_train.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'count'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['count'] = data['count'].cumsum()
print("cleaned dataset shape: ", cleaned_train.shape)
data.plot(kind='line', x='REF_DATE', y='count', figsize=(10,6))
130/23:
males = cleaned_train[cleaned_train.VAR2 == 'M']
data = pd.DataFrame(males.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'HOMENS'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['HOMENS'] = data['HOMENS'].cumsum()
print("total de homens: ", males.shape[0])
ax= data.plot(kind='line', x='REF_DATE', y='HOMENS', figsize=(10,6))

females = cleaned_train[cleaned_train.VAR2 == 'F']
data = pd.DataFrame(females.REF_DATE.value_counts().reset_index().values, columns=['REF_DATE', 'MULHERES'])
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['MULHERES'] = data['MULHERES'].cumsum()
print("total de mulheres: ", females.shape[0])
data.plot(kind='line', x='REF_DATE', y='MULHERES', figsize=(10,6), ax=ax)
130/24:
data = pd.concat([cleaned_train.REF_DATE, cleaned_train.IDADE], axis=1)
data = data.sort_values(by=['REF_DATE'], ascending=True)
data['counter'] = range(1, len(data)+1, 1)
data = data.rename(columns={'IDADE': 'AVERAGE AGE'})
data['AVERAGE AGE'] = data['AVERAGE AGE'].cumsum() / data.counter
data.plot(kind='line', x='REF_DATE', y='AVERAGE AGE', figsize=(10,6))

print("Descrição da feature idade")
cleaned_train.IDADE.describe()
130/25:
data = pd.concat([cleaned_train.IDADE, cleaned_train.VAR2], axis=1)
data.boxplot(by='VAR2', figsize=(10,6))
130/26:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
130/27:
categorical_transform(cleaned_train)
cleaned_train.head()
130/28:
## checando os tipos de dados no novo dataframe
cleaned_train.dtypes
## apenas int e floats
130/29:
def prepare_data_vectors(cleaned_base):
    ## separating X feature vector from y targets
    y = cleaned_base['TARGET']
    X = cleaned_base.loc[:, cleaned_base.columns != 'TARGET']

    return X, y
130/30: X_train, y_train = prepare_data_vectors(cleaned_train)
130/31:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
130/32: X_train = scale_features(X_train)
130/33: y_train.value_counts()
130/34: X_train.head()
130/35: X_train.shape
130/36: y_train.shape
130/37:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=10))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
130/38:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

## saga solver was pointed out as a good fit for a large dataset
lr = LogisticRegression(max_iter=100, solver='saga')
lr.fit(X_train, y_train)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
130/39:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
130/40:
plot_confusion_matrix(lr, X_train, y_train)
plot_confusion_matrix(rfc, X_train, y_train)
plot_confusion_matrix(dtc, X_train, y_train)
130/41: test_base = pd.read_csv('../datasets/credit_01/test.gz', compression='gzip', header=0, low_memory=False)
130/42: test_base.head()
130/43:
cleaned_test = data_cleaning(test_base)
cleaned_test = sort_ref_date(cleaned_test)
categorical_transform(cleaned_test)
cleaned_test
130/44:
X_test, y_test = prepare_data_vectors(cleaned_test)
X_test = scale_features(X_test)
130/45: y_test.value_counts()
130/46: X_test.head()
130/47:
neg, pos = np.bincount(cleaned_train.TARGET)
total = neg + pos
print('Examples:\n    Total: {}\n    Bad payer: {} ({:.2f}% of total)\n'.format(
    total, neg, 100 * neg / total))
130/48:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
130/49:
X_train_under = X_train.iloc[:, range(10)]
X_test_under = X_test.iloc[:, range(10)]
130/50:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=1000, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/51:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=30))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
130/52:
X_train_under = X_train.iloc[:, range(30)]
X_test_under = X_test.iloc[:, range(30)]
130/53:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=100, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/54:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=4, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/55:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=4, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/56:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=100, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/57:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=4, batch_size=50, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/58:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=4, batch_size=200, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/59:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

model = Sequential() 
model.add(Dense(256, activation='relu', input_dim=20))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
130/60:
X_train_under = X_train.iloc[:, range(20)]
X_test_under = X_test.iloc[:, range(20)]
130/61:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=4, batch_size=200, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/62:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=4, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/63:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=8, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/64:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

model = Sequential() 
model.add(Dense(16, activation='relu', input_dim=20))
#model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid')) 
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]) 
model.summary()
130/65:
X_train_under = X_train.iloc[:, range(20)]
X_test_under = X_test.iloc[:, range(20)]
130/66:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=8, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/67:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=20, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/68:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
130/69:
X_train_under = X_train.iloc[:, range(20)]
X_test_under = X_test.iloc[:, range(20)]
130/70:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=20, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/71:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
130/72:
X_train_under = X_train.iloc[:, range(20)]
X_test_under = X_test.iloc[:, range(20)]
130/73:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=20, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/74:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=10, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/75:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total * 3.0 / 4.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
130/76:
X_train_under = X_train.iloc[:, range(20)]
X_test_under = X_test.iloc[:, range(20)]
130/77:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=10, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/78:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 4.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
130/79:
X_train_under = X_train.iloc[:, range(20)]
X_test_under = X_test.iloc[:, range(20)]
130/80:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=10, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/81:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
130/82:
X_train_under = X_train.iloc[:, range(20)]
X_test_under = X_test.iloc[:, range(20)]
130/83:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=10, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/84:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]

def make_model(metrics=METRICS, output_bias=None):
    if output_bias is not None:
        output_bias = tf.keras.initializers.Constant(output_bias)
        
    model = Sequential()
    model.add(Dense(16, activation='relu', input_dim=20))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid', bias_initializer=output_bias)) 

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-3),
        loss=keras.losses.BinaryCrossentropy(),
        metrics=METRICS)

    return model
130/85:
X_test.head()
X_test.shape
130/86: X_test.head()
130/87:
neg, pos = np.bincount(cleaned_train.TARGET)
total = neg + pos
print('Examples:\n    Total: {}\n    Bad payer: {} ({:.2f}% of total)\n'.format(
    total, neg, 100 * neg / total))
130/88:
# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))
130/89:
initial_bias = np.log([pos/neg])
initial_bias
130/90:
initial_bias = np.log([neg/pos])
initial_bias
130/91:
initial_bias = np.log([pos/neg])
initial_bias
130/92:
X_train_under = X_train.iloc[:, range(20)]
X_test_under = X_test.iloc[:, range(20)]
130/93:
model = make_model(output_bias=initial_bias)
model.summary()
130/94:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=10, batch_size=150, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/95:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=10, batch_size=50, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/96:
model = make_model()
model.summary()
130/97:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=10, batch_size=50, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
130/98:
hist = model.fit(X_train_under, y_train, validation_data=(X_test_under, y_test), epochs=10, batch_size=100, class_weight=class_weight)

from sklearn.metrics import confusion_matrix
import seaborn as sns
 
y_predicted = model.predict(X_test_under) > 0.5
mat = confusion_matrix(y_test, y_predicted)
labels = ['Bad', 'Good']
 
sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=labels, yticklabels=labels)
 
plt.xlabel('Predicted label')
plt.ylabel('Actual label')
132/1: import pandas as pd
132/2:
test_df = pd.read_csv('../datasets/credit_01/test.gz',
                      compression='gzip', header=0, low_memory=False)
132/3: test_df
132/4:
import pandas as pd
from app.utils.preprocessing import data_preprocessing
132/5:
test_df = pd.read_csv('../datasets/credit_01/test.gz',
                      compression='gzip', header=0, low_memory=False)
132/6: test_df, X_test = data_preprocessing(test_df)
132/7: X_test.head()
132/8:
test_df, X_test = data_preprocessing(test_df)
y_test = test_df.TARGET
132/9:
test_df = pd.read_csv('../datasets/credit_01/test.gz',
                      compression='gzip', header=0, low_memory=False)
132/10:
test_df, X_test = data_preprocessing(test_df)
y_test = test_df.TARGET
132/11:
model_file = open('model.pkl', 'rb')
model = pickle.load(model_file)
132/12:
import pandas as pd
import pickle
from app.utils.preprocessing import data_preprocessing
132/13:
import pandas as pd
import pickle
from app.utils.preprocessing import data_preprocessing
132/14:
test_df = pd.read_csv('../datasets/credit_01/test.gz',
                      compression='gzip', header=0, low_memory=False)
132/15:
test_df, X_test = data_preprocessing(test_df)
y_test = test_df.TARGET
132/16:
model_file = open('model.pkl', 'rb')
model = pickle.load(model_file)
132/17:
import pandas as pd
import pickle
from app.utils.preprocessing import data_preprocessing
132/18:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
132/19:
records_file = open('batch_records.json')
records_file
132/20:
import pandas as pd
import pickle
import json
from app.utils.preprocessing import data_preprocessing
132/21:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
132/22:
records_file = open('batch_records.json')
records_json = json.load(records_file)
132/23:
records_file = open('batch_records.json')
records_json = json.load(records_file)
records_json
132/24:
records_file = open('batch_records.json')
records_json = json.load(records_file)
132/25:
records_file = open('batch_records.json')
records_json = json.load(records_file)
records_json
132/26:
records_file = open('batch_records.json')
records_json = json.load(records_file)
type(records_json)
132/27:
records_file = open('batch_records.json')
records_json = json.load(records_file)
132/28: type(records_json)
132/29: type(records_json[0])
132/30: type(type(records_json[0]))
132/31: type(type(records_json))
132/32: type(records_json[:])
132/33: type(records_json[0])
132/34:
import pandas as pd
import pickle
import json
import requests
from app.utils.preprocessing import data_preprocessing
132/35:
import pandas as pd
import pickle
import json
import requests
from app.utils.preprocessing import data_preprocessing
132/36:
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}
payload = records_json

res = requests.post('http://127.0.0.1:8001/performance/', data=payload, headers=headers)
print(res)
132/37:
headers = {'Content-Type': 'application/json'}
payload = records_json

res = requests.post('http://127.0.0.1:8001/performance/', data=payload, headers=headers)
print(res)
132/38:
headers = {'Content-Type': 'application/json'}
payload = records_json

requests.post('http://127.0.0.1:8001/performance/', data=payload, headers=headers)
132/39:
headers = {'Content-Type': 'application/json'}
payload = records_json

post('http://127.0.0.1:8001/performance/', data=payload, headers=headers)
132/40:
headers = {'Content-Type': 'application/json'}
payload = records_json

reqs.post('http://127.0.0.1:8001/performance/', data=payload, headers=headers)
132/41:
import pandas as pd
import pickle
import json
import requests as reqs
from app.utils.preprocessing import data_preprocessing
132/42:
headers = {'Content-Type': 'application/json'}
payload = records_json

reqs.post('http://127.0.0.1:8001/performance/', data=payload, headers=headers)
132/43:
headers = {'Content-Type': 'application/json'}
payload = records_json

reqs.post('http://127.0.0.1:8001/performance/', data=payload)
132/44:
headers = {'Content-Type': 'application/json'}
payload = records_json

reqs.post('https://127.0.0.1:8001/performance/', data=payload)
132/45:
headers = {'Content-Type': 'application/json'}
payload = records_json

reqs.post('http://127.0.0.1:8001/performance/', data=payload)
132/46:
headers = {'Content-Type': 'application/json'}

reqs.post('http://127.0.0.1:8001/performance/', json=records_json)
132/47:
headers = {'Content-Type': 'application/json'}

reqs.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)
132/48:
import pandas as pd
import pickle
import json
import requests
from app.utils.preprocessing import data_preprocessing
132/49:
headers = {'Content-Type': 'application/json'}

requests.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)
132/50:
headers = {'Content-Type': 'application/json'}

res = requests.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)
print(res)
132/51:
headers = {'Content-Type': 'application/json'}

res = requests.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)
print(res.text)
132/52:
headers = {'Content-Type': 'application/json'}

res = requests.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)
print(type(res.text))
132/53:
headers = {'Content-Type': 'application/json'}

res = requests.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)

res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/54:
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)
132/55:
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)
132/56:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/57: req_json = {'req-dataset': path_train}
132/58:
home = 'http://127.0.0.1:8001'
endpoint = 'aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/59:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/60:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/61:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/62:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/63:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)
132/64:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/65:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post('http://127.0.0.1:8001/performance/', json=records_json, headers=headers)
132/66:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/67:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post('http://127.0.0.1:8001/aderencia/', json=records_json, headers=headers)
132/68:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/69:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post('http://127.0.0.1:8001/aderencia/', json=req_json, headers=headers)
132/70:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/71:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/72:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/73:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/74:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/75:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/76:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/77:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/78:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/79:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/80:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/81:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/82:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/83:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/84:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/85:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/86:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/87:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/88:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/89:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/90:
#res_parsed = json.loads(res.text)
#print(json.dumps(res_parsed, indent=4))
132/91:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

keras_model = Sequential()
keras_model.add(Dense(16, activation='relu', input_dim=119))
keras_model.add(Dropout(0.5))
keras_model.add(Dense(1, activation='sigmoid')) 

keras_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=METRICS)    
print(keras_model.summary())
132/92:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]

keras_model = Sequential()
keras_model.add(Dense(16, activation='relu', input_dim=119))
keras_model.add(Dropout(0.5))
keras_model.add(Dense(1, activation='sigmoid')) 

keras_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=METRICS)    
print(keras_model.summary())
132/93:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]

keras_model = Sequential()
keras_model.add(Dense(16, activation='relu', input_dim=119))
keras_model.add(Dropout(0.5))
keras_model.add(Dense(1, activation='sigmoid')) 

keras_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=METRICS)
132/94: print(keras_model.summary())
132/95: from app.utils.preprocessing import data_preprocessing
132/96:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, X_train = data_preprocessing(df)
train_df
132/97:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, X_train = data_preprocessing(df)
X_train
132/98:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, X_train = data_preprocessing(df)
132/99: y_train = train_df.TARGET
132/100: y_train = train_df.TARGET
132/101: neg, pos = np.bincount(cleaned_train.TARGET)
132/102:
import pandas as pd
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
132/103: neg, pos = np.bincount(cleaned_train.TARGET)
132/104: neg, pos = np.bincount(y_train)
132/105:
neg, pos = np.bincount(y_train)
print(neg, pos)
132/106:
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
132/107:
neg, pos = np.bincount(y_train)
total = neg + pos
132/108:
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
132/109:
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
class_weight
132/110:
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
132/111: keras_model.fit(X_train, y_train, epochs=10, batch_size=100, class_weight=class_weight)
132/112: #keras_model.fit(X_train, y_train, epochs=10, batch_size=100, class_weight=class_weight)
132/113: X_train.select_dtypes(include='object')
132/114: X_train.select_dtypes(include='object').index
132/115: X_train.select_dtypes(include='object').columns
132/116: type(X_train.select_dtypes(include='object').columns)
132/117: X_train.select_dtypes(include='object').columns.to_numpy()
132/118: len(X_train.select_dtypes(include='object').columns.to_numpy())
132/119:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
len(CATEGORICAL_FEATURES)
132/120:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
CATEGORICAL_FEATURES
132/121: CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
132/122:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/123:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/124:
column_transform = ColumnTransformer(transformers=[('pipeline-1',
                                                    Pipeline(steps=[('num_imputer',
                                                                    SimpleImputer(strategy='median'))]),
                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000017E22D00370>),
                                                    ('pipeline-2',
                                                  Pipeline(steps=[('cat_imputer',
                                                                   SimpleImputer(strategy='most_frequent')),
                                                                  ('encoder',
                                                                   OneHotEncoder())]),
                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000017E22D01630>)]))
132/125:
column_transform = ColumnTransformer(transformers=[('pipeline-1',
                                                    Pipeline(steps=[('num_imputer',
                                                                    SimpleImputer(strategy='median'))]),
                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000017E22D00370>),
                                                    ('pipeline-2',
                                                  Pipeline(steps=[('cat_imputer',
                                                                   SimpleImputer(strategy='most_frequent')),
                                                                  ('encoder',
                                                                   OneHotEncoder())]),
                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000017E22D01630>)])
132/126:
column_transform = ColumnTransformer(transformers=[('pipeline-1',
                                                    Pipeline(steps=[('num_imputer',
                                                                    SimpleImputer(strategy='median'))])),
                                                    ('pipeline-2',
                                                  Pipeline(steps=[('cat_imputer',
                                                                   SimpleImputer(strategy='most_frequent')),
                                                                  ('encoder',
                                                                   OneHotEncoder())]),
                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000017E22D01630>)])
132/127:
column_transform = ColumnTransformer(transformers=[('pipeline-1',
                                                    Pipeline(steps=[('num_imputer',
                                                                    SimpleImputer(strategy='median'))])),
                                                    ('pipeline-2',
                                                  Pipeline(steps=[('cat_imputer',
                                                                   SimpleImputer(strategy='most_frequent')),
                                                                  ('encoder',
                                                                   OneHotEncoder())]))])
132/128:
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
132/129:
column_transform = ColumnTransformer(transformers=[('pipeline-1',
                                                    Pipeline(steps=[('num_imputer',
                                                                    SimpleImputer(strategy='median'))])),
                                                    ('pipeline-2',
                                                  Pipeline(steps=[('cat_imputer',
                                                                   SimpleImputer(strategy='most_frequent')),
                                                                  ('encoder',
                                                                   OneHotEncoder())]))])
132/130:
column_transform = ColumnTransformer(transformers=[('pipeline-1',
                                                    Pipeline(steps=[('num_imputer',
                                                                    SimpleImputer(strategy='median'))])),
                                                    ('pipeline-2',
                                                  Pipeline(steps=[('cat_imputer',
                                                                   SimpleImputer(strategy='most_frequent')),
                                                                  ('encoder',
                                                                   OneHotEncoder())]))])
132/131: column_transform.fit_transform(X_train)
132/132: a = column_transform.fit_transform(X_train)
132/133:
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
132/134: X_train[CATEGORICAL_FEATURES]
132/135:
ohe = OneHotEncoder()
transformed = ohe.fit_transform(X_train[CATEGORICAL_FEATURES])
132/136: transformed
132/137:
ohe = OneHotEncoder(sparse=True, handle_unkown='ignore')
transformed = ohe.fit_transform(X_train[CATEGORICAL_FEATURES])
132/138:
ohe = OneHotEncoder(sparse=True, handle_unknown='ignore')
transformed = ohe.fit_transform(X_train[CATEGORICAL_FEATURES])
132/139: transformed
132/140:
ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')
transformed = ohe.fit_transform(X_train[CATEGORICAL_FEATURES])
132/141: transformed
132/142: pd.DataFrame(transformed)
132/143:
ohe = OneHotEncoder(sparse=True, handle_unknown='ignore')
transformed = ohe.fit_transform(X_train[CATEGORICAL_FEATURES])
132/144: pd.DataFrame(transformed)
132/145:
df = pd.DataFrame()
df[ohe.categories_[0]] = transformed.toarray()
132/146:
ohe = OneHotEncoder(handle_unknown='ignore')
transformed = ohe.fit_transform(X_train[CATEGORICAL_FEATURES])
132/147:
df = pd.DataFrame()
df[ohe.categories_[0]] = transformed.toarray()
132/148:
df = pd.DataFrame()
X_train[ohe.categories_[0]] = transformed.toarray()
132/149:
categorical_transform(X_train)
X_train
132/150:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
132/151:
categorical_transform(X_train)
X_train
132/152:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
132/153:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
132/154:
categorical_transform(X_train)
X_train
132/155:
categorical_transform(train_df)
train_df = train_df.dropna()
train_df
132/156:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
132/157:
categorical_transform(train_df)
X_train = X_train.dropna()
y_train = y_train.dropna()
132/158:
categorical_transform(train_df)
X_train = X_train.dropna()
y_train = y_train.dropna()
y_train
132/159:
categorical_transform(train_df)
X_train = X_train.dropna()
y_train = y_train.dropna()
y_train.value_counts()
132/160:
categorical_transform(train_df)
X_train = X_train.dropna()
y_train = y_train.dropna()
X_train = scale_features(X_train)
X_train
132/161:
categorical_transform(train_df)
X_train = X_train.dropna()
y_train = y_train.dropna()
X_train = scale_features(X_train)
132/162: keras_model.fit(X_train, y_train, epochs=10, batch_size=100, class_weight=class_weight)
132/163:
categorical_transform(train_df)
train_df = train_df.dropna()
X_train = train_df.loc[:, train_df.columns != 'TARGET']
y_train = train_df.TARGET
X_train = scale_features(X_train)
132/164: keras_model.fit(X_train, y_train, epochs=10, batch_size=100, class_weight=class_weight)
132/165: keras_model.fit(X_train, y_train, epochs=10, batch_size=150, class_weight=class_weight)
132/166: keras_model.fit(X_train, y_train, epochs=10, batch_size=200, class_weight=class_weight)
132/167:
confusion_matrix = metrics.confusion_matrix(y_test, score)
print(confusion_matrix)
print(roc_auc_score(y_test, score))
132/168:
from sklearn.metrics import roc_auc_score

confusion_matrix = metrics.confusion_matrix(y_test, score)
print(confusion_matrix)
print(roc_auc_score(y_test, score))
132/169:
from sklearn import metrics

confusion_matrix = metrics.confusion_matrix(y_test, score)
print(confusion_matrix)
print(roc_auc_score(y_test, score))
132/170: model.predict(X_train)
132/171: keras_model.predict(X_train)
132/172: keras_model.predict(X_train) > 0.5
132/173:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.5] = 1
yhat[yath < 0.5] = 0
yhat
132/174:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.5] = 1
yhat[yhat < 0.5] = 0
yhat
132/175:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.5] = 1
yhat[yhat < 0.5] = 0
132/176:
from sklearn import metrics

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_test, yhat))
132/177:
from sklearn import metrics
from sklearn.metrics import roc_auc_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_test, yhat))
132/178:
from sklearn import metrics
from sklearn.metrics import roc_auc_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_train, yhat))
132/179: X_train.shape
132/180: train_df.shape
132/181:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, X_train = data_preprocessing(df)
132/182: y_train = train_df.TARGET
132/183:
neg, pos = np.bincount(y_train)
total = neg + pos
132/184:
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
132/185: CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
132/186:
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
132/187:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
132/188:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
132/189: train_df.shape
132/190:
categorical_transform(train_df)
train_df = train_df.dropna()
X_train = train_df.loc[:, train_df.columns != 'TARGET']
y_train = train_df.TARGET
X_train = scale_features(X_train)
132/191: X_train.shape
132/192: X_train.shape
132/193: keras_model.fit(X_train, y_train, epochs=10, batch_size=200, class_weight=class_weight)
132/194:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.5] = 1
yhat[yhat < 0.5] = 0
132/195:
from sklearn import metrics
from sklearn.metrics import roc_auc_score, precision_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_train, yhat))
print(precision_score(y_train, yhat))
132/196:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = X_train[[col for col in X_train.columns and col not in CATEGORICAL_FEATURES]]
132/197:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in X_train.columns and col not in CATEGORICAL_FEATURES]]
col for col in X_train.column
132/198:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in X_train.columns and col not in CATEGORICAL_FEATURES]]
[col for col in X_train.column]
132/199:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in X_train.columns and col not in CATEGORICAL_FEATURES]]
[col for col in X_train.columns]
132/200:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in X_train.columns and col not in CATEGORICAL_FEATURES]]
print(X_train.columns)
[col for col in X_train.columns]
132/201:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in X_train.columns and col not in CATEGORICAL_FEATURES]]
print(train_df.columns)
[col for col in X_train.columns]
132/202:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]
132/203:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

[col for col in train_df.columns]
132/204:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

[col for col in train_df.columns]
132/205:
CATEGORICAL_FEATURES = X_train.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

[col for col in train_df.columns]
CATEGORICAL_FEATURES
132/206:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

[col for col in train_df.columns]
CATEGORICAL_FEATURES
132/207:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, X_train = data_preprocessing(df)
132/208: y_train = train_df.TARGET
132/209:
neg, pos = np.bincount(y_train)
total = neg + pos
132/210:
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
132/211:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

[col for col in train_df.columns]
CATEGORICAL_FEATURES
132/212:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

[col for col in train_df.columns]
CATEGORICAL_FEATURES
132/213:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

[col for col in train_df.columns]
CATEGORICAL_FEATURES
132/214:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

[col for col in train_df.columns]
CATEGORICAL_FEATURES[0]
132/215:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

a = [col for col in train_df.columns]
CATEGORICAL_FEATURES[0] == a[0]
132/216:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()

#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

a = [col for col in train_df.columns]
print(a[0] not in CATEGORICAL_FEATURES)
132/217:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()

NUMERICAL_FEATURES = X_train[[col for col in train_df.columns and col not in CATEGORICAL_FEATURES]]

a = [col for col in train_df.columns]
print(a[0] not in CATEGORICAL_FEATURES)
132/218:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()

NUMERICAL_FEATURES = X_train[[col for col in train_df.columns & col not in CATEGORICAL_FEATURES]]

a = [col for col in train_df.columns]
print(a[0] not in CATEGORICAL_FEATURES)
132/219:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()

#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns & col not in CATEGORICAL_FEATURES]]

a = [col for col in train_df.columns]
print(a[0] not in CATEGORICAL_FEATURES)
132/220:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
train_df[CATEGORICAL_FEATURES] = imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values.reshape(-1,1))[:,0]
132/221: from app.utils.preprocessing import data_preprocessing
132/222:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, X_train = data_preprocessing(df)
132/223: y_train = train_df.TARGET
132/224:
neg, pos = np.bincount(y_train)
total = neg + pos
132/225:
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
132/226:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()

#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns & col not in CATEGORICAL_FEATURES]]

a = [col for col in train_df.columns]
print(a[0] not in CATEGORICAL_FEATURES)
132/227:
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
132/228:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
train_df[CATEGORICAL_FEATURES] = imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values.reshape(-1,1))[:,0]
132/229:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
train_df[CATEGORICAL_FEATURES] = imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values)[:,0]
132/230: train_df.shape
132/231: from app.utils.preprocessing import data_preprocessing
132/232:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, X_train = data_preprocessing(df)
132/233: train_df.shape
132/234: y_train = train_df.TARGET
132/235: train_df[CATEGORICAL_FEATURES]
132/236: train_df[CATEGORICAL_FEATURES].shape
132/237: y_train = train_df.TARGET
132/238:
neg, pos = np.bincount(y_train)
total = neg + pos
132/239:
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
132/240:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()

#NUMERICAL_FEATURES = X_train[[col for col in train_df.columns & col not in CATEGORICAL_FEATURES]]

a = [col for col in train_df.columns]
print(a[0] not in CATEGORICAL_FEATURES)
132/241:
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
132/242: train_df[CATEGORICAL_FEATURES].shape
132/243:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
train_df[CATEGORICAL_FEATURES] = imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values.reshape(-1,1))
132/244:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values.reshape(-1,1))
132/245:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values.reshape(-1,1)).shape
132/246:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values).shape
132/247:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values)
132/248:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values)[:,0]
132/249:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values)
132/250:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
train_df[CATEGORICAL_FEATURES] = imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values)
132/251:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
train_df[CATEGORICAL_FEATURES] = imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values)
train_df
132/252: train_df.isnull().sum()
132/253: train_df.isnull().dtypes
132/254:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = train_df.select_dtypes(exclude='object').columns.to_numpy()

NUMERICAL_FEATURES
132/255:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = train_df.select_dtypes(exclude='object').columns.to_numpy()
132/256:
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
train_df[CATEGORICAL_FEATURES] = imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values)

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
train_df[NUMERICAL_FEATURES] = imputer.fit_transform(train_df[NUMERICAL_FEATURES].values)
132/257: train_df
132/258: train_df.isnull().sum()
132/259: train_df.isnull().sum().value_counts()
132/260:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
132/261:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
132/262: train_df.shape
132/263:
categorical_transform(train_df)
X_train = train_df.loc[:, train_df.columns != 'TARGET']
y_train = train_df.TARGET
X_train = scale_features(X_train)
132/264: X_train.shape
132/265: keras_model.fit(X_train, y_train, epochs=10, batch_size=200, class_weight=class_weight)
132/266: keras_model.fit(X_train, y_train, epochs=10, batch_size=100, class_weight=class_weight)
132/267:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.5] = 1
yhat[yhat < 0.5] = 0
132/268:
from sklearn import metrics
from sklearn.metrics import roc_auc_score, precision_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_train, yhat))
print(precision_score(y_train, yhat))
132/269:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, _ = data_preprocessing(df)
132/270: train_df.shape
132/271:
neg, pos = np.bincount(y_train)
total = neg + pos
132/272:
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
132/273:
weight_0 = (1 / neg) * (total / 2.0)
weight_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_0, 1: weight_1}
132/274:
neg, pos = np.bincount(y_train)
total = neg + pos
132/275:
weight_0 = (1 / neg) * (total / 2.0)
weight_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_0, 1: weight_1}
132/276:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = train_df.select_dtypes(exclude='object').columns.to_numpy()
132/277:
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
132/278:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
132/279:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
132/280: train_df.shape
132/281:
categorical_transform(train_df)

X_train = train_df.loc[:, train_df.columns != 'TARGET']
X_train = scale_features(X_train)

y_train = train_df.TARGET
132/282: X_train.shape
132/283:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]

keras_model = Sequential()
keras_model.add(Dense(16, activation='relu', input_dim=119))
keras_model.add(Dropout(0.5))
keras_model.add(Dense(1, activation='sigmoid')) 

keras_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=METRICS)
132/284: print(keras_model.summary())
132/285: keras_model.fit(X_train, y_train, epochs=10, batch_size=100, class_weight=class_weight)
132/286:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.5] = 1
yhat[yhat < 0.5] = 0
132/287:
from sklearn import metrics
from sklearn.metrics import roc_auc_score, precision_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_train, yhat))
print(precision_score(y_train, yhat))
132/288: from app.utils.preprocessing import data_preprocessing
132/289:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, _ = data_preprocessing(df)
132/290:
neg, pos = np.bincount(y_train)
total = neg + pos
132/291:
weight_0 = (1 / neg) * (total / 2.0)
weight_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_0, 1: weight_1}
132/292:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = train_df.select_dtypes(exclude='object').columns.to_numpy()
132/293:
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
132/294:
# filling np.nan values

imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
train_df[CATEGORICAL_FEATURES] = imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values)

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
train_df[NUMERICAL_FEATURES] = imputer.fit_transform(train_df[NUMERICAL_FEATURES].values)
132/295:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
132/296:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
132/297:
categorical_transform(train_df)

X_train = train_df.loc[:, train_df.columns != 'TARGET']
X_train = scale_features(X_train)

y_train = train_df.TARGET
132/298: X_train.shape
132/299:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]

keras_model = Sequential()
keras_model.add(Dense(16, activation='relu', input_dim=119))
keras_model.add(Dropout(0.5))
keras_model.add(Dense(1, activation='sigmoid')) 

keras_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=METRICS)
132/300: print(keras_model.summary())
132/301: keras_model.fit(X_train, y_train, epochs=10, batch_size=100, class_weight=class_weight)
132/302:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.5] = 1
yhat[yhat < 0.5] = 0
132/303:
from sklearn import metrics
from sklearn.metrics import roc_auc_score, precision_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_train, yhat))
print(precision_score(y_train, yhat))
132/304: keras_model.save("keras")
132/305: again = keras_model.load("keras")
132/306: again = keras.load("keras")
132/307: again = keras.models.load_model("keras")
132/308:
again = keras.models.load_model("keras")
again.summary()
132/309:
again = keras.models.load_model("keras")
again.predict(X_train)
132/310: keras_model.save("keras_model")
132/311:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/312:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/313:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/314:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/315:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/316:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/317:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/318:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/319:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/320:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/321:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/322:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/323:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/324:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/325:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/326:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/327:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/328:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/329: from app.utils.preprocessing import data_preprocessing
132/330:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, _ = data_preprocessing(df)
132/331:
neg, pos = np.bincount(y_train)
total = neg + pos
132/332:
weight_0 = (1 / neg) * (total / 2.0)
weight_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_0, 1: weight_1}
132/333:
CATEGORICAL_FEATURES = train_df.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = train_df.select_dtypes(exclude='object').columns.to_numpy()
132/334:
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
132/335:
# filling np.nan values

imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
train_df[CATEGORICAL_FEATURES] = imputer.fit_transform(train_df[CATEGORICAL_FEATURES].values)

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
train_df[NUMERICAL_FEATURES] = imputer.fit_transform(train_df[NUMERICAL_FEATURES].values)
132/336:
def categorical_transform(cleaned_base):
    for feature in cleaned_base:
        dtype = cleaned_base[feature].dtypes
        if dtype.name == 'object' or dtype.name == 'category':
            cleaned_base[feature] = pd.Categorical(cleaned_base[feature])
            cleaned_base[feature] = cleaned_base[feature].cat.codes
132/337:
from sklearn.preprocessing import MinMaxScaler

def scale_features(X):
    scaler = MinMaxScaler()
    
    # applying minMaxScaling to X feature vector
    X = scaler.fit_transform(X)
    
    # transforming back to dataframe
    X = pd.DataFrame(X)
    return X
132/338:
categorical_transform(train_df)

X_train = train_df.loc[:, train_df.columns != 'TARGET']
X_train = scale_features(X_train)

y_train = train_df.TARGET
132/339: X_train.shape
132/340:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]

keras_model = Sequential()
keras_model.add(Dense(16, activation='relu', input_dim=119))
keras_model.add(Dropout(0.5))
keras_model.add(Dense(1, activation='sigmoid')) 

keras_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=METRICS)
132/341: print(keras_model.summary())
132/342: keras_model.fit(X_train, y_train, epochs=10, batch_size=100, class_weight=class_weight)
132/343: keras_model.fit(X_train, y_train, epochs=20, batch_size=100, class_weight=class_weight)
132/344: keras_model.fit(X_train, y_train, epochs=20, batch_size=150, class_weight=class_weight)
132/345:
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from tensorflow import keras
import tensorflow as tf

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
]

keras_model = Sequential()
keras_model.add(Dense(32, activation='relu', input_dim=119))
keras_model.add(Dense(16, activation='relu'))
keras_model.add(Dropout(0.5))
keras_model.add(Dense(1, activation='sigmoid')) 

keras_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss=keras.losses.BinaryCrossentropy(),
    metrics=METRICS)
132/346: print(keras_model.summary())
132/347: keras_model.fit(X_train, y_train, epochs=20, batch_size=150, class_weight=class_weight)
132/348:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.5] = 1
yhat[yhat < 0.5] = 0
132/349:
from sklearn import metrics
from sklearn.metrics import roc_auc_score, precision_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_train, yhat))
print(precision_score(y_train, yhat))
132/350: keras_model.save("keras_model")
132/351:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/352:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/353:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/354:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/355:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/356:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/357:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/358:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/359: req_json = {'req-dataset': path_oot}
132/360:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/361:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/362:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/363:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/364:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/365:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/366:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/367:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/368:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/369:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/370:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.2] = 1
yhat[yhat < 0.2] = 0
132/371:
from sklearn import metrics
from sklearn.metrics import roc_auc_score, precision_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_train, yhat))
print(precision_score(y_train, yhat))
132/372:
yhat = keras_model.predict(X_train)
yhat[yhat >= 0.5] = 1
yhat[yhat < 0.5] = 0
132/373:
from sklearn import metrics
from sklearn.metrics import roc_auc_score, precision_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)
print(roc_auc_score(y_train, yhat))
print(precision_score(y_train, yhat))
132/374:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/375:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/376:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/377:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/378:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/379:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/380:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/381:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/382:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/383:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/384:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/385:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/386:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/387:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/388:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/389:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/390:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/391:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/392:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/393:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/394:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/395:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/396:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/397:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/398:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/399:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/400:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/401:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/402:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/403:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/keras'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/404:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/405:
yhat = keras_model.predict(X_train)
print(roc_auc_score(y_train, yhat))
yhat[yhat >= 0.5] = 1
yhat[yhat < 0.5] = 0
132/406:
from sklearn import metrics
from sklearn.metrics import roc_auc_score, precision_score

confusion_matrix = metrics.confusion_matrix(y_train, yhat)
print(confusion_matrix)

print(precision_score(y_train, yhat))
132/407:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/408:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/409:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/410:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/411:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/412:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/413:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/414:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/415:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/416:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/417:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/418:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/419:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/420:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/421:
enhanced_model = read_default_model()
enhanced_model.steps.pop(1)
enhanced_model.steps.append(['encoder', DecisionTreeClassifier(max_depth=8, min_samples_leaf=0.15371419169712677,
                       min_samples_split=0.2572078354486276, class_weight={0: 1.0, 1: 0.28})])
    
train_df, X_train = read_traindb()
y_train = train_df.TARGET
enhanced_model.fit(X_train, y_train)
132/422:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
from app.utils.reads import read_default_model
132/423:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/scikit'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/424:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/425:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
from app.utils.reads import read_default_model
132/426:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
#from app.utils.reads import read_default_model
132/427:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
from app.utils.reads import read_default_model
132/428:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.reads import read_default_model
from app.utils.preprocessing import data_preprocessing
132/429:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.reads import read_default_model
#from app.utils.preprocessing import data_preprocessing
132/430:
enhanced_model = pickle.load(open('model.pkl'))
enhanced_model.steps.pop(1)
enhanced_model.steps.append(['encoder', DecisionTreeClassifier(max_depth=8, min_samples_leaf=0.15371419169712677,
                       min_samples_split=0.2572078354486276, class_weight={0: 1.0, 1: 0.28})])
    
train_df, X_train = read_traindb()
y_train = train_df.TARGET
enhanced_model.fit(X_train, y_train)
132/431:
enhanced_model = pickle.load(open('model.pkl', 'rb'))
enhanced_model.steps.pop(1)
enhanced_model.steps.append(['encoder', DecisionTreeClassifier(max_depth=8, min_samples_leaf=0.15371419169712677,
                       min_samples_split=0.2572078354486276, class_weight={0: 1.0, 1: 0.28})])
    
train_df, X_train = read_traindb()
y_train = train_df.TARGET
enhanced_model.fit(X_train, y_train)
132/432:
from sklearn.tree import DecisionTreeClassifier

enhanced_model = pickle.load(open('model.pkl', 'rb'))
enhanced_model.steps.pop(1)
enhanced_model.steps.append(['encoder', DecisionTreeClassifier(max_depth=8, min_samples_leaf=0.15371419169712677,
                       min_samples_split=0.2572078354486276, class_weight={0: 1.0, 1: 0.28})])
    
train_df, X_train = read_traindb()
y_train = train_df.TARGET
enhanced_model.fit(X_train, y_train)
132/433:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
from app.utils.reads import read_traindb
132/434:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
#from app.utils.reads import read_traindb
132/435: from app.utils.preprocessing import data_preprocessing
132/436:
df = pd.read_csv('../datasets/credit_01/train.gz',
                 compression='gzip', header=0, low_memory=False)
train_df, _ = data_preprocessing(df)
132/437:
neg, pos = np.bincount(y_train)
total = neg + pos
132/438:
weight_0 = (1 / neg) * (total / 2.0)
weight_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_0, 1: weight_1}
132/439:
from sklearn.tree import DecisionTreeClassifier

enhanced_model = pickle.load(open('model.pkl', 'rb'))
enhanced_model.steps.pop(1)
enhanced_model.steps.append(['encoder', DecisionTreeClassifier(max_depth=8, min_samples_leaf=0.15371419169712677,
                       min_samples_split=0.2572078354486276, class_weight={0: 1.0, 1: 0.28})])

y_train = train_df.TARGET
enhanced_model.fit(X_train, y_train)
132/440:
with open('enhanced_model.pkl', 'w') as file:
    pickle.dump(enhanced_model, file)
132/441:
with open('enhanced_model.pkl', 'w') as file:
    pickle.dumps(enhanced_model, file)
132/442:
with open('enhanced_model.pkl', 'wb') as file:
    pickle.dumps(enhanced_model, file)
132/443:
with open('enhanced_model.pkl', 'wb') as file:
    pickle.dump(enhanced_model, file)
132/444:
with open('enhanced_model.pkl', 'wb') as file:
    pickle.dump(enhanced_model, file)
132/445:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/446:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/447:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/448:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/449:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/450:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/451:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/452:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/453:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/454:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/455:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/456:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/457:
from joblib import dump

dump(enhanced_model, 'enhanced_model.joblib')
132/458:
from joblib import dump

a = pickle.load(open('model.pkl', 'rb'))
pickle.dump(a, 'teste.pkl')
132/459:
from joblib import dump

a = pickle.load(open('model.pkl', 'rb'))
pickle.dump(a, open('teste.pkl', 'w'))
132/460:
from joblib import dump

a = pickle.load(open('model.pkl', 'rb'))
pickle.dumps(a, open('teste.pkl', 'w'))
132/461:
from joblib import dump

a = pickle.load(open('model.pkl', 'rb'))
pickle.dumps(a, open('teste.pkl', 'wb'))
132/462:
from joblib import dump

a = pickle.load(open('model.pkl', 'rb'))
pickle.dump(a, open('teste.pkl', 'wb'))
132/463:
home = 'http://127.0.0.1:8001'
endpoint = '/performance'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/464:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/465:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/466:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/467:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/468:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/469:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/470:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/471: req_json = {'req-dataset': path_train}
132/472:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/473:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/474: req_json = {'req-dataset': path_oot}
132/475:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
132/476:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/477:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/478:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
132/479:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
132/480:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/1:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/2:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
#from app.utils.reads import read_traindb
133/3:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/4:
records_file = open('batch_records.json')
records_json = json.load(records_file)
133/5: type(records_json)
133/6: type(records_json[0])
133/7:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/8:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/9:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
133/10: req_json = {'req-dataset': path_oot}
133/11:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
133/12:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/13:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/14:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/15:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/16:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/17:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/18:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/19:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/20:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/21:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/22:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/23:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/24:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/25:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/26:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/27:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/28:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/aaaa'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/29:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/30:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/aaaa'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/31:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/32:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/33:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/34:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/aedjklnfd'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/35:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/36:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/37:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/38:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=[1], headers=headers)
133/39:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/40:
df = pd.read_csv('../../ait_01/test.gz',
                     compression='gzip', header=0, low_memory=False)
133/41:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=[1], headers=headers)
133/42:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/43:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/44:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/45:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/46:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/47:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/48:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/49:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/enhanced'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/50:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/51:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/52:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/53:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/54:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/55:
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
133/56:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/57:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/58:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/59: req_json = {'req-dataset': path_train}
133/60:
req_json = {'req-dataset': path_train}

res = requests.post(home + endpoint, json=req_json, headers=headers)
133/61:
req_json = {'req-dataset': path_oot}
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
133/62:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/63:
req_json = {'req-dataset': path_train}

res = requests.post(home + endpoint, json=req_json, headers=headers)
133/64:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/65:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/66:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/67:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/68:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/69:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/70:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/71:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/72:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/73:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/74:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/75:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/76:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/77:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/78:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/79:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/80:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/81:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/82:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/83:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/84:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/85:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/86:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/87:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/88:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/89:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/90:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/91:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/92:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/93:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/94:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/95:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/96:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/97:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/98:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/99:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/100:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/101:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/102:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/103:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/104:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/105:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/106:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/107:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/108:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/109:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/110:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/111:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/112:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/113:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/114:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/115:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/116:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/117:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/118:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/119:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/120:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/121:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/122:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/123:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/124:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/125:
req_json = {'req-dataset': path_oot}
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
133/126:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/127:
req_json = {'req-dataset': path_oot}
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
133/128:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/129:
req_json = {'req-dataset': path_train}

res = requests.post(home + endpoint, json=req_json, headers=headers)
133/130:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/131:
records_file = open('batch_records.json')
records_json = json.load(records_file)
133/132: type(records_json)
133/133: type(records_json[0])
133/134:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/135:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
133/136:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
133/137:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
134/1:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
#from app.utils.reads import read_traindb
134/2:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
134/3:
req_json = {'req-dataset': path_oot}
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
134/4:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
134/5:
req_json = {'req-dataset': path_train}

res = requests.post(home + endpoint, json=req_json, headers=headers)
134/6:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
134/7:
records_file = open('batch_records.json')
records_json = json.load(records_file)
134/8: type(records_json)
134/9: type(records_json[0])
134/10:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
134/11:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
134/12:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
134/13:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
134/14:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
#from app.utils.reads import read_traindb
134/15:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
134/16:
req_json = {'req-dataset': path_oot}
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
134/17:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
134/18:
req_json = {'req-dataset': path_train}

res = requests.post(home + endpoint, json=req_json, headers=headers)
134/19:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
134/20:
records_file = open('batch_records.json')
records_json = json.load(records_file)
134/21: type(records_json)
134/22: type(records_json[0])
134/23:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
134/24:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
134/25:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
134/26:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
135/1:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
#from app.utils.reads import read_traindb
135/2:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
135/3:
req_json = {'req-dataset': path_oot}
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
135/4:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
135/5:
req_json = {'req-dataset': path_train}

res = requests.post(home + endpoint, json=req_json, headers=headers)
135/6:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
135/7:
records_file = open('batch_records.json')
records_json = json.load(records_file)
135/8: type(records_json)
135/9: type(records_json[0])
135/10:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
135/11:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
135/12:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
135/13:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
135/14:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
#from app.utils.reads import read_traindb
135/15:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
135/16:
req_json = {'req-dataset': path_oot}
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
135/17:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
135/18:
req_json = {'req-dataset': path_train}

res = requests.post(home + endpoint, json=req_json, headers=headers)
135/19:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
135/20:
records_file = open('batch_records.json')
records_json = json.load(records_file)
135/21: type(records_json)
135/22: type(records_json[0])
135/23:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
135/24:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
135/25:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
135/26:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
136/1:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
#from app.utils.reads import read_traindb
136/2:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
136/3:
req_json = {'req-dataset': path_oot}
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
136/4:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
136/5:
req_json = {'req-dataset': path_train}

res = requests.post(home + endpoint, json=req_json, headers=headers)
136/6:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
136/7:
records_file = open('batch_records.json')
records_json = json.load(records_file)
136/8: type(records_json)
136/9: type(records_json[0])
136/10:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
136/11:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
136/12:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
136/13:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
136/14:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
#from app.utils.reads import read_traindb
136/15:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
136/16:
req_json = {'req-dataset': path_oot}
home = 'http://127.0.0.1:8001'
endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=req_json, headers=headers)
136/17:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
136/18:
req_json = {'req-dataset': path_train}

res = requests.post(home + endpoint, json=req_json, headers=headers)
136/19:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
136/20:
records_file = open('batch_records.json')
records_json = json.load(records_file)
136/21: type(records_json)
136/22: type(records_json[0])
136/23:
home = 'http://127.0.0.1:8001'
endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

res = requests.post(home + endpoint, json=records_json, headers=headers)
136/24:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
136/25:
endpoint = '/performance/enhanced'

res = requests.post(home + endpoint, json=records_json, headers=headers)
136/26:
res_parsed = json.loads(res.text)
print(json.dumps(res_parsed, indent=4))
136/27:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
136/28:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
136/29:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
136/30:
req_json = {'req-dataset': path_oot}
home = 'http://localhost:8001'
ad_endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /aderencia
res_oot = requests.post(home + ad_endpoint, json=req_json, headers=headers)
136/31:
res_parsed = json.loads(res_oot.text)
print(json.dumps(res_parsed, indent=4))
136/32:
req_json = {'req-dataset': path_train}

# POST feito no endpoint /aderencia
res_train = requests.post(home + ad_endpoint, json=req_json, headers=headers)
136/33:
res_parsed = json.loads(res_train.text)
print(json.dumps(res_parsed, indent=4))
136/34:
records_file = open('batch_records.json')
records_json = json.load(records_file)
136/35: type(records_json)
136/36: type(records_json[0])
136/37: res_enhanced_parsed['roc_score'] - res_default_parsed['roc_score']
136/38:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
136/39:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
136/40:
req_json = {'req-dataset': path_oot}
home = 'http://localhost:8001'
ad_endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /aderencia
res_oot = requests.post(home + ad_endpoint, json=req_json, headers=headers)
136/41:
res_parsed = json.loads(res_oot.text)
print(json.dumps(res_parsed, indent=4))
136/42:
req_json = {'req-dataset': path_train}

# POST feito no endpoint /aderencia
res_train = requests.post(home + ad_endpoint, json=req_json, headers=headers)
136/43:
res_parsed = json.loads(res_train.text)
print(json.dumps(res_parsed, indent=4))
136/44:
records_file = open('batch_records.json')
records_json = json.load(records_file)
136/45: type(records_json)
136/46: type(records_json[0])
136/47:
home = 'http://localhost:8001'
default_endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /performance/{model_from}
res_default = requests.post(home + default_endpoint, json=records_json, headers=headers)
136/48:
res_default_parsed = json.loads(res_default.text)
print(json.dumps(res_default_parsed, indent=4))
136/49:
enhanced_endpoint = '/performance/enhanced'

# POST feito no endpoint /performance/{model_from}
res_enhanced = requests.post(home + enhanced_endpoint, json=records_json, headers=headers)
136/50:
res_enhanced_parsed = json.loads(res_enhanced.text)
print(json.dumps(res_enhanced_parsed, indent=4))
136/51: ganho_percentual = res_enhanced_parsed['roc_score'] - res_default_parsed['roc_score']
136/52: print(round(ganho_percentual*100, 2))
136/53: print(round(ganho_percentual*100, 2) + '%')
136/54: print(str(round(ganho_percentual*100, 2)) + '%')
137/1:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
137/2:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
137/3:
req_json = {'req-dataset': path_oot}
home = 'http://localhost:8001'
ad_endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /aderencia
res_oot = requests.post(home + ad_endpoint, json=req_json, headers=headers)
137/4:
res_parsed = json.loads(res_oot.text)
print(json.dumps(res_parsed, indent=4))
137/5:
req_json = {'req-dataset': path_train}

# POST feito no endpoint /aderencia
res_train = requests.post(home + ad_endpoint, json=req_json, headers=headers)
137/6:
res_parsed = json.loads(res_train.text)
print(json.dumps(res_parsed, indent=4))
137/7:
records_file = open('batch_records.json')
records_json = json.load(records_file)
137/8: type(records_json)
137/9: type(records_json[0])
137/10:
home = 'http://localhost:8001'
default_endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /performance/{model_from}
res_default = requests.post(home + default_endpoint, json=records_json, headers=headers)
137/11:
res_default_parsed = json.loads(res_default.text)
print(json.dumps(res_default_parsed, indent=4))
137/12:
enhanced_endpoint = '/performance/enhanced'

# POST feito no endpoint /performance/{model_from}
res_enhanced = requests.post(home + enhanced_endpoint, json=records_json, headers=headers)
137/13:
res_enhanced_parsed = json.loads(res_enhanced.text)
print(json.dumps(res_enhanced_parsed, indent=4))
137/14: ganho_percentual = res_enhanced_parsed['roc_score'] - res_default_parsed['roc_score']
137/15: print(str(round(ganho_percentual*100, 2)) + '%')
138/1:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
138/2:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
138/3:
req_json = {'req-dataset': path_oot}
home = 'http://localhost:8001'
ad_endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /aderencia
res_oot = requests.post(home + ad_endpoint, json=req_json, headers=headers)
138/4:
res_parsed = json.loads(res_oot.text)
print(json.dumps(res_parsed, indent=4))
138/5:
req_json = {'req-dataset': path_train}

# POST feito no endpoint /aderencia
res_train = requests.post(home + ad_endpoint, json=req_json, headers=headers)
138/6:
res_parsed = json.loads(res_train.text)
print(json.dumps(res_parsed, indent=4))
138/7:
records_file = open('batch_records.json')
records_json = json.load(records_file)
138/8: type(records_json)
138/9: type(records_json[0])
138/10:
home = 'http://localhost:8001'
default_endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /performance/{model_from}
res_default = requests.post(home + default_endpoint, json=records_json, headers=headers)
138/11:
res_default_parsed = json.loads(res_default.text)
print(json.dumps(res_default_parsed, indent=4))
138/12:
enhanced_endpoint = '/performance/enhanced'

# POST feito no endpoint /performance/{model_from}
res_enhanced = requests.post(home + enhanced_endpoint, json=records_json, headers=headers)
138/13:
res_enhanced_parsed = json.loads(res_enhanced.text)
print(json.dumps(res_enhanced_parsed, indent=4))
138/14: ganho_percentual = res_enhanced_parsed['roc_score'] - res_default_parsed['roc_score']
138/15: print(str(round(ganho_percentual*100, 2)) + '%')
138/16:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
138/17:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
138/18:
req_json = {'req-dataset': path_oot}
home = 'http://localhost:8001'
ad_endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /aderencia
res_oot = requests.post(home + ad_endpoint, json=req_json, headers=headers)
138/19:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
138/20:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
138/21:
req_json = {'req-dataset': path_oot}
home = 'http://localhost:8001'
ad_endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /aderencia
res_oot = requests.post(home + ad_endpoint, json=req_json, headers=headers)
139/1:
import pandas as pd
pd.options.mode.chained_assignment = None
import pickle
import json
import requests
import numpy as np
from app.utils.preprocessing import data_preprocessing
139/2:
# arquivos locais do repositório base
# os caminhos são relativos à API 
path_train = '../../datasets/credit_01/train.gz'
path_oot   = '../../datasets/credit_01/oot.gz'
139/3:
req_json = {'req-dataset': path_oot}
home = 'http://localhost:8001'
ad_endpoint = '/aderencia'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /aderencia
res_oot = requests.post(home + ad_endpoint, json=req_json, headers=headers)
139/4:
res_parsed = json.loads(res_oot.text)
print(json.dumps(res_parsed, indent=4))
139/5:
req_json = {'req-dataset': path_train}

# POST feito no endpoint /aderencia
res_train = requests.post(home + ad_endpoint, json=req_json, headers=headers)
139/6:
res_parsed = json.loads(res_train.text)
print(json.dumps(res_parsed, indent=4))
139/7:
records_file = open('batch_records.json')
records_json = json.load(records_file)
139/8: type(records_json)
139/9: type(records_json[0])
139/10:
home = 'http://localhost:8001'
default_endpoint = '/performance/default'
headers = {'Content-Type': 'application/json', 'accept': 'application/json'}

# POST feito no endpoint /performance/{model_from}
res_default = requests.post(home + default_endpoint, json=records_json, headers=headers)
139/11:
res_default_parsed = json.loads(res_default.text)
print(json.dumps(res_default_parsed, indent=4))
139/12:
enhanced_endpoint = '/performance/enhanced'

# POST feito no endpoint /performance/{model_from}
res_enhanced = requests.post(home + enhanced_endpoint, json=records_json, headers=headers)
139/13:
res_enhanced_parsed = json.loads(res_enhanced.text)
print(json.dumps(res_enhanced_parsed, indent=4))
139/14: ganho_percentual = res_enhanced_parsed['roc_score'] - res_default_parsed['roc_score']
139/15: print(str(round(ganho_percentual*100, 2)) + '%')
140/1:
# Importing dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import geopandas as gpd
import seaborn as sns
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
140/2:
# Reading base dataset
base = pd.read_csv("./sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
140/3:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M']
for column in notas:
    base[column] = base[column].str.replace(',', '.').astype(float)
140/4: base.shape
140/5: base.APROVADO.value_counts()
140/6:
aprovados = base[base.APROVADO == 'S']
percentual_aprovados = aprovados.shape[0] / base.shape[0]
print('Percentual de aprovados no SISU 2022:', round(percentual_aprovados * 100, 2), '%')
140/7:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TURNO', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)
corte.NOTA_CORTE = corte.NOTA_CORTE.str.replace(',', '.').astype(float)
140/8:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TURNO', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte_ampla = corte[corte.TIPO_MOD_CONCORRENCIA == 'A']
corte_bonus = corte[corte.TIPO_MOD_CONCORRENCIA == 'B']
corte = pd.concat([corte_ampla, corte_bonus], axis=0).sort_values(by='NOTA_CORTE', ascending=False)
140/9:
# UFPE ampla concorrência e bonificação

corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe.drop_duplicates().reset_index(drop=True)
corte_ufpe.head(6)
140/10:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.yticks(range(0, 950, 50))
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
140/11:
aprovados_ufpe = aprovados[aprovados.SIGLA_IES == 'UFPE']
med_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'MEDICINA']
med_ufpe.TIPO_MOD_CONCORRENCIA.value_counts()
140/12:
def situacao_por_estado(aprovados_curso):
    fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
    fig.tight_layout()
    
    count = aprovados_curso.UF_CANDIDATO.value_counts().reset_index()
    count.columns = ['UF', 'COUNT']
    sns.barplot(data=count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(count, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))
140/13:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
situacao_por_estado(aprovados_med)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
140/14:
cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']
aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]

situacao_por_estado(aprovados_tech)

plt.title("Distribuição por estado dos aprovados\n em cursos de computação pelo SISU 2022")
plt.show()
140/15:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas.columns = ['UF', 'LING', 'HUM', 'NAT', 'MAT', 'RED']
notas_heatmap = notas.pivot_table(columns='UF', aggfunc='mean')
140/16:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black', ax=ax)
plt.title("Nota média dos candidatos por UF e áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.yticks(rotation=0)
plt.show()
140/17: base.head(1)
140/18:
med_por_estado = medicina.UF_CANDIDATO.value_counts()
med_por_estado
140/19:
redacao_estado = base[['UF_CANDIDATO', 'NOTA_R']]
redacao_estado
140/20:
redacao_estado = base[['UF_CANDIDATO', 'NOTA_R']]
redacao_estado.groupby(by='UF_CANDIDATO').sum()
140/21:
redacao_estado = base[['UF_CANDIDATO', 'NOTA_R']]
redacao_estado
140/22:
redacao_estado = base[['UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R >= 900]
redacao_estado
140/23:
redacao_estado = base[['UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R >= 900]
redacao_estado
print(base.shape)
140/24:
redacao_estado = base[['UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R >= 900]
print(base.shape)
redacao_estado
140/25:
redacao_estado = base[['UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R >= 900]
count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
count_uf_900_R
140/26:
redacao_estado = base[['UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R >= 960]
count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
count_uf_900_R
140/27:
redacao_estado = base[['UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000]
count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
count_uf_900_R
140/28:
redacao_estado = base[['NOME_CANDIDATO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000]
count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
count_uf_900_R
140/29: base
140/30:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000]
count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
count_uf_900_R
140/31:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000]
#count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
#count_uf_900_R
redacao_estado
140/32:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000]
#count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
#count_uf_900_R
redacao_estado.reset_index(drop=True)
140/33:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000]
#count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
#count_uf_900_R
redacao_estado.reset_index(drop=True).sort_values(by='INSCRITO', ascending=True)
140/34:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000].drop_duplicates()
#count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
#count_uf_900_R
redacao_estado.reset_index(drop=True).sort_values(by='INSCRITO', ascending=True)
140/35:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000].drop_duplicates()
count_uf_900_R = redacao_estado.UF_CANDIDATO.value_counts()
count_uf_900_R
140/36:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000].drop_duplicates()

situacao_por_estado(redacao_estado)
140/37:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000].drop_duplicates()

situacao_por_estado(redacao_estado)

plt.title("Distribuição por estado dos que tiraram nota 1000 na Redação")
plt.show()
140/38:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000].drop_duplicates()

situacao_por_estado(redacao_estado)

plt.title("Distribuição por estado dos que\ntiraram nota 1000 na Redação")
plt.show()
140/39: base
140/40:
nota_M_CH = base[['INSCRICAO_ENEM', 'INSCRITO', 'NOTA_M', 'NOTA_R']]
nota_M_CH.shape
140/41:
nota_M_CH = base[['INSCRICAO_ENEM', 'INSCRITO', 'NOTA_M', 'NOTA_R']].drop_duplicates()
nota_M_CH.shape
140/42:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TURNO', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA.str.contains('|'.join(['A', 'B']))].sort_values(by='NOTA_CORTE', ascending=False)
140/43:
corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe.drop_duplicates().reset_index(drop=True)
corte_ufpe.head(6)
140/44:
# UFPE ampla concorrência e bonificação

corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe.drop_duplicates().reset_index(drop=True)
corte_ufpe.head(6)
140/45:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.yticks(range(0, 950, 50))
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
140/46:
aprovados_ufpe = aprovados[aprovados.SIGLA_IES == 'UFPE']
med_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'MEDICINA']
med_ufpe.TIPO_MOD_CONCORRENCIA.value_counts()
140/47:
def situacao_por_estado(aprovados_curso):
    fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
    fig.tight_layout()
    
    count = aprovados_curso.UF_CANDIDATO.value_counts().reset_index()
    count.columns = ['UF', 'COUNT']
    sns.barplot(data=count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

    estados = gpd.read_file('bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(count, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))
140/48:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
situacao_por_estado(aprovados_med)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
140/49:
cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']
aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]

situacao_por_estado(aprovados_tech)

plt.title("Distribuição por estado dos aprovados\n em cursos de computação pelo SISU 2022")
plt.show()
140/50:
notas = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas.columns = ['UF', 'LING', 'HUM', 'NAT', 'MAT', 'RED']
notas_heatmap = notas.pivot_table(columns='UF', aggfunc='mean')
140/51:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black', ax=ax)
plt.title("Nota média dos candidatos por UF e áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.yticks(rotation=0)
plt.show()
140/52:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000].drop_duplicates()

situacao_por_estado(redacao_estado)

plt.title("Distribuição por estado dos que\ntiraram nota 1000 na Redação")
plt.show()
140/53:
notas = base.drop_duplicates(subset=['INSCRICAO_ENEM', 'INSCRITO'])
notas.shape
140/54:
nota_M_CH = base[['INSCRICAO_ENEM', 'INSCRITO', 'NOTA_M', 'NOTA_R']].drop_duplicates(subset=['INSCRICAO_ENEM', 'INSCRITO'])
nota_M_CH.shape
140/55: base
140/56:
nota_M_CH = base[['INSCRICAO_ENEM', 'INSCRITO', 'NOTA_M', 'NOTA_R']].drop_duplicates(subset=['INSCRICAO_ENEM', 'INSCRITO', 'CPF'])
nota_M_CH.shape
140/57:
nota_M_CH = base[['INSCRICAO_ENEM', 'INSCRITO', 'NOTA_M', 'NOTA_R', 'CPF']].drop_duplicates(subset=['INSCRICAO_ENEM', 'INSCRITO', 'CPF'])
nota_M_CH.shape
140/58:
nota_M_CH = base[['INSCRICAO_ENEM', 'INSCRITO', 'NOTA_M', 'NOTA_R', 'CPF']].drop_duplicates(subset=['INSCRITO', 'CPF'])
nota_M_CH.shape
140/59:
notas = base.drop_duplicates(subset=['CPF', 'INSCRITO'])
notas.shape
140/60: notas = base.drop_duplicates(subset=['CPF', 'INSCRITO'])
140/61:
notas = base.drop_duplicates(subset=['CPF', 'INSCRITO'])
notas.shape
140/62:
notas_uf = base[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_uf.columns = ['UF', 'LING', 'HUM', 'NAT', 'MAT', 'RED']
notas_heatmap = notas_uf.pivot_table(columns='UF', aggfunc='mean')
140/63:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black', ax=ax)
plt.title("Nota média dos candidatos por UF e áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.yticks(rotation=0)
plt.show()
140/64:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000].drop_duplicates()

situacao_por_estado(redacao_estado)

plt.title("Distribuição por estado dos que\ntiraram nota 1000 na Redação")
plt.show()
140/65:
notas_uf = notas[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_uf.columns = ['UF', 'LING', 'HUM', 'NAT', 'MAT', 'RED']
notas_heatmap = notas_uf.pivot_table(columns='UF', aggfunc='mean')
140/66:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black', ax=ax)
plt.title("Nota média dos candidatos por UF e áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.yticks(rotation=0)
plt.show()
140/67:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000].drop_duplicates()

situacao_por_estado(redacao_estado)

plt.title("Distribuição por estado dos que\ntiraram nota 1000 na Redação")
plt.show()
140/68:
notas_M_CH = notas
notas_M_CH.shape
140/69:
notas_M_CH = notas[['NOTA_M', 'NOTA_CH']]
notas_M_CH.shape
140/70:
notas_M_CH = notas[['NOTA_M', 'NOTA_CH']]
notas_M_CH.head(10)
140/71:
notas_M_CH = notas[['NOTA_M', 'NOTA_CH']]
notas_M_CH.shape
140/72: sns.scatterplot(data=notas_M_CH, x='NOTA_M', y='NOTA_CH')
140/73: base.isnull().sum()
140/74:
regioes = {'AC': 'NORTE', 'AL': 'NORDESTE', 'AP': 'NORTE', 'AM': 'NORTE', 'BA': 'NORDESTE', 'CE': 'NORDESTE', 'ES': 'SUDESTE', 'GO': 'CENTRO-OESTE', 'MA': 'NORDESTE', 'MT': 'CENTRO-OESTE', 'MS': 'CENTRO-OESTE', 'MG': 'SUDESTE', 'PA': 'NORTE', 'PB': 'NORDESTE', 'PR': 'SUL', 'PE': 'NORDESTE', 'PI': 'NORDESTE', 'RJ': 'SUDESTE', 'RN': 'NORDESTE', 'RS': 'SUL', 'RO': 'NORTE', 'RR': 'NORTE', 'SC': 'SUL', 'SP': 'SUDESTE', 'SE': 'NORDESTE', 'TO': 'NORTE', 'DF': 'CENTRO-OESTE'}
regioes.keys()
140/75: regioes = {'AC': 'NORTE', 'AL': 'NORDESTE', 'AP': 'NORTE', 'AM': 'NORTE', 'BA': 'NORDESTE', 'CE': 'NORDESTE', 'ES': 'SUDESTE', 'GO': 'CENTRO-OESTE', 'MA': 'NORDESTE', 'MT': 'CENTRO-OESTE', 'MS': 'CENTRO-OESTE', 'MG': 'SUDESTE', 'PA': 'NORTE', 'PB': 'NORDESTE', 'PR': 'SUL', 'PE': 'NORDESTE', 'PI': 'NORDESTE', 'RJ': 'SUDESTE', 'RN': 'NORDESTE', 'RS': 'SUL', 'RO': 'NORTE', 'RR': 'NORTE', 'SC': 'SUL', 'SP': 'SUDESTE', 'SE': 'NORDESTE', 'TO': 'NORTE', 'DF': 'CENTRO-OESTE'}
140/76:
notas_M_CH = notas[['UF_CANDIDATO', 'NOTA_M', 'NOTA_CH']]
notas_M_CH.shape
140/77: sns.scatterplot(data=notas_M_CH, x='NOTA_M', y='NOTA_CH')
140/78: notas_M_CH['REGIAO_CANDIDATO'] = notas_M_CH.UF_CANDIDATO.map(lambda x: regiao[x])
140/79:
regioes = {'AC': 'NORTE', 'AL': 'NORDESTE', 'AP': 'NORTE', 'AM': 'NORTE', 'BA': 'NORDESTE', 'CE': 'NORDESTE', 'ES': 'SUDESTE',
           'GO': 'CENTRO-OESTE', 'MA': 'NORDESTE', 'MT': 'CENTRO-OESTE', 'MS': 'CENTRO-OESTE', 'MG': 'SUDESTE', 'PA': 'NORTE',
           'PB': 'NORDESTE', 'PR': 'SUL', 'PE': 'NORDESTE', 'PI': 'NORDESTE', 'RJ': 'SUDESTE', 'RN': 'NORDESTE', 'RS': 'SUL', 
           'RO': 'NORTE', 'RR': 'NORTE', 'SC': 'SUL', 'SP': 'SUDESTE', 'SE': 'NORDESTE', 'TO': 'NORTE', 'DF': 'CENTRO-OESTE'}
140/80: notas_M_CH['REGIAO_CANDIDATO'] = notas_M_CH.UF_CANDIDATO.map(lambda x: regiao[x])
140/81: notas_M_CH['REGIAO_CANDIDATO'] = notas_M_CH.UF_CANDIDATO.map(lambda x: regioes[x])
140/82:
notas_M_CH['REGIAO_CANDIDATO'] = notas_M_CH.UF_CANDIDATO.map(lambda x: regioes[x])
notas_M_CH.shape
140/83:
notas_M_CH['REGIAO_CANDIDATO'] = notas_M_CH.UF_CANDIDATO.map(lambda x: regioes[x])
notas_M_CH.head(10)
140/84: notas_M_CH['REGIAO_CANDIDATO'] = notas_M_CH.UF_CANDIDATO.map(lambda x: regioes[x])
140/85: sns.scatterplot(data=notas_M_CH, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')
140/86: notas_M_CH_ne_se = notas_M_CH[notas_M_CH.REGIAO_CANDIDATO.str.contains('|'.join(['NORDESTE', 'SUDESTE']))]
140/87: sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')
140/88:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.show()
140/89:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO', figsize=(10, 10))

plt.legend(loc='lower left')
plt.show()
140/90:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO', figsize=(10, 10))

plt.legend(loc='lower left')
plt.figure(figsize=(10,6))
plt.show()
140/91:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.figure(figsize=(10,6))
plt.show()
140/92:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.figure(figsize=(10,10))
plt.show()
140/93:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.show()
140/94:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.show()
140/95:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.show()
140/96: notas['REGIAO_CANDIDATO'] = notas.UF_CANDIDATO.map(lambda x: regioes[x])
140/97:
notas_M_CH = notas[['UF_CANDIDATO', 'REGIAO_CANDIDATO', 'NOTA_M', 'NOTA_CH']]
notas_M_CH.shape
140/98:
regioes = {'AC': 'NORTE', 'AL': 'NORDESTE', 'AP': 'NORTE', 'AM': 'NORTE', 'BA': 'NORDESTE', 'CE': 'NORDESTE', 'ES': 'SUDESTE',
           'GO': 'CENTRO-OESTE', 'MA': 'NORDESTE', 'MT': 'CENTRO-OESTE', 'MS': 'CENTRO-OESTE', 'MG': 'SUDESTE', 'PA': 'NORTE',
           'PB': 'NORDESTE', 'PR': 'SUL', 'PE': 'NORDESTE', 'PI': 'NORDESTE', 'RJ': 'SUDESTE', 'RN': 'NORDESTE', 'RS': 'SUL', 
           'RO': 'NORTE', 'RR': 'NORTE', 'SC': 'SUL', 'SP': 'SUDESTE', 'SE': 'NORDESTE', 'TO': 'NORTE', 'DF': 'CENTRO-OESTE'}
140/99:
notas_M_CH = notas[['UF_CANDIDATO', 'REGIAO_CANDIDATO', 'NOTA_M', 'NOTA_CH']]
notas_M_CH.shape
140/100: notas_M_CH_ne_se = notas_M_CH[notas_M_CH.REGIAO_CANDIDATO.str.contains('|'.join(['NORDESTE', 'SUDESTE']))]
140/101:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.show()
140/102:
notas_M_CH = notas[['REGIAO_CANDIDATO', 'NOTA_M', 'NOTA_CH']]
notas_M_CH.shape
140/103: notas_M_CH_ne_se = notas_M_CH[notas_M_CH.REGIAO_CANDIDATO.str.contains('|'.join(['NORDESTE', 'SUDESTE']))]
140/104:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.show()
140/105:
notas_R_CH = notas[['REGIAO_CANDIDATO', 'NOTA_R', 'NOTA_CH']]
notas_R_CH.shape
140/106:
notas_regiao = notas[['REGIAO_CANDIDATO', 'NOTA_M', 'NOTA_CH']]
notas_regiao.shape
140/107:
notas_regiao = notas[['REGIAO_CANDIDATO', 'NOTA_M', 'NOTA_CH', 'NOTA_R', 'NOTA_L', 'NOTA_CN']]
notas_regiao.shape
140/108: notas_ne_se = notas_regiao[notas_regiao.REGIAO_CANDIDATO.str.contains('|'.join(['NORDESTE', 'SUDESTE']))]
140/109:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.show()
140/110:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_CH', y='NOTA_R', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em redação")
plt.xlabel("Nota em ciências humanas")
plt.show()
140/111:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.show()
140/112: notas_ne_se = notas_regiao[notas_regiao.REGIAO_CANDIDATO.str.contains('|'.join(['NORDESTE', 'SUDESTE']))]
140/113:
sns.scatterplot(data=notas_M_CH_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.show()
140/114:
sns.scatterplot(data=notas_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.show()
140/115:
sns.scatterplot(data=notas_ne_se, x='NOTA_CH', y='NOTA_R', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em redação")
plt.xlabel("Nota em ciências humanas")
plt.show()
140/116:
sns.scatterplot(data=notas_ne_se, x='NOTA_R', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em redação")
plt.show()
140/117:
sns.scatterplot(data=notas_ne_se, x='NOTA_M', y='NOTA_CN', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em redação")
plt.show()
140/118:
sns.scatterplot(data=notas_ne_se, x='NOTA_M', y='NOTA_CN', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências da natureza")
plt.xlabel("Nota em matemática")
plt.show()
140/119:
sns.scatterplot(data=notas_ne_se, x='NOTA_M', y='NOTA_CN', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências da natureza")
plt.xlabel("Nota em matemática")
plt.title("Distribuição de notas entre as regiões Nordeste e Sudeste")
plt.show()
140/120:
sns.scatterplot(data=notas_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.title("Distribuição de notas entre as regiões Nordeste e Sudeste")
plt.show()
141/1:
# Importing dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import geopandas as gpd
import seaborn as sns
pd.set_option('display.max_columns', None)
pd.options.mode.chained_assignment = None
141/2:
# Reading base dataset
base = pd.read_csv("../datasets/sisu-2022-1.csv", header=0, sep='|',encoding='mbcs', low_memory=False)
141/3:
notas = ['NOTA_CORTE', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M']
for column in notas:
    base[column] = base[column].str.replace(',', '.').astype(float)
141/4: base.shape
141/5: base.APROVADO.value_counts()
141/6:
aprovados = base[base.APROVADO == 'S']
percentual_aprovados = aprovados.shape[0] / base.shape[0]
print('Percentual de aprovados no SISU 2022:', round(percentual_aprovados * 100, 2), '%')
141/7:
aprovados = base[base.APROVADO == 'S']
corte = aprovados[['NOME_CURSO', 'SIGLA_IES', 'MUNICIPIO_CAMPUS', 'TURNO', 'TIPO_MOD_CONCORRENCIA', 'NOTA_CORTE']]
corte = corte[corte.TIPO_MOD_CONCORRENCIA.str.contains('|'.join(['A', 'B']))].sort_values(by='NOTA_CORTE', ascending=False)
141/8:
# UFPE ampla concorrência e bonificação

corte_ufpe = corte[corte.SIGLA_IES == 'UFPE']
corte_ufpe = corte_ufpe.drop_duplicates().reset_index(drop=True)
corte_ufpe.head(6)
141/9:
most_corte_ufpe = corte_ufpe.head(6)
sns.barplot(data=most_corte_ufpe, x="NOME_CURSO", y="NOTA_CORTE", hue="MUNICIPIO_CAMPUS")
plt.xticks(rotation=45)
plt.yticks(range(0, 950, 50))
plt.ylabel('Nota de corte')
plt.xlabel('')
plt.show()
141/10:
aprovados_ufpe = aprovados[aprovados.SIGLA_IES == 'UFPE']
med_ufpe = aprovados_ufpe[aprovados_ufpe.NOME_CURSO == 'MEDICINA']
med_ufpe.TIPO_MOD_CONCORRENCIA.value_counts()
141/11:
def situacao_por_estado(aprovados_curso):
    fig, ax = plt.subplots(ncols=2, figsize=(8, 5))
    fig.tight_layout()
    
    count = aprovados_curso.UF_CANDIDATO.value_counts().reset_index()
    count.columns = ['UF', 'COUNT']
    sns.barplot(data=count, x="COUNT", y="UF", ax=ax[0], palette="rocket")

    estados = gpd.read_file('../datasets/bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')
    estados = estados.rename({'sigla': 'UF'}, axis=1)
    estados = estados.merge(count, on='UF', how='left')
    estados.COUNT = estados.COUNT.fillna(0)
    estados.plot(column='COUNT', edgecolor='black', cmap='Reds', legend=True, ax=ax[1], legend_kwds={'shrink': 0.55}, figsize=(20, 20))
141/12:
aprovados_med = aprovados[aprovados.NOME_CURSO == 'MEDICINA']
situacao_por_estado(aprovados_med)

plt.title("Distribuição por estado dos aprovados\n em Medicina pelo SISU 2022")
plt.show()
141/13:
cursos_tech = ['ENGENHARIA DA COMPUTAÇÃO', 'CIÊNCIAS DA COMPUTAÇÃO', 'SISTEMAS DE INFORMAÇÃO']
aprovados_tech = aprovados[aprovados.NOME_CURSO.str.contains('|'.join(cursos_tech))]

situacao_por_estado(aprovados_tech)

plt.title("Distribuição por estado dos aprovados\n em cursos de computação pelo SISU 2022")
plt.show()
141/14:
notas = base.drop_duplicates(subset=['CPF', 'INSCRITO'])
notas.shape
141/15:
notas_uf = notas[['UF_CANDIDATO', 'NOTA_L', 'NOTA_CH', 'NOTA_CN', 'NOTA_M', 'NOTA_R']]
notas_uf.columns = ['UF', 'LING', 'HUM', 'NAT', 'MAT', 'RED']
notas_heatmap = notas_uf.pivot_table(columns='UF', aggfunc='mean')
141/16:
fig, ax = plt.subplots(figsize=(20,6))

sns.heatmap(notas_heatmap, annot=True, fmt='.1f', cmap='crest', linewidth=.5, linecolor='black', ax=ax)
plt.title("Nota média dos candidatos por UF e áreas do conhecimento")
plt.xlabel('')
plt.ylabel('')
plt.yticks(rotation=0)
plt.show()
141/17:
redacao_estado = base[['INSCRITO', 'UF_CANDIDATO', 'NOTA_R']]
redacao_estado = redacao_estado[redacao_estado.NOTA_R == 1000].drop_duplicates()

situacao_por_estado(redacao_estado)

plt.title("Distribuição por estado dos que\ntiraram nota 1000 na Redação")
plt.show()
141/18:
regioes = {'AC': 'NORTE', 'AL': 'NORDESTE', 'AP': 'NORTE', 'AM': 'NORTE', 'BA': 'NORDESTE', 'CE': 'NORDESTE', 'ES': 'SUDESTE',
           'GO': 'CENTRO-OESTE', 'MA': 'NORDESTE', 'MT': 'CENTRO-OESTE', 'MS': 'CENTRO-OESTE', 'MG': 'SUDESTE', 'PA': 'NORTE',
           'PB': 'NORDESTE', 'PR': 'SUL', 'PE': 'NORDESTE', 'PI': 'NORDESTE', 'RJ': 'SUDESTE', 'RN': 'NORDESTE', 'RS': 'SUL', 
           'RO': 'NORTE', 'RR': 'NORTE', 'SC': 'SUL', 'SP': 'SUDESTE', 'SE': 'NORDESTE', 'TO': 'NORTE', 'DF': 'CENTRO-OESTE'}
141/19: notas['REGIAO_CANDIDATO'] = notas.UF_CANDIDATO.map(lambda x: regioes[x])
141/20:
notas_regiao = notas[['REGIAO_CANDIDATO', 'NOTA_M', 'NOTA_CH', 'NOTA_R', 'NOTA_L', 'NOTA_CN']]
notas_regiao.shape
141/21: notas_ne_se = notas_regiao[notas_regiao.REGIAO_CANDIDATO.str.contains('|'.join(['NORDESTE', 'SUDESTE']))]
141/22:
sns.scatterplot(data=notas_ne_se, x='NOTA_M', y='NOTA_CH', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências humanas")
plt.xlabel("Nota em matemática")
plt.title("Distribuição de notas entre as regiões Nordeste e Sudeste")
plt.show()
141/23:
sns.scatterplot(data=notas_ne_se, x='NOTA_M', y='NOTA_CN', hue='REGIAO_CANDIDATO')

plt.legend(loc='lower left')
plt.ylabel("Nota em ciências da natureza")
plt.xlabel("Nota em matemática")
plt.title("Distribuição de notas entre as regiões Nordeste e Sudeste")
plt.show()
143/1:
import numpy as np
import matplotlib.pyplot as plt
144/1: import os
144/2: breed_list = os.listdir("./dataset/images/Images/")
144/3: num_breeds = len(breed_list)
144/4:
num_breeds = len(breed_list)
num_breeds
144/5:
num_breeds = len(breed_list)
print(num_breeds, "breeds")
144/6:
num_breeds = len(breeds)
print(num_breeds, "breeds")
144/7: breeds = os.listdir("./dataset/images/Images/")
144/8:
num_breeds = len(breeds)
print(num_breeds, "breeds")
144/9:
total_imgs = 0

for breed in breeds:
    total_imgs += len(os.listdir("./dataset/images/Images/" + breed))

print("Total of", total_imgs, "images")
144/10:
total_imgs = 0

for breed in breeds:
    print(breed)
    total_imgs += len(os.listdir("./dataset/images/Images/" + breed))

print("Total of", total_imgs, "images")
144/11:
total_imgs = 0

for breed in breeds:
    total_imgs += len(os.listdir("./dataset/images/Images/" + breed))

print("Total of", total_imgs, "images")
144/12:
total_imgs = 0

for breed in breeds:
    total_imgs += len(os.listdir("./dataset/images/Images/" + breed))

print("Total of", total_imgs, "images")
144/13:
import os
import numpy as np
import PIL
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
144/14:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)
data_dir
144/15:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)
144/16: total_imgs = len(od.listdir(data_dir))
144/17: total_imgs = len(os.listdir(data_dir))
144/18: print("Total of", total_imgs, "images.")
144/19:
total_imgs = 0

for flower in flower_list:
    total_imgs += len(os.listdir(data_dir + "/" + flower))
total_imgs = len(os.listdir(data_dir))
144/20:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

flower_list = os.listdir(data_dir)
144/21:
total_imgs = 0

for flower in flower_list:
    total_imgs += len(os.listdir(data_dir + "/" + flower))
total_imgs = len(os.listdir(data_dir))
144/22:
total_imgs = 0

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
total_imgs = len(os.listdir(data_dir))
144/23:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

data_dir
144/24:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

print(os.listdir(data_dir))
144/25:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

os.remove('LICENSE.txt')
print(os.listdir(data_dir))
144/26:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

os.remove(str(data_dir) + '/LICENSE.txt')
print(os.listdir(data_dir))
144/27:
total_imgs = 0

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
total_imgs = len(os.listdir(data_dir))
144/28:
total_imgs = 0

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
total_imgs = len(os.listdir(data_dir))
144/29:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

os.remove(str(data_dir) + '/LICENSE.txt')
print(os.listdir(data_dir))
144/30:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

os.remove(str(data_dir) + '/LICENSE.txt')
print(os.listdir(data_dir))
144/31:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

print(os.listdir(data_dir))
144/32:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

print(os.listdir(data_dir))
144/33:
total_imgs = 0

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
total_imgs = len(os.listdir(data_dir))
144/34:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

print(os.listdir(data_dir))
144/35:
total_imgs = 0

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
total_imgs = len(os.listdir(data_dir))
144/36:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

print(os.listdir(data_dir))
144/37: print(os.listdir(data_dir))
144/38:
total_imgs = 0
flower_list = os.listdir(data_dir)
144/39:
total_imgs = 0
flower_list = os.listdir(data_dir)

for flower in flower_list:
    print(flower)
144/40:
total_imgs = 0
flower_list = os.listdir(data_dir)

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
144/41:
total_imgs = 0
flower_list = os.listdir(data_dir)

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
144/42: print("Total of", total_imgs, "images.")
144/43: ax, fig = plt.subplots(cols=5)
144/44: ax, fig = plt.subplots(columns=5)
144/45: ax, fig = plt.subplots(col=5)
144/46: fig, ax = plt.subplots(col=5)
144/47: fig, ax = plt.subplots(ncols=5)
144/48:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = os.listdir(str(data_dir) + "/" + flower_list[i])
    ax[i] = plt.imread(str(data_dir) + "/" + flower_list[i] + "/" + ith_flowers[0])

plt.show()
144/49:
roses = list(data_dir.glob('roses/*'))
PIL.Image.open(str(roses[0]))
144/50:
roses = list(data_dir.glob('roses/*'))
PIL.Image.open(str(roses[0]))
roses = list(data_dir.glob('daisy/*'))
PIL.Image.open(str(roses[0]))
roses = list(data_dir.glob('dandalion/*'))
PIL.Image.open(str(roses[0]))
144/51:
roses = list(data_dir.glob('roses/*'))
PIL.Image.open(str(roses[0]))
roses = list(data_dir.glob('daisy/*'))
PIL.Image.open(str(roses[0]))
roses = list(data_dir.glob('dandelion/*'))
PIL.Image.open(str(roses[0]))
144/52:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    print(str(data_dir) + "/" + flower_list[i])
    ith_flowers = os.listdir(str(data_dir) + "/" + flower_list[i])
    ax[i] = plt.imread(str(data_dir) + "/" + flower_list[i] + "/" + ith_flowers[0])

plt.show()
144/53:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = os.listdir(str(data_dir) + "/" + flower_list[i])
    print(ith_flowers)
    ax[i] = plt.imread(str(data_dir) + "/" + flower_list[i] + "/" + ith_flowers[0])

plt.show()
144/54:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = os.listdir(str(data_dir) + "/" + flower_list[i])
    ax[i] = plt.imread(str(data_dir) + "/" + flower_list[i] + "/" + ith_flowers[0])
    str(data_dir) + "/" + flower_list[i] + "/" + ith_flowers[0]
    
plt.show()
144/55:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = os.listdir(str(data_dir) + "/" + flower_list[i])
    ax[i] = plt.imread(str(data_dir) + "/" + flower_list[i] + "/" + ith_flowers[0])
    print(str(data_dir) + "/" + flower_list[i] + "/" + ith_flowers[0])
    
plt.show()
144/56:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = os.listdir(str(data_dir) + "/" + flower_list[i])
    ax[i] = plt.imshow(str(data_dir) + "/" + flower_list[i] + "/" + ith_flowers[0])
    print(str(data_dir) + "/" + flower_list[i] + "/" + ith_flowers[0])
    
plt.show()
144/57:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    roses = list(data_dir.glob('roses/*'))
    PIL.Image.open(str(roses[0]))
    
plt.show()
144/58:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    roses = list(data_dir.glob('roses/*'))
    PIL.Image.open(str(roses[0]))
144/59:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    PIL.Image.open(str(ith_flowers[0]))
    
plt.show()
144/60:
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    PIL.Image.open(str(ith_flowers[0]))
144/61:


for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    PIL.Image.open(str(ith_flowers[0]))
144/62:
examples = []

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    
examples
144/63:
examples = []

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    
print(examples)
144/64:
examples = []

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    
print(examples[i])
144/65:
examples = []

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    
print(examples[0].show)
144/66:
examples = []

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    
examples[0].show()
144/67:
examples = []

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    
display(examples[0])
144/68:
examples = []

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    display(examples[i])
144/69:
examples = []

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    display(examples[i])
    plt.imshow(np.asarray(examples[i]))
144/70:
examples = []

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    plt.imshow(np.asarray(examples[i]))
144/71:
examples = []
ax, fig = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    examples.append(PIL.Image.open(str(ith_flowers[0])))
    ax[i] = plt.imshow(np.asarray(examples[i]))
144/72:
examples = []
ax, fig = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i] = plt.imshow(np.asarray(eg_ith_flower))
144/73:
examples = []
ax, fig = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
144/74:
examples = []
fig, ax = plt.subplots(ncols=5)

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
144/75:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(8,8))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
144/76:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(8,8))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))

plt.xticks("")
144/77:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(8,8))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))

plt.xticks([])
144/78:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(8,8))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
    ax[i].xticks([])
144/79:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(8,8))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
144/80:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(10,10))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
144/81:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(16,16))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
144/82:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(16,16))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
    ax[i].sex_xticks([])
144/83:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(16,16))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
    ax[i].set_xticks([])
144/84:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(16,16))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
    ax[i].set_xticks([])
    ax[i].set_yticks([])
144/85:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(16,16))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
    ax[i].set_xticks([])
    ax[i].set_yticks([])
    ax[i].title.set_title(flower_list[i])
144/86:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(16,16))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
    ax[i].set_xticks([])
    ax[i].set_yticks([])
    ax[i].title.set_text(flower_list[i])
144/87:
batch_size = 32
img_height = 180
img_width = 180
144/88:
train = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
144/89:
val = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
144/90:
class_names = train.class_names
print(class_names)
144/91:
plt.figure(figsize=(10, 10))
for images, labels in train.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
144/92:
plt.figure(figsize=(10, 10))
for images, labels in train.take(1):
    for i in range(9):
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
144/93:
plt.figure(figsize=(10, 10))
for images, labels in train.take(1):
    for i in range(12):
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
144/94: img_batch, label_batch = train.take(1)
144/95: train.take(1)
144/96:
for image_batch, labels_batch in train_ds:
    print(image_batch.shape)
    print(labels_batch.shape)
    break
144/97:
for image_batch, labels_batch in train:
    print(image_batch.shape)
    print(labels_batch.shape)
    break
144/98:
# avoiding I/O deadlock

AUTOTUNE = tf.data.AUTOTUNE

train = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val = val.cache().prefetch(buffer_size=AUTOTUNE)
144/99: normalization_layer = layers.Rescaling(1./255)
144/100:
normalized = train.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized))
first_image = image_batch[0]
print(np.min(first_image),  np.max(first_image))
144/101:
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)
144/102:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
144/103:
import os
import numpy as np
import PIL
import tensorflow as tf
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
144/104:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
144/105:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
144/106:
import os
import numpy as np
import PIL
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
144/107:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        #ax = plt.subplot(4, 4, i + 1)
        #plt.imshow(augmented_images[0].numpy().astype("uint8"))
        #plt.axis("off")
plt.show()
144/108:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        #augmented_images = data_augmentation(images)
        #ax = plt.subplot(4, 4, i + 1)
        #plt.imshow(augmented_images[0].numpy().astype("uint8"))
        #plt.axis("off")
plt.show()
144/109:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        #augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        #plt.imshow(augmented_images[0].numpy().astype("uint8"))
        #plt.axis("off")
plt.show()
144/110:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        #plt.imshow(augmented_images[0].numpy().astype("uint8"))
        #plt.axis("off")
plt.show()
144/111:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = augment_imgs(images)
        ax = plt.subplot(4, 4, i + 1)
        #plt.imshow(augmented_images[0].numpy().astype("uint8"))
        #plt.axis("off")
plt.show()
144/112:
def augment_imgs(inputs):
    inputs = tf.keras.layers.RandomZoom(height_factor=(-0.20, 0))(inputs)
    inputs = tf.keras.layers.RandomFlip()(inputs)
    inputs = tf.keras.layers.RandomRotation(factor=0.1)(inputs)
    inputs = tf.keras.layers.RandomTranslation(height_factor=0.1,
                                               width_factor=0.1)(inputs)
    return inputs
144/113:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = augment_imgs(images)
        ax = plt.subplot(4, 4, i + 1)
        #plt.imshow(augmented_images[0].numpy().astype("uint8"))
        #plt.axis("off")
plt.show()
145/1:
import os
import numpy as np
import PIL
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
145/2:
import os
import numpy as np
import PIL
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
145/3:
import os
import numpy as np
import PIL
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
145/4:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

print(os.listdir(data_dir))
145/5:
total_imgs = 0
flower_list = os.listdir(data_dir)

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
145/6: print("Total of", total_imgs, "images.")
145/7:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(16,16))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
    ax[i].set_xticks([])
    ax[i].set_yticks([])
    ax[i].title.set_text(flower_list[i])
145/8:
batch_size = 32
img_height = 180
img_width = 180
145/9:
train = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
145/10:
val = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
145/11:
class_names = train.class_names
print(class_names)
145/12:
plt.figure(figsize=(10, 10))
for images, labels in train.take(1):
    for i in range(12):
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
145/13:
for image_batch, labels_batch in train:
    print(image_batch.shape)
    print(labels_batch.shape)
    break
145/14:
# avoiding I/O deadlock

AUTOTUNE = tf.data.AUTOTUNE

train = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val = val.cache().prefetch(buffer_size=AUTOTUNE)
145/15:
def augment_imgs(inputs):
    inputs = tf.keras.layers.RandomZoom(height_factor=(-0.20, 0))(inputs)
    inputs = tf.keras.layers.RandomFlip()(inputs)
    inputs = tf.keras.layers.RandomRotation(factor=0.1)(inputs)
    inputs = tf.keras.layers.RandomTranslation(height_factor=0.1,
                                               width_factor=0.1)(inputs)
    return inputs
145/16:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = augment_imgs(images)
        ax = plt.subplot(4, 4, i + 1)
        #plt.imshow(augmented_images[0].numpy().astype("uint8"))
        #plt.axis("off")
plt.show()
145/17:
num_classes = len(class_names)

model = Sequential([
  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])
145/18:
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
145/19:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = augment_imgs(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/20:
def augmentation(inputs):
    inputs = tf.keras.layers.RandomZoom(height_factor=(-0.20, 0))(inputs)
    inputs = tf.keras.layers.RandomFlip()(inputs)
    inputs = tf.keras.layers.RandomRotation(factor=0.1)(inputs)
    inputs = tf.keras.layers.RandomTranslation(height_factor=0.1,
                                               width_factor=0.1)(inputs)
    return inputs
145/21:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/22:
num_classes = len(class_names)

model = Sequential([
    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
])
145/23:
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
145/24: model.summary()
145/25:
epochs = 15

history = model.fit(
  augmentation(train),
  validation_data=val,
  epochs=epochs
)
145/26:
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)
145/27:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/28:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/29:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/30:
#epochs = 15

#history = model.fit(
#  augmentation(train),
#  validation_data=val,
#  epochs=epochs
#)
145/31:
import os
import numpy as np
import PIL
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
145/32:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

print(os.listdir(data_dir))
145/33:
total_imgs = 0
flower_list = os.listdir(data_dir)

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
145/34: print("Total of", total_imgs, "images.")
145/35:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(16,16))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
    ax[i].set_xticks([])
    ax[i].set_yticks([])
    ax[i].title.set_text(flower_list[i])
145/36:
batch_size = 32
img_height = 180
img_width = 180
145/37:
train = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
145/38:
val = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
145/39:
class_names = train.class_names
print(class_names)
145/40:
plt.figure(figsize=(10, 10))
for images, labels in train.take(1):
    for i in range(12):
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
145/41:
for image_batch, labels_batch in train:
    print(image_batch.shape)
    print(labels_batch.shape)
    break
145/42:
# avoiding I/O deadlock

AUTOTUNE = tf.data.AUTOTUNE

train = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val = val.cache().prefetch(buffer_size=AUTOTUNE)
145/43:
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)
145/44:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/45:
num_classes = len(class_names)

model = Sequential([
    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
])
145/46:
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
145/47: model.summary()
145/48:
#epochs = 15

#history = model.fit(
#  augmentation(train),
#  validation_data=val,
#  epochs=epochs
#)
145/49:
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.5),
    layers.RandomZoom(0.1),
  ]
)
145/50:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/51:
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("vertical",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.5),
    layers.RandomZoom(0.1),
  ]
)
145/52:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/53:
def augmentation(inputs):
    inputs = tf.keras.layers.RandomZoom(height_factor=(-0.20, 0))(inputs)
    inputs = tf.keras.layers.RandomFlip()(inputs)
    inputs = tf.keras.layers.RandomRotation(factor=0.1)(inputs)
    inputs = tf.keras.layers.RandomTranslation(height_factor=0.1,
                                               width_factor=0.1)(inputs)
    return inputs
145/54:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/55:
def augmentation(inputs):
    inputs = tf.keras.layers.RandomZoom(height_factor=(-0.20, 0))(inputs)
    inputs = tf.keras.layers.RandomFlip()(inputs)
    inputs = tf.keras.layers.RandomRotation(factor=0.1)(inputs)
    inputs = tf.keras.layers.RandomTranslation(height_factor=0.1,
                                               width_factor=0.1)(inputs)
    return inputs
145/56:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/57:
data_augmentation = tf.keras.Sequential([
  layers.RandomFlip("horizontal_and_vertical"),
  layers.RandomRotation(0.2),
])
145/58:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/59:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images, training=True)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/60:
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)
145/61:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images, training=True)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/62:
num_classes = len(class_names)

model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
])
145/63:
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
145/64: model.summary()
145/65:
epochs = 15

history = model.fit(
  augmentation(train),
  validation_data=val,
  epochs=epochs
)
145/66:
epochs = 15

history = model.fit(
  train,
  validation_data=val,
  epochs=epochs
)
145/67:
epochs = 15

history = model.fit(
  train,
  validation_data=val,
  epochs=epochs
)
145/68:
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
145/69:
import os
import numpy as np
import PIL
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
145/70:
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

print(os.listdir(data_dir))
145/71:
total_imgs = 0
flower_list = os.listdir(data_dir)

for flower in flower_list:
    total_imgs += len(os.listdir(str(data_dir) + "/" + flower))
145/72: print("Total of", total_imgs, "images.")
145/73:
examples = []
fig, ax = plt.subplots(ncols=5, figsize=(16,16))

for i in range(5):
    ith_flowers = list(data_dir.glob(flower_list[i] + "/*"))
    eg_ith_flower = PIL.Image.open(str(ith_flowers[0]))
    ax[i].imshow(np.asarray(eg_ith_flower))
    ax[i].set_xticks([])
    ax[i].set_yticks([])
    ax[i].title.set_text(flower_list[i])
145/74:
batch_size = 32
img_height = 180
img_width = 180
145/75:
train = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
145/76:
val = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
145/77:
class_names = train.class_names
print(class_names)
145/78:
plt.figure(figsize=(10, 10))
for images, labels in train.take(1):
    for i in range(12):
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
145/79:
for image_batch, labels_batch in train:
    print(image_batch.shape)
    print(labels_batch.shape)
    break
145/80:
# avoiding I/O deadlock

AUTOTUNE = tf.data.AUTOTUNE

train = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val = val.cache().prefetch(buffer_size=AUTOTUNE)
145/81:
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)
145/82:
plt.figure(figsize=(10, 10))
for images, _ in train.take(1):
    for i in range(12):
        augmented_images = data_augmentation(images, training=True)
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
plt.show()
145/83:
num_classes = len(class_names)

model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
])
145/84:
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
145/85: model.summary()
145/86:
epochs = 15

history = model.fit(
  train,
  validation_data=val,
  epochs=epochs
)
145/87:
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
145/88:
sunflower_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/89:
sunflower_url = "https://www.homeit.com.br/wp-content/uploads/2018/11/tulipa-1-768x768.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/90:
sunflower_url = "https://www.homeit.com.br/wp-content/uploads/2018/11/tulipa-1-768x768.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/91:
sunflower_url = "https://www.homeit.com.br/wp-content/uploads/2018/11/tulipa-1-768x768.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/92:
sunflower_url = "https://www.homeit.com.br/wp-content/uploads/2018/11/tulipa-1-768x768.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/93:
sunflower_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/94:
sunflower_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/95:
sunflower_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/96:
sunflower_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])
print(predictions)
print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/97:
sunflower_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
sunflower_path = tf.keras.utils.get_file('tulipa', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])
print(predictions)
print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/98:
sunflower_url = "https://www.homeit.com.br/wp-content/uploads/2018/11/tulipa-1-768x768.jpg"
sunflower_path = tf.keras.utils.get_file('tulipa', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])
print(predictions)
print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/99:
import os
import numpy as np
import PIL
import tensorflow as tf
import matplotlib.pyplot as plt
import requests
from io import BytesIO

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
145/100:
url = "https://www.homeit.com.br/wp-content/uploads/2018/11/tulipa-1-768x768.jpg"

response = requests.get(url)
img = Image.open(BytesIO(response.content))
145/101:
url = "https://www.homeit.com.br/wp-content/uploads/2018/11/tulipa-1-768x768.jpg"

response = requests.get(url)
img = PIL.Image.open(BytesIO(response.content))
145/102: display(img)
145/103: array = np.asarray(img)
145/104: array
145/105: array = tf.expand_dims(array, 0)
145/106: array
145/107: predictions = model.predict(img_array)
145/108: print(predictions)
145/109: score = tf.nn.softmax(predictions[0])
145/110:
print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/111:
score = tf.nn.softmax(predictions[0])
score
145/112: print(class_names)
145/113:
score = tf.nn.softmax(predictions)
score
145/114:
score = tf.nn.softmax(predictions[1])
score
145/115:
score = tf.nn.softmax(predictions[0])
score
145/116:
sunflower_url = "https://www.homeit.com.br/wp-content/uploads/2018/11/tulipa-1-768x768.jpg"
sunflower_path = tf.keras.utils.get_file('tulipa', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])
print(predictions)
print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
145/117:
sunflower_url = "https://www.homeit.com.br/wp-content/uploads/2018/11/tulipa-1-768x768.jpg"
sunflower_path = tf.keras.utils.get_file('tulipa', origin=sunflower_url)

img = tf.keras.utils.load_img(
    sunflower_path, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])
print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)
146/1:
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import OrdinalEncoder
import numpy as np
146/2: base = pd.read_csv("../dataset/observations.csv")
146/3: base.shape
146/4: base.head()
146/5:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
146/6:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pevlvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis slope', 'class']
146/7:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['class_int'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
146/8: base[['class', 'class_int']]
146/9: base.describe()
146/10:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
146/11: base[['class', 'class_int']]
146/12: base.describe()
146/13:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
146/14: base[['class', 'TARGET']]
146/15: base.describe()
146/16: base.head()
146/17:
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import OrdinalEncoder
import numpy as np
146/18: base = pd.read_csv("../dataset/observations.csv")
146/19: base.shape
146/20: base.head()
146/21:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
146/22:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pevlvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis slope', 'class']
146/23:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
146/24: base.head()
146/25:
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import OrdinalEncoder
import numpy as np
146/26: base = pd.read_csv("../dataset/observations.csv")
146/27: base.shape
146/28: base.head()
146/29:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
146/30:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis slope', 'class']
146/31:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
146/32: base.head()
146/33:
pelvic_incidence_tilt = base[['pelvic_incidence', 'pelvic_tilt']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_incidence", y="pelvic_tilt", hue="class")
146/34:
pelvic_incidence_tilt = base[['pelvic_incidence', 'pelvic_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_incidence", y="pelvic_tilt", hue="class")
146/35:
pelvic_incidence_tilt = base[['pelvic_incidence', 'pelvic_slope', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_incidence", y="pelvic_slope", hue="class")
146/36:
pelvic_incidence_tilt = base[['pelvic_tilt', 'pelvic_slope', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="pelvic_slope", hue="class")
146/37:
pelvic_incidence_tilt = base[['pelvic_tilt', 'cervical_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="cervical_tilt", hue="class")
146/38:
pelvic_incidence_tilt = base[['pelvic_tilt', 'pelvic_radius', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="pelvic_radius", hue="class")
146/39: pelvic_tilt_corr = base.pelvic_tilt.corr(base.TARGET)
146/40: pelvic_tilt_corr
146/41: pelvic_tilt_corr = base.TARGET.corr(base.pelvic_tilt)
146/42: pelvic_tilt_corr
146/43:
x = base.pelvic_tilt
y = base.TARGET

pelvic_tilt_corr = base.TARGET.corr(base.pelvic_tilt)
146/44:
x = base.pelvic_tilt
y = base.TARGET

pelvic_tilt_corr = y.corr(x)
146/45: pelvic_tilt_corr
146/46:
plt.scatter(x, y) 
plt.plot(np.poly1d(np.polyfit(x, y, 1)), color='red')
146/47:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder
import numpy as np
146/48:
x = base.pelvic_tilt
y = base.TARGET

pelvic_tilt_corr = y.corr(x)
146/49: pelvic_tilt_corr
146/50:
plt.scatter(x, y) 
plt.plot(np.poly1d(np.polyfit(x, y, 1)), color='red')
146/51:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder
import numpy as np
146/52:
x = base.pelvic_tilt
y = base.TARGET

pelvic_tilt_corr = y.corr(x)
146/53: pelvic_tilt_corr
146/54:
plt.scatter(x, y) 
plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1)) 
         (np.unique(x)), color='red')
147/1:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
import numpy as np
147/2:
scaler = MinMaxScaler()
scaled_base = scaler.fit_transform(base)
147/3:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
import numpy as np
147/4: base = pd.read_csv("../dataset/observations.csv")
147/5: base.shape
147/6: base.head()
147/7:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
147/8:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis slope', 'class']
147/9:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
147/10: base.head()
147/11:
scaler = MinMaxScaler()
scaled_base = scaler.fit_transform(base)
147/12:
scaler = MinMaxScaler()
scaled_base = scaler.fit_transform(base[base.column != 'class'])
147/13:
scaler = MinMaxScaler()
scaled_base = scaler.fit_transform(base[base.columns != 'class'])
147/14:
scaler = MinMaxScaler()
scaled_base = scaler.fit_transform()
base[base.columns != 'class']
147/15:
scaler = MinMaxScaler()
#scaled_base = scaler.fit_transform()
base[base.columns != 'class']
147/16:
scaler = MinMaxScaler()
#scaled_base = scaler.fit_transform()
base.loc[:, base.columns != 'class']
147/17:
scaler = MinMaxScaler()
scaled_base = scaler.fit_transform(base.loc[:, base.columns != 'class'])
147/18:
scaler = MinMaxScaler()
scaled_base = scaler.fit_transform(base.loc[:, base.columns != 'class'])
base.loc[:, base.columns != 'class'] = scaled_base
147/19:
scaler = MinMaxScaler()
scaled_base = scaler.fit_transform(base.loc[:, base.columns != 'class'])
base.loc[:, base.columns != 'class'] = scaled_base
base
147/20:
pelvic_incidence_tilt = base[['pelvic_incidence', 'pelvic_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_incidence", y="pelvic_tilt", hue="class")
147/21:
pelvic_incidence_tilt = base[['pelvic_tilt', 'pelvic_radius', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="pelvic_radius", hue="class")
147/22:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
import numpy as np
147/23: base = pd.read_csv("../dataset/observations.csv")
147/24: base.shape
147/25: base.head()
147/26:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
147/27:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis slope', 'class']
147/28:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
147/29: base.head()
147/30:
pelvic_incidence_tilt = base[['pelvic_incidence', 'pelvic_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_incidence", y="pelvic_tilt", hue="class")
147/31:
pelvic_incidence_tilt = base[['pelvic_tilt', 'pelvic_radius', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="pelvic_radius", hue="class")
147/32:
pelvic_incidence_tilt = base[['pelvic_tilt', 'direct_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="direct_tilt", hue="class")
147/33:
pelvic_incidence_tilt = base[['pelvic_tilt', 'thoracic_slope', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="thoracic_slope", hue="class")
147/34:
pelvic_incidence_tilt = base[['pelvic_tilt', 'cervical_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="cervical_tilt", hue="class")
147/35:
pelvic_incidence_tilt = base[['pelvic_tilt', 'sacrum_angle', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="sacrum_angle", hue="class")
147/36:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
import numpy as np
147/37: base = pd.read_csv("../dataset/observations.csv")
147/38: base.shape
147/39: base.head()
147/40:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
147/41:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
147/42:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
147/43: base.head()
147/44:
pelvic_incidence_tilt = base[['pelvic_incidence', 'pelvic_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_incidence", y="pelvic_tilt", hue="class")
147/45:
pelvic_incidence_tilt = base[['pelvic_tilt', 'pelvic_radius', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="pelvic_radius", hue="class")
147/46:
pelvic_incidence_tilt = base[['pelvic_tilt', 'sacrum_angle', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="sacrum_angle", hue="class")
147/47:
pelvic_incidence_tilt = base[['scoliosis_slope', 'sacrum_angle', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="scoliosis_slope", y="sacrum_angle", hue="class")
147/48:
pelvic_incidence_tilt = base[['lumbar_lordosis_angle', 'sacrum_angle', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="lumbar_lordosis_angle", y="sacrum_angle", hue="class")
147/49:
pelvic_incidence_tilt = base[['scoliosis_slope', 'lumbar_lordosis_angle', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="scoliosis_slope", y="lumbar_lordosis_angle", hue="class")
147/50:
pelvic_incidence_tilt = base[['degree_spondylolisthesis', 'lumbar_lordosis_angle', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="degree_spondylolisthesis", y="lumbar_lordosis_angle", hue="class")
147/51:
pelvic_incidence_tilt = base[['degree_spondylolisthesis', 'thoracic_slope', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="degree_spondylolisthesis", y="thoracic_slope", hue="class")
147/52:
pelvic_incidence_tilt = base[['degree_spondylolisthesis', 'sacral_slope', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="degree_spondylolisthesis", y="sacral_slope", hue="class")
147/53:
pelvic_incidence_tilt = base[['degree_spondylolisthesis', 'sacral_slope', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="degree_spondylolisthesis", y="sacral_slope", hue="class")
147/54:
pelvic_incidence_tilt = base[['degree_spondylolisthesis', 'pelvic_slope', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="degree_spondylolisthesis", y="pelvic_slope", hue="class")
147/55:
pelvic_incidence_tilt = base[['degree_spondylolisthesis', 'scoliosis_slope', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="degree_spondylolisthesis", y="scoliosis_slope", hue="class")
147/56:
pelvic_incidence_tilt = base[['degree_spondylolisthesis', 'pelvic_incidence', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="degree_spondylolisthesis", y="pelvic_incidence", hue="class")
147/57:


pelvic_incidence_tilt = base[['lumbar_lordosis_angle', 'pelvic_incidence', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="lumbar_lordosis_angle", y="pelvic_incidence", hue="class")
147/58:
pelvic_incidence_tilt = base[['lumbar_lordosis_angle', 'pelvic_incidence', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="lumbar_lordosis_angle", y="pelvic_incidence", hue="class")
147/59:
pelvic_incidence_tilt = base[['lumbar_lordosis_angle', 'sacrum_angle', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="lumbar_lordosis_angle", y="sacrum_angle", hue="class")
147/60:
pelvic_incidence_tilt = base[['lumbar_lordosis_angle', 'direct_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="lumbar_lordosis_angle", y="direct_tilt", hue="class")
147/61:
pelvic_incidence_tilt = base[['cervical_tilt', 'direct_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="cervical_tilt", y="direct_tilt", hue="class")
147/62:
pelvic_incidence_tilt = base[['cervical_tilt', 'degree_spondylolisthesis', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="cervical_tilt", y="degree_spondylolisthesis", hue="class")
147/63:
pelvic_incidence_tilt = base[['degree_spondylolisthesis', 'direct_tilt', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="degree_spondylolisthesis", y="direct_tilt", hue="class")
147/64:
pelvic_incidence_tilt = base[['pelvic_incidence', 'pelvic_radius', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_incidence", y="pelvic_radius", hue="class")
147/65:
pelvic_incidence_tilt = base[['pelvic_tilt', 'pelvic_radius', 'class']]

sns.scatterplot(data=pelvic_incidence_tilt, x="pelvic_tilt", y="pelvic_radius", hue="class")
147/66:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])

pairs
147/67:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])

len(pairs)
147/68:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])

pairs
147/69:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
147/70: fig, ax = plt.subplots(nrows=11, ncols=6, figsize(6,6))
147/71: fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(6,6))
147/72: fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(8,8))
147/73:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(8,8))

for pair in pairs:
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class")

plt.show()
147/74:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(8,8))

i=0
for pair in pairs:
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i])
    i += 1

plt.show()
147/75:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(8,8))

i=0
for pair in pairs:
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[0][0])
    i += 1

plt.show()
147/76:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(8,8))

i=0, j=0
for pair in pairs:
    if j == 5:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[0][0])
    j += 1

plt.show()
147/77:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(8,8))

i, j = 0
for pair in pairs:
    if j == 5:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[0][0])
    j += 1

plt.show()
147/78:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(8,8))

i=0
j=0
for pair in pairs:
    if j == 5:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/79:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(8,8))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/80:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(12,12))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/81:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(20,20))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/82:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(30,30))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/83:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/84:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
import numpy as np
147/85: base = base.drop(columns=['class'])
147/86:
base = base.drop(columns=['class'])
base
147/87:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
import numpy as np
147/88: base = pd.read_csv("../dataset/observations.csv")
147/89: base.shape
147/90: base.head()
147/91:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
147/92:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
147/93:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
147/94: base.head()
147/95:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
147/96:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/97:
base = base.drop(columns=['class'])
base
147/98: base = base.drop(columns=['class'])
147/99:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
import numpy as np
147/100: base = pd.read_csv("../dataset/observations.csv")
147/101: base.shape
147/102: base.head()
147/103:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
147/104:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
147/105:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
147/106: base.head()
147/107:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
147/108:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/109: base = base.drop(columns=['class'])
147/110:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.25, random_state=42)
147/111:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
147/112: X_train
147/113:
X_train
y_train
147/114:
scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)
147/115: X_test
147/116: X_train
147/117:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
147/118: X_train
147/119: X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
147/120:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_train
147/121:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
147/122: X_train
147/123: X_train.shape
147/124: X_test.shape
147/125:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
147/126: model = MLPClassifier(hidden_layer_sizes=np.array([5]))
147/127: model
147/128: model.n_layers
147/129: model.n_layers_
147/130: model.n_layers_
147/131: model = MLPClassifier(hidden_layer_sizes=np.array([5]))
147/132: model.n_layers_
147/133: model.classes_
147/134: model.n_outputs_
147/135: model
147/136: model.describe()
147/137: model.get_params
147/138: model.fit(X_train, y_train)
147/139: model.predict(X_test)
147/140: model = MLPClassifier(hidden_layer_sizes=np.array([5]), activation='tanh')
147/141: model.fit(X_train, y_train)
147/142: model = MLPClassifier(hidden_layer_sizes=np.array([5]), activation='tanh', max_iter=300)
147/143: model.fit(X_train, y_train)
147/144: model = MLPClassifier(hidden_layer_sizes=np.array([5]), activation='tanh', max_iter=400)
147/145: model.fit(X_train, y_train)
147/146: model = MLPClassifier(hidden_layer_sizes=np.array([5]), activation='tanh', max_iter=500)
147/147: model.fit(X_train, y_train)
147/148: model = MLPClassifier(hidden_layer_sizes=np.array([5]), activation='tanh', max_iter=1000)
147/149: model.fit(X_train, y_train)
147/150: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5]), activation='tanh', max_iter=1000)
147/151: model.fit(X_train, y_train)
147/152: model.predict(X_test)
147/153: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5]), activation='tanh', max_iter=300)
147/154: model.fit(X_train, y_train)
147/155: model.predict(X_test)
147/156: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=300)
147/157: model.fit(X_train, y_train)
147/158: model.predict(X_test)
147/159: model.score(X_test, y_test)
147/160: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=300, solver='lbfgs')
147/161: model.fit(X_train, y_train)
147/162: model.predict(X_test)
147/163: model.score(X_test, y_test)
147/164: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=400, solver='lbfgs')
147/165: model.fit(X_train, y_train)
147/166: model.predict(X_test)
147/167: model.score(X_test, y_test)
147/168: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/169: model.fit(X_train, y_train)
147/170: model.predict(X_test)
147/171: model.score(X_test, y_test)
147/172: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=100, solver='lbfgs')
147/173: model.fit(X_train, y_train)
147/174: model.predict(X_test)
147/175: model.score(X_test, y_test)
147/176: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=100, solver='lbfgs', random_state=0)
147/177: model.fit(X_train, y_train)
147/178: model.predict(X_test)
147/179: model.score(X_test, y_test)
147/180: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=100, random_state=0)
147/181: model.fit(X_train, y_train)
147/182: model.predict(X_test)
147/183: model.score(X_test, y_test)
147/184: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=100)
147/185: model.fit(X_train, y_train)
147/186: model.predict(X_test)
147/187: model.score(X_test, y_test)
147/188: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=100, solver='lbfgs')
147/189: model.fit(X_train, y_train)
147/190: model.predict(X_test)
147/191: model.score(X_test, y_test)
147/192: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/193: model.fit(X_train, y_train)
147/194: model.predict(X_test)
147/195: model.score(X_test, y_test)
147/196: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=100, solver='lbfgs')
147/197: model.fit(X_train, y_train)
147/198: model.predict(X_test)
147/199: model.score(X_test, y_test)
147/200: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/201: model.fit(X_train, y_train)
147/202: model.predict(X_test)
147/203: model.score(X_test, y_test)
147/204: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/205: model.fit(X_train, y_train)
147/206: model.predict(X_test)
147/207: model.score(X_test, y_test)
147/208: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/209: model.fit(X_train, y_train)
147/210: model.predict(X_test)
147/211: model.score(X_test, y_test)
147/212: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/213: model.fit(X_train, y_train)
147/214: model.predict(X_test)
147/215: model.score(X_test, y_test)
147/216: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/217: model.fit(X_train, y_train)
147/218: model.predict(X_test)
147/219: model.score(X_test, y_test)
147/220: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/221: model.fit(X_train, y_train)
147/222: model.predict(X_test)
147/223: model.score(X_test, y_test)
147/224: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/225: model.fit(X_train, y_train)
147/226: model.predict(X_test)
147/227: model.score(X_test, y_test)
147/228: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/229: model.fit(X_train, y_train)
147/230: model.predict(X_test)
147/231: model.score(X_test, y_test)
147/232: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=200, solver='lbfgs')
147/233: model.fit(X_train, y_train)
147/234: model.predict(X_test)
147/235: model.score(X_test, y_test)
147/236: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=300, solver='lbfgs')
147/237: model.fit(X_train, y_train)
147/238: model.predict(X_test)
147/239: model.score(X_test, y_test)
147/240: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=300, solver='lbfgs')
147/241: model.fit(X_train, y_train)
147/242: model.predict(X_test)
147/243: model.score(X_test, y_test)
147/244: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=300, solver='lbfgs')
147/245: model.fit(X_train, y_train)
147/246: model.predict(X_test)
147/247: model.score(X_test, y_test)
147/248: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=300, solver='lbfgs')
147/249: model.fit(X_train, y_train)
147/250: model.predict(X_test)
147/251: model.score(X_test, y_test)
147/252: model = MLPClassifier(hidden_layer_sizes=np.array([5, 5, 5]), activation='tanh', max_iter=300, solver='lbfgs')
147/253: model.fit(X_train, y_train)
147/254: model.predict(X_test)
147/255: model.score(X_test, y_test)
147/256: model = MLPClassifier(hidden_layer_sizes=np.array([16, 8, 4]), activation='tanh', max_iter=300, solver='lbfgs')
147/257: model.fit(X_train, y_train)
147/258: model.predict(X_test)
147/259: model.score(X_test, y_test)
147/260: model.score(X_train, y_train)
147/261:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
147/262: base = pd.read_csv("../dataset/observations.csv")
147/263: base.shape
147/264: base.head()
147/265:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
147/266:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
147/267:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
147/268: base.head()
147/269:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
147/270:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/271: base = base.drop(columns=['class'])
147/272:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
147/273:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
147/274:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
147/275: X_train.shape
147/276: X_test.shape
147/277: model = MLPClassifier(hidden_layer_sizes=np.array([16, 8, 4]), activation='tanh', max_iter=300, solver='lbfgs')
147/278: model.fit(X_train, y_train)
147/279: model.predict(X_test)
147/280: model.score(X_test, y_test)
147/281: model.score(X_train, y_train)
147/282: model = MLPClassifier(hidden_layer_sizes=np.array([5]), activation='tanh', max_iter=300, solver='lbfgs')
147/283: model.fit(X_train, y_train)
147/284:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
147/285:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
147/286:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
147/287: X_train.shape
147/288: X_test.shape
147/289: model = MLPClassifier(hidden_layer_sizes=np.array([5]), activation='tanh', max_iter=300, solver='lbfgs')
147/290: model.fit(X_train, y_train)
147/291: model.predict(X_test)
147/292: model.score(X_test, y_test)
147/293:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
147/294: base = pd.read_csv("../dataset/observations.csv")
147/295: base.shape
147/296: base.head()
147/297:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
147/298:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
147/299:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
147/300: base.head()
147/301:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
147/302:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/303: base = base.drop(columns=['class'])
147/304:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
147/305:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
147/306:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
147/307: X_train.shape
147/308: X_test.shape
147/309: model = MLPClassifier(hidden_layer_sizes=np.array([16, 8, 4]), activation='tanh', max_iter=300, solver='lbfgs')
147/310: model.fit(X_train, y_train)
147/311: model.predict(X_test)
147/312: model.score(X_test, y_test)
147/313: model.score(X_train, y_train)
147/314: model = MLPClassifier(hidden_layer_sizes=np.array([8, 8, 4]), activation='tanh', max_iter=300, solver='lbfgs')
147/315: model.fit(X_train, y_train)
147/316: model = MLPClassifier(hidden_layer_sizes=np.array([16, 8, 4]), activation='tanh', max_iter=300, solver='lbfgs')
147/317: model.fit(X_train, y_train)
147/318:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
147/319: base = pd.read_csv("../dataset/observations.csv")
147/320: base.shape
147/321: base.head()
147/322:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
147/323:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
147/324:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
147/325: base.head()
147/326:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
147/327:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
147/328: base = base.drop(columns=['class'])
147/329:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
147/330:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
147/331:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
147/332: X_train.shape
147/333: X_test.shape
147/334: model = MLPClassifier(hidden_layer_sizes=np.array([5]), activation='tanh', max_iter=300, solver='lbfgs')
147/335: model.fit(X_train, y_train)
147/336: model.predict(X_test)
147/337: model.score(X_test, y_test)
147/338: model.score(X_train, y_train)
149/1: import numpy as np
149/2: import numpy as np
149/3:
import numpy as np
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from keras.applications import resnet50
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/4:
import numpy as np
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from keras.applications import resnet50
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/5:
import numpy as np
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from keras.applications.resnet50 import preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/6:
import numpy as np
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/7:
import numpy as np
from tensorflow.keras.preprocessing.image import image
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/8:
import numpy as np
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/9:
import numpy as np
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/10:
import numpy as np
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/11: img = image.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/12:
import numpy as np
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/13: img = image.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/14:
import numpy as np
import pillow
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/15:
import numpy as np
import Pillow
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/16:
import numpy as np
from Pillow import Image
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/17:
import numpy as np
from Pillow import Image
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/18:
import numpy as np
from PIL import Image
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/19: img = image.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/20: img = image.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/21:
import numpy as np
from PIL import Image
from keras.preprocessing.image import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/22: img = image.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/23: img = Image.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/24: img = image.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/25:
import numpy as np
from PIL import Image
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/26: img = image.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/27:
import numpy as np
from PIL import Image
from tensorflow.keras.preprocessing import Iterator
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/28:
import numpy as np
from PIL import Image
from tensorflow.keras.preprocessing import Iterator
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/29: img = image.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/30:
import numpy as np
from PIL import Image
from keras.preprocessing import image?
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/31:
import numpy as np
from PIL import Image
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/32: img = tf.keras.utils.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/33: img = tensorflow.keras.utils.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/34:
import numpy as np
from PIL import Image
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/35: img = tf.keras.utils.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
149/36:
import numpy as np
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
149/37: img = tf.keras.utils.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
150/1:
import numpy as np
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
150/2: img = tf.keras.utils.load_img('dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
150/3: img = tf.keras.utils.load_img('../dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
150/4: img = image.load_img('../dataset/Images/n02099601-golden_retriever.jpg', target_size=(224, 224))
150/5: img = image.load_img('../dataset/Images/n02099601-golden_retriever/n02099601_67.jpg', target_size=(224, 224))
150/6: plt.imshow(img)
150/7:
img = image.img_to_array(img)
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
150/8: img
150/9: model = ResNet50(weights=’imagenet’)
150/10: model = ResNet50(weights='imagenet')
150/11: pred = model.predict(img)
150/12: print('Predicted:', decode_predictions(pred, top=1)[0])
150/13: img = image.load_img('../dataset/Images/n02106550-Rottweiler/n02106550_4962.jpg', target_size=(224, 224))
150/14: plt.imshow(img)
150/15:
img = image.img_to_array(img)
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
150/16: model = ResNet50(weights='imagenet')
150/17: pred = model.predict(img)
150/18: print('Predicted:', decode_predictions(pred, top=1)[0])
150/19:
img = image.load_img('../dataset/Images/n02106550-Rottweiler/n02106550_4962.jpg', target_size=(224, 224))
plt.imshow()
io.show()
150/20:
img = image.load_img('../dataset/Images/n02106550-Rottweiler/n02106550_4962.jpg', target_size=(224, 224))
plt.imshow()
150/21:
img = image.load_img('../dataset/Images/n02106550-Rottweiler/n02106550_4962.jpg', target_size=(224, 224))
plt.imshow(img)
150/22:

#Dimension of the original image
rows = img.shape[0]
cols = img.shape[1]
150/23: from sklearn.cluster import KMeans
150/24: from sklearn.cluster import KMeans
150/25:
img = image.load_img('../dataset/Images/n02106550-Rottweiler/n02106550_4962.jpg', target_size=(224, 224))
plt.imshow(img)
150/26: img = img.reshape(224*224, 3)
150/27:
img = image.img_to_array(img)
img = img.reshape(224*224, 3)
150/28: img
150/29: img.shape
150/30:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=64)
kmeans.fit(img)
150/31:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(rows, cols, 3)
150/32:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/33: plt.imshow(compressed_image)
150/34:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=16)
kmeans.fit(img)
150/35:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/36: plt.imshow(compressed_image)
150/37:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=8)
kmeans.fit(img)
150/38:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/39: plt.imshow(compressed_image)
150/40:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(img)
150/41:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/42: plt.imshow(compressed_image)
150/43: compressed_pred = model.predict(compressed_image)
150/44: compressed_image.shape
150/45:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/46: compressed_pred = model.predict(compressed_image)
150/47: print('Predicted:', decode_predictions(pred, top=1)[0])
150/48:
import numpy as np
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
150/49: img = image.load_img('../dataset/Images/n02106550-Rottweiler/n02106550_4962.jpg', target_size=(224, 224))
150/50: plt.imshow(img)
150/51:
img = image.img_to_array(img)
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
150/52: model = ResNet50(weights='imagenet')
150/53: pred = model.predict(img)
150/54: print('Predicted:', decode_predictions(pred, top=1)[0])
150/55: from sklearn.cluster import KMeans
150/56:
img = image.load_img('../dataset/Images/n02106550-Rottweiler/n02106550_4962.jpg', target_size=(224, 224))
plt.imshow(img)
150/57:
img = image.img_to_array(img)
img = img.reshape(224*224, 3)
150/58: img.shape
150/59:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=2)
kmeans.fit(img)
150/60:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/61: plt.imshow(compressed_image)
150/62: compressed_image.shape
150/63:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/64: compressed_pred = model.predict(compressed_image)
150/65: print('Predicted:', decode_predictions(pred, top=1)[0])
150/66: img = image.load_img('../dataset/Images/n02088364-beagle/n02088364_852.jpg', target_size=(224, 224))
150/67:
import numpy as np
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
150/68: img = image.load_img('../dataset/Images/n02088364-beagle/n02088364_852.jpg', target_size=(224, 224))
150/69: plt.imshow(img)
150/70:
img = image.img_to_array(img)
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
150/71: model = ResNet50(weights='imagenet')
150/72: pred = model.predict(img)
150/73: print('Predicted:', decode_predictions(pred, top=1)[0])
150/74: from sklearn.cluster import KMeans
150/75:
img = image.load_img('../dataset/Images/n02088364-beagle/n02088364_852.jpg', target_size=(224, 224))
plt.imshow(img)
150/76:
img = image.img_to_array(img)
img = img.reshape(224*224, 3)
150/77: img.shape
150/78:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=2)
kmeans.fit(img)
150/79:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/80: plt.imshow(compressed_image)
150/81: compressed_image.shape
150/82:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/83: compressed_pred = model.predict(compressed_image)
150/84: print('Predicted:', decode_predictions(pred, top=1)[0])
150/85:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(img)
150/86:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/87: plt.imshow(compressed_image)
150/88: compressed_image.shape
150/89:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/90: compressed_pred = model.predict(compressed_image)
150/91: print('Predicted:', decode_predictions(pred, top=1)[0])
150/92:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=8)
kmeans.fit(img)
150/93:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/94: plt.imshow(compressed_image)
150/95: compressed_image.shape
150/96:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/97: compressed_pred = model.predict(compressed_image)
150/98: print('Predicted:', decode_predictions(pred, top=1)[0])
150/99:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(img)
150/100:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/101: plt.imshow(compressed_image)
150/102: compressed_image.shape
150/103:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/104: compressed_pred = model.predict(compressed_image)
150/105: print('Predicted:', decode_predictions(pred, top=1)[0])
150/106:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=2)
kmeans.fit(img)
150/107:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/108: plt.imshow(compressed_image)
150/109: compressed_image.shape
150/110:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/111: compressed_pred = model.predict(compressed_image)
150/112: print('Predicted:', decode_predictions(pred, top=1)[0])
150/113:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=1)
kmeans.fit(img)
150/114:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/115: plt.imshow(compressed_image)
150/116: compressed_image.shape
150/117:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/118: compressed_pred = model.predict(compressed_image)
150/119: print('Predicted:', decode_predictions(pred, top=1)[0])
150/120: print('Predicted:', decode_predictions(compressed_pred, top=1)[0])
150/121:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=2)
kmeans.fit(img)
150/122:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/123: plt.imshow(compressed_image)
150/124: compressed_image.shape
150/125:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/126: compressed_pred = model.predict(compressed_image)
150/127: print('Predicted:', decode_predictions(compressed_pred, top=1)[0])
150/128:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(img)
150/129:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/130: plt.imshow(compressed_image)
150/131: compressed_image.shape
150/132:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/133: compressed_pred = model.predict(compressed_image)
150/134: print('Predicted:', decode_predictions(compressed_pred, top=1)[0])
150/135:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=8)
kmeans.fit(img)
150/136:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/137: plt.imshow(compressed_image)
150/138: compressed_image.shape
150/139:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/140: compressed_pred = model.predict(compressed_image)
150/141: print('Predicted:', decode_predictions(compressed_pred, top=1)[0])
150/142:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(img)
150/143:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/144: plt.imshow(compressed_image)
150/145: compressed_image.shape
150/146:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/147: compressed_pred = model.predict(compressed_image)
150/148: print('Predicted:', decode_predictions(compressed_pred, top=1)[0])
150/149:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=2)
kmeans.fit(img)
150/150:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/151: plt.imshow(compressed_image)
150/152: compressed_image.shape
150/153:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/154: compressed_pred = model.predict(compressed_image)
150/155: print('Predicted:', decode_predictions(compressed_pred, top=1)[0])
150/156:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(img)
150/157:
compressed_image = kmeans.cluster_centers_[kmeans.labels_]
compressed_image = np.clip(compressed_image.astype('uint8'), 0, 255)

#Reshape the image to original dimension
compressed_image = compressed_image.reshape(224, 224, 3)
150/158: plt.imshow(compressed_image)
150/159: compressed_image.shape
150/160:
compressed_image = image.img_to_array(compressed_image)
compressed_image = np.expand_dims(compressed_image, axis=0)
compressed_image = preprocess_input(compressed_image)
150/161: compressed_pred = model.predict(compressed_image)
150/162: print('Predicted:', decode_predictions(compressed_pred, top=1)[0])
150/163:
import numpy as np
import os
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
150/164: os.dir()
150/165: os.currdir()
150/166: os.getcdw()
150/167: os.getcwd()
150/168:
import numpy as np
import os
import pathlib
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
150/169:
import numpy as np
import os
from pathlib import Path
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
150/170: data_dir = Path('../dataset/Images')
150/171:
data_dir = Path('../dataset/Images')
data_dir
150/172:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
150/173: total_breeds
150/174:
for breed in total_breeds:
    print(breed)
150/175:
for breed in total_breeds:
    print(breed[10:])
150/176:
for breed in total_breeds:
    os.rename(breed, breed[10:])
150/177:
data_dir = Path('../dataset/Images')
os.chdir(r"../dataset/Images")
total_breeds = os.listdir(data_dir)
os.getcwd()
150/178:
data_dir = Path(os.getcwd() + '../dataset/Images')
os.chdir(r"../dataset/Images")
total_breeds = os.listdir(data_dir)
150/179:
data_dir = Path(os.getcwd() + '../dataset/Images')
os.chdir(data_dir)
total_breeds = os.listdir(data_dir)
150/180:
data_dir = Path(os.getcwd() + '../dataset/Images')
print(data_dir)
os.chdir(data_dir)
total_breeds = os.listdir(data_dir)
150/181:
data_dir = Path(os.getcwd() + '../dataset/Images')
print(data_dir)
#os.chdir(data_dir)
total_breeds = os.listdir(data_dir)
150/182:
data_dir = Path(os.getcwd() + '../dataset/Images')
print(data_dir)
#os.chdir(data_dir)
#total_breeds = os.listdir(data_dir)
150/183:
data_dir = Path(os.getcwd())
print(data_dir)
#os.chdir(data_dir)
#total_breeds = os.listdir(data_dir)
150/184:
data_dir = Path(os.getcwd())
print(data_dir)
#os.chdir(data_dir)
#total_breeds = os.listdir(data_dir)
150/185:
data_dir = Path(os.getcwd())

total_breeds = os.listdir(data_dir)
150/186:
for breed in total_breeds:
    print(breed)
150/187:
for breed in total_breeds:
    os.rename(breed, breed[10:])
150/188:
total_breeds = os.listdir(data_dir)
total_breeds
150/189:
data_dir = Path(os.getcwd())

total_breeds = os.listdir(data_dir)
150/190:
data_dir = Path(os.getcwd())

total_breeds = os.listdir(data_dir)
total_breeds
150/191:
data_dir = Path(os.getcwd())
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
150/192: os.getcwd()
150/193:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/1:
import numpy as np
import os
from pathlib import Path
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
151/2:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/3: os.getcwd()
151/4: img = image.load_img('../dataset/Images/n02088364-beagle/n02088364_852.jpg', target_size=(224, 224))
151/5: data_dir
151/6:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    print()
151/7:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    print(breed_dir)
151/8:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    print(os.listdir(breed_dir))
151/9:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images' + breed + '/' + breed_img)
        print(breed_img_dir)
151/10:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(breed_img_dir)
151/11:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
151/12:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
151/13:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/14:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))

print(original)
151/15:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))

print(len(original))
151/16:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))

print(len(original.keys()))
151/17:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/18: plt.imshow(original['beagle'][0])
151/19: plt.imshow(original['beagle'][1])
151/20: plt.imshow(original['beagle'][7])
151/21:
for key in original:
    print(len(original[key]))
151/22:
for key in original:
    print('Total of ', key, ' images: ', len(original[key]))
151/23:
for key in original:
    print('Total of', key, ' images: ', len(original[key]))
151/24:
for key in original:
    print('Total of', key, ' images:', len(original[key]))
151/25:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/26:
fig, ax = plt.subplots(ncols=4)

ax[0].imshow(original['beagle'][7])
151/27:
fig, ax = plt.subplots(ncols=4)

for i in range(4):
    ax[i].imshow(original[breeds[i]][10])
151/28:
fig, ax = plt.subplots(ncols=4, figsize=(10,10))

for i in range(4):
    ax[i].imshow(original[breeds[i]][10])
151/29:
fig, ax = plt.subplots(ncols=4, figsize=(20,20))

for i in range(4):
    ax[i].imshow(original[breeds[i]][10])
151/30:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].imshow(original[breeds[i]][10])
151/31:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].title('a')
    ax[i].imshow(original[breeds[i]][10])
151/32:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title('a')
    ax[i].imshow(original[breeds[i]][10])
151/33:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/34:
img = image.img_to_array(original[breeds[0]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/35: model = ResNet50(weights='imagenet')
151/36: pred = model.predict(img)
151/37: print('Predicted:', decode_predictions(pred, top=1)[0])
151/38:
img = image.img_to_array(original[breeds[1]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/39: model = ResNet50(weights='imagenet')
151/40: pred = model.predict(img)
151/41: print('Predicted:', decode_predictions(pred, top=1)[0])
151/42:
img = image.img_to_array(original[breeds[2]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/43: pred = model.predict(img)
151/44: print('Predicted:', decode_predictions(pred, top=1)[0])
151/45:
img = image.img_to_array(original[breeds[3]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/46: pred = model.predict(img)
151/47: print('Predicted:', decode_predictions(pred, top=1)[0])
151/48: print('Predicted:', decode_predictions(pred, top=5)[0])
151/49:
img = image.img_to_array(original[breeds[0]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/50: pred = model.predict(img)
151/51: print('Predicted:', decode_predictions(pred, top=5)[0])
151/52: print('Predicted:', decode_predictions(pred, top=10)[0])
151/53:
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decode_predictions[0]
151/54:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
151/55:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
print(yhat)
151/56:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
expected_score = decoded_pred[(_, 'breeds[0]', _)]
151/57:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
expected_score = decoded_pred[(_, breeds[0], _)]
151/58:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
expected_score = decoded_pred.index((_, breeds[0], _))
151/59:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
expected_score = [item for item in decoded_pred if item[1] == breeds[0]]
151/60:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
expected_score = [item for item in decoded_pred if item[1] == breeds[0]]
expected_score
151/61:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
expected_score = item for item in decoded_pred if item[1] == breeds[0]
expected_score
151/62:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
expected_score = [item for item in decoded_pred if item[1] == breeds[0]]
expected_score
151/63:
decoded_pred = decode_predictions(pred, top=10)[0]
print('Predicted:', decode_predictions(pred, top=10)[0])
yhat = decoded_pred[0]
expected_score = [item for item in decoded_pred if item[1] == breeds[0]][0]
expected_score
151/64:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
expected_score = [item for item in decoded_pred if item[1] == breeds[0]][0]
expected_score
151/65:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]][0]
print('expected class: ', y)
print('predicted class: ', yhat)
151/66:
img = image.img_to_array(original[breeds[1]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/67: pred = model.predict(img)
151/68:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[1]][0]
print('expected class: ', y)
print('predicted class: ', yhat)
151/69:
def pre_process_images():
    for breed in original:
        for img in original[breed]:
            img = image.img_to_array(original[breeds[1]][10])
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
151/70:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/71: data_dir
151/72:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/73:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/74:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/75: model = ResNet50(weights='imagenet')
151/76:
def pre_process_images():
    for breed in original:
        for img in original[breed]:
            img = image.img_to_array(original[breeds[1]][10])
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
151/77:
pre_process_images()
original
151/78:
def pre_process_images():
    for breed in original:
        for img in original[breed]:
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
151/79:
pre_process_images()
original
151/80:
def pre_process_images():
    for breed in original:
        for img in original[breed]:
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            print(img)
151/81:
pre_process_images()
original
151/82:
def pre_process_images():
    for breed in original:
        for img, i in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            print(i)
151/83:
pre_process_images()
original
151/84:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            print(i)
151/85:
pre_process_images()
original
151/86:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/87:
pre_process_images()
original
151/88: pre_process_images()
151/89:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/90: data_dir
151/91:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/92:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/93:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/94: model = ResNet50(weights='imagenet')
151/95:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/96: pre_process_images()
151/97: original
151/98: pred = model.predict(original[breeds[0]])
151/99:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        predictions_original[breed].append(model.predict(img))

predictions_original
151/100:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        predictions_original[breed].append(decode_predictions(pred, top=1)[0])

predictions_original
151/101:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/102:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=20)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/103:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=30)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/104:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=30)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/105:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=40)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/106:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=50)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/107:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/108:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/109:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/110: data_dir
151/111:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/112:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/113:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/114: model = ResNet50(weights='imagenet')
151/115:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/116: pre_process_images()
151/117:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/118:
img = image.img_to_array(original[breeds[0]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/119:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/120: data_dir
151/121:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/122:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/123:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/124:
img = image.img_to_array(original[breeds[0]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/125: pred = model.predict(img)
151/126:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[1]][0]
print('expected class: ', y)
print('predicted class: ', yhat)
151/127:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[1]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/128:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/129:
img = image.img_to_array(original[breeds[0]][12])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/130: pred = model.predict(img)
151/131:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/132:
img = image.img_to_array(original[breeds[0]][16])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/133: pred = model.predict(img)
151/134:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/135:
img = image.img_to_array(original[breeds[0]][19])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/136: pred = model.predict(img)
151/137:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/138:
img = image.img_to_array(original[breeds[0]][20])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
151/139: pred = model.predict(img)
151/140:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/141:
img = image.img_to_array(original[breeds[0]][20])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0][20]])
151/142:
img = image.img_to_array(original[breeds[0]][20])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/143:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/144: data_dir
151/145:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i, breed_img in enumerate(breed_imgs):
        print(i, '../dataset/Images/' + breed + '/' + breed_img)
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/146:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/147: data_dir
151/148:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i, breed_img in enumerate(breed_imgs):
        print(i, '../dataset/Images/' + breed + '/' + breed_img)
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/149:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/150: data_dir
151/151:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/152:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/153:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/154:
img = image.img_to_array(original[breeds[0]][20])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/155: pred = model.predict(img)
151/156:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/157:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/158: pre_process_images()
151/159:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/160:
img = image.img_to_array(original[breeds[0]][80])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/161:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/162: data_dir
151/163:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/164:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/165:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/166:
img = image.img_to_array(original[breeds[0]][80])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/167: pred = model.predict(img)
151/168:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/169:
img = image.img_to_array(original[breeds[0]][100])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/170: pred = model.predict(img)
151/171:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/172:
img = image.img_to_array(original[breeds[0]][92])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/173: pred = model.predict(img)
151/174:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/175:
img = image.img_to_array(original[breeds[0]][100])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/176: pred = model.predict(img)
151/177:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/178:
img = image.img_to_array(original[breeds[0]][95])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/179: pred = model.predict(img)
151/180:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/181:
img = image.img_to_array(original[breeds[0]][96])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/182: pred = model.predict(img)
151/183:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/184:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/185:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/186: data_dir
151/187:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/188:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/189:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/190:
img = image.img_to_array(original[breeds[0]][96])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/191: pred = model.predict(img)
151/192:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/193:
img = image.img_to_array(original[breeds[0]][150])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/194: pred = model.predict(img)
151/195:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/196:
img = image.img_to_array(original[breeds[0]][170])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/197: pred = model.predict(img)
151/198:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/199:
img = image.img_to_array(original[breeds[0]][171])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/200: pred = model.predict(img)
151/201:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/202:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/203: data_dir
151/204:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/205:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/206: data_dir
151/207:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/208:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/209:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/210:
img = image.img_to_array(original[breeds[0]][171])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/211: pred = model.predict(img)
151/212:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/213:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/214: pre_process_images()
151/215:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/216:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/217:
img = image.img_to_array(original[breeds[2]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/218:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/219: data_dir
151/220:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/221:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/222:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/223:
img = image.img_to_array(original[breeds[2]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/224: pred = model.predict(img)
151/225:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[0]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/226:
img = image.img_to_array(original[breeds[2]][10])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/227: pred = model.predict(img)
151/228:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/229:
img = image.img_to_array(original[breeds[2]][12])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/230: pred = model.predict(img)
151/231:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/232:
img = image.img_to_array(original[breeds[2]][30])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/233: pred = model.predict(img)
151/234:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/235:
img = image.img_to_array(original[breeds[2]][40])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/236: pred = model.predict(img)
151/237:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/238:
img = image.img_to_array(original[breeds[2]][39])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/239: pred = model.predict(img)
151/240:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/241:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/242: data_dir
151/243:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/244:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/245:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/246:
img = image.img_to_array(original[breeds[2]][39])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/247: pred = model.predict(img)
151/248:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/249:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/250: pre_process_images()
151/251:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/252:
img = image.img_to_array(original[breeds[2]][20])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/253:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/254: data_dir
151/255:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/256:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/257:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/258:
img = image.img_to_array(original[breeds[2]][20])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/259: pred = model.predict(img)
151/260:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/261:
img = image.img_to_array(original[breeds[2]][35])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/262: pred = model.predict(img)
151/263:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/264:
img = image.img_to_array(original[breeds[2]][38])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/265: pred = model.predict(img)
151/266:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/267:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/268: data_dir
151/269:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/270:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/271:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/272:
img = image.img_to_array(original[breeds[2]][38])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/273: pred = model.predict(img)
151/274:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/275:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/276: pre_process_images()
151/277:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/278:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/279: data_dir
151/280:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/281:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/282:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/283:
img = image.img_to_array(original[breeds[2]][38])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/284:
img = image.img_to_array(original[breeds[2]][120])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/285: pred = model.predict(img)
151/286:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/287:
img = image.img_to_array(original[breeds[2]][100])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/288: pred = model.predict(img)
151/289:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/290:
img = image.img_to_array(original[breeds[2]][102])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/291: pred = model.predict(img)
151/292:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/293:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/294: data_dir
151/295:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/296:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/297:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/298:
img = image.img_to_array(original[breeds[2]][102])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/299: pred = model.predict(img)
151/300:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/301:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/302: pre_process_images()
151/303:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/304:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/305:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/306:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/307: data_dir
151/308:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/309:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/310:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/311:
img = image.img_to_array(original[breeds[2]][102])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/312:
img = image.img_to_array(original[breeds[3]][140])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/313: pred = model.predict(img)
151/314:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[2]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/315:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[3]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/316:
img = image.img_to_array(original[breeds[3]][141])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/317: pred = model.predict(img)
151/318:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[3]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/319:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/320: data_dir
151/321:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/322:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/323:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/324:
img = image.img_to_array(original[breeds[3]][141])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/325: pred = model.predict(img)
151/326:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[3]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/327:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/328: pre_process_images()
151/329:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/330: pre_process_images()
151/331:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/332: data_dir
151/333:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for i,breed_img in enumerate(breed_imgs):
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        print(i, breed_img_dir)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/334:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/335:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/336:
img = image.img_to_array(original[breeds[3]][141])
img = np.expand_dims(img, axis=0)
img = preprocess_input(img)
print(original[breeds[0]][20])
151/337: pred = model.predict(img)
151/338:
decoded_pred = decode_predictions(pred, top=10)[0]
yhat = decoded_pred[0]
y = [item for item in decoded_pred if item[1] == breeds[3]]
print('expected class: ', y)
print('predicted class: ', yhat)
151/339:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/340: pre_process_images()
151/341:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})

predictions_original
151/342:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/343:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/344:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/345:
import numpy as np
import os
from pathlib import Path
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
151/346:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/347: data_dir
151/348:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/349:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/350:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/351: model = ResNet50(weights='imagenet')
151/352:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/353: pre_process_images()
151/354:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})
151/355: df = pd.DataFrame()
151/356:
import numpy as np
import os
from pathlib import Path
import pandas as pd
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
151/357:
import numpy as np
import os
from pathlib import Path
import pandas as pd
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
151/358: df = pd.DataFrame()
151/359:
df = pd.DataFrame()
df['true_class_score'] = predictions_original[breeds[0]]['y']
151/360:
df = pd.DataFrame()
df['true_class_score'] = predictions_original[breeds[0]]['y'][3]
151/361:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]
predictions_original[breeds[0]]['y']
151/362:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]
predictions_original[breeds[0]]
151/363:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]
df['true_class_score'] = predictions_original[breeds[0]]
predictions_original[breeds[0]]
151/364:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]
df['true_class_score'] = predictions_original[breeds[0]]
df
151/365:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]

true_class_scores = [item['y'][2] for item in predictions_origial]

df['true_class_score'] = predictions_original[breeds[0]]
df
151/366:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]

true_class_scores = [item['y'][2] for item in predictions_original]

df['true_class_score'] = predictions_original[breeds[0]]
df
151/367:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]

true_class_scores = [item['y'] for item in predictions_original]

df['true_class_score'] = predictions_original[breeds[0]]
df
151/368:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]

true_class_scores = [item[0] for item in predictions_original]

df['true_class_score'] = predictions_original[breeds[0]]
df
151/369:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]

true_class_scores = [item[0] for item in predictions_original]
true_class_scores

#df['true_class_score'] = predictions_original[breeds[0]]
#df
151/370:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]

true_class_scores = [item[0] for item in predictions_original[breeds[0]]]
true_class_scores

#df['true_class_score'] = predictions_original[breeds[0]]
#df
151/371:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]

true_class_scores = [item['y'] for item in predictions_original[breeds[0]]]
true_class_scores

#df['true_class_score'] = predictions_original[breeds[0]]
#df
151/372:
#df = pd.DataFrame()
#df['true_class_score'] = predictions_original[breeds[0]]['y'][3]

true_class_scores = [item['y'][2] for item in predictions_original[breeds[0]]]
true_class_scores

#df['true_class_score'] = predictions_original[breeds[0]]
#df
151/373:
df = pd.DataFrame()
df['true_class_score'] = [item['y'][2] for item in predictions_original[breeds[0]]]
151/374:
df = pd.DataFrame()
df['true_class_score'] = [item['y'][2] for item in predictions_original[breeds[0]]]
df
151/375:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/376: data_dir
151/377:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/378:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/379:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/380: data_dir
151/381:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/382:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/383:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/384: data_dir
151/385:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/386:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/387:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/388: data_dir
151/389:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/390:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/391:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/392: data_dir
151/393:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/394:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/395:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/396:
def pre_process_images():
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original[breed][i] = img
151/397: pre_process_images()
151/398:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})
151/399:
df = pd.DataFrame()
df['true_class_score'] = [item['y'][2] for item in predictions_original[breeds[0]]]
df
151/400:
df = pd.DataFrame()
df['true_class_score'] = [item['y'][2] for item in predictions_original[breeds[0]]]
df['class'] = 'beagle'
df
151/401:
df = pd.DataFrame()
df['true_class'] = 'beagle'
df['true_class_score'] = [item['y'][2] for item in predictions_original[breeds[0]]]
df['predicted_class'] = [item['yhat'][1] for item in predictions_original[breeds[0]]]
df['predicted_class_score'] = [item['yhat'][2] for item in predictions_original[breeds[0]]]
df
151/402:
df = pd.DataFrame()
df['true_class_score'] = [item['y'][2] for item in predictions_original[breeds[0]]]
df['true_class'] = 'beagle'
df['predicted_class_score'] = [item['yhat'][2] for item in predictions_original[breeds[0]]]
df['predicted_class'] = [item['yhat'][1] for item in predictions_original[breeds[0]]]
df
151/403:
df = pd.DataFrame()
df['true_class_score'] = [item['y'][2] for item in predictions_original[breeds[0]]]
df['true_class'] = 'beagle'
df['predicted_class_score'] = [item['yhat'][2] for item in predictions_original[breeds[0]]]
df['predicted_class'] = [item['yhat'][1] for item in predictions_original[breeds[0]]]
df.predicted_class.value_counts()
151/404:
df = pd.DataFrame()
df['true_class_score'] = [item['y'][2] for item in predictions_original[breeds[0]]]
df['true_class'] = 'beagle'
df['predicted_class_score'] = [item['yhat'][2] for item in predictions_original[breeds[0]]]
df['predicted_class'] = [item['yhat'][1] for item in predictions_original[breeds[0]]]
df
151/405:
df = pd.DataFrame()
df['true_class_score'] = [item['y'][2] for item in predictions_original[breeds[0]]]
df['true_class'] = 'beagle'
df['predicted_class_score'] = [item['yhat'][2] for item in predictions_original[breeds[0]]]
df['predicted_class'] = [item['yhat'][1] for item in predictions_original[breeds[0]]]
151/406: [1,2,3] + 2
151/407: [1,2,3] + [2]
151/408:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]
    true_class += [breed]
    predicted_class_score = [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class = [item['yhat'][1] for item in predictions_original[breed]]
    
true_class
151/409:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed]
    predicted_class_score = [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class = [item['yhat'][1] for item in predictions_original[breed]]
    
true_class
151/410:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += ['breed' for i in range(147)]
    predicted_class_score = [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class = [item['yhat'][1] for item in predictions_original[breed]]
    
true_class
151/411:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score = [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class = [item['yhat'][1] for item in predictions_original[breed]]
    
true_class
151/412:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score = [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class = [item['yhat'][1] for item in predictions_original[breed]]
    
predicted_class_score
151/413:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score = [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class = [item['yhat'][1] for item in predictions_original[breed]]
    
predicted_class
151/414:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score += [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class += [item['yhat'][1] for item in predictions_original[breed]]
    
predicted_class
151/415:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score += [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class += [item['yhat'][1] for item in predictions_original[breed]]
    
original_df = pd.DataFrame()
original_df['true_class_score'] = true_class_score
original_df['true_class'] = true_class
original_df['predicted_class_score'] = predicted_class_score
original_df['predicted_class'] = predicted_class
original_df
151/416:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score += [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class += [item['yhat'][1] for item in predictions_original[breed]]
    
original_df = pd.DataFrame()
original_df['true_class_score'] = true_class_score
original_df['true_class'] = true_class
original_df['predicted_class_score'] = predicted_class_score
original_df['predicted_class'] = predicted_class
original_df.shape
151/417:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score += [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class += [item['yhat'][1] for item in predictions_original[breed]]
    
original_df = pd.DataFrame()
original_df['true_class_score'] = true_class_score
original_df['true_class'] = true_class
original_df['predicted_class_score'] = predicted_class_score
original_df['predicted_class'] = predicted_class
original_df.shape[0] / 4
151/418:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score += [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class += [item['yhat'][1] for item in predictions_original[breed]]
    
original_df = pd.DataFrame()
original_df['true_class_score'] = true_class_score
original_df['true_class'] = true_class
original_df['predicted_class_score'] = predicted_class_score
original_df['predicted_class'] = predicted_class
original_df.shape
151/419:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score += [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class += [item['yhat'][1] for item in predictions_original[breed]]
    
original_df = pd.DataFrame()
original_df['true_class_score'] = true_class_score
original_df['true_class'] = true_class
original_df['predicted_class_score'] = predicted_class_score
original_df['predicted_class'] = predicted_class
original_df.true_class.value_counts()
151/420:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score += [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class += [item['yhat'][1] for item in predictions_original[breed]]
    
original_df = pd.DataFrame()
original_df['true_class_score'] = true_class_score
original_df['true_class'] = true_class
original_df['predicted_class_score'] = predicted_class_score
original_df['predicted_class'] = predicted_class
original_df
151/421: sns.lineplot(data=original_df, x=original_df.index, y="true_class_score", hue="true_class")
151/422:
import numpy as np
import os
from pathlib import Path
import pandas as pd
import seaborn as sns
import PIL
import tensorflow as tf
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
151/423: sns.lineplot(data=original_df, x=original_df.index, y="true_class_score", hue="true_class")
151/424: sns.histplot(data=original_df, x=original_df.index, y="true_class_score", hue="true_class")
151/425: original_df.hist()
151/426:
original_beagles = original_df[original_df.true_class == 'beagle']
original_beagles.hist()
151/427:
original_beagles = original_df[original_df.true_class == 'beagle']
original_beagles.bar()
151/428:
original_beagles = original_df[original_df.true_class == 'beagle']
original_beagles.barplot()
151/429:
original_beagles = original_df[original_df.true_class == 'beagle']
original_beagles.plot()
151/430:
original_beagles = original_df[original_df.true_class == 'beagle']
original_beagles.hist()
151/431:
original_beagles = original_df[original_df.true_class == 'beagle']
original_beagles.true_class_score.hist()
151/432: sns.lineplot(data=original_beagles, x=original_beagles.index, y="true_class_score")
151/433: sns.histplot(data=original_beagles, x=original_beagles.index, y="true_class_score")
151/434: sns.histplot(data=original_beagles, x="true_class_score", bins=30)
151/435: sns.histplot(data=original_beagles, x="true_class_score", bins=40)
151/436: sns.histplot(data=original_beagles, x="true_class_score", bins=35)
151/437: sns.histplot(data=original_beagles, x="true_class_score", bins=30)
151/438: sns.histplot(data=original_beagles, x="true_class_score", bins=50)
151/439: sns.histplot(data=original_beagles, x="true_class_score", bins=30)
151/440: sns.histplot(data=original_beagles, x="true_class_score", bins=50)
151/441: sns.histplot(data=original_beagles, x="true_class_score", bins=30)
151/442: sns.histplot(data=original_beagles, x="true_class_score", bins=60)
151/443: sns.histplot(data=original_beagles, x="true_class_score", bins=100)
151/444: sns.histplot(data=original_beagles, x="true_class_score", bins=147)
151/445: sns.histplot(data=original_beagles, x="true_class_score", bins=60)
151/446: sns.histplot(data=original_beagles, x="true_class_score", bins=60, kde=True)
151/447:
original_beagles = original_df[original_df.true_class == 'beagle']
sns.histplot(data=original_beagles, x="true_class_score", bins=60, kde=True)
151/448:
wrong_pred_beagles = original_beagles[original_beagles.predicted_class != 'beagle']
wrong_pred_beagles
151/449:
wrong_pred_beagles = original_beagles[original_beagles.predicted_class != 'beagle']
wrong_pred_beagles.shape
151/450: sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=60, kde=True)
151/451: sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=30, kde=True)
151/452: sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=60, kde=True)
151/453: sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=30, kde=True)
151/454: sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=20, kde=True)
151/455: sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=10, kde=True)
151/456: sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=25, kde=True)
151/457:
original_beagles = original_df[original_df.true_class == 'beagle']
sns.histplot(data=original_beagles, x="true_class_score", bins=40, kde=True)
151/458:
wrong_pred_beagles = original_beagles[original_beagles.predicted_class != 'beagle']
wrong_pred_beagles.shape
151/459: sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=25, kde=True)
151/460: sns.histplot(data=original_df, x="true_class_score", bins=40, kde=True, hue='true_class')
151/461: sns.histplot(data=original_df, x="true_class_score", bins=50, kde=True, hue='true_class')
151/462: sns.histplot(data=original_df, x="true_class_score", bins=40, kde=True, hue='true_class')
151/463: sns.histplot(data=original_df, x="true_class_score", bins=40, kde=True, hue='true_class', figsize=(20,20))
151/464: sns.histplot(data=original_df, x="true_class_score", bins=40, kde=True, hue='true_class')
151/465: sns.histplot(data=original_df, x="true_class_score", bins=40, kde=True, hue='true_class', stacked=False)
151/466: sns.histplot(data=original_df, x="true_class_score", bins=40, kde=True, hue='true_class', element="step")
151/467: sns.histplot(data=original_df, x="true_class_score", bins=40, kde=True, hue='true_class', multiple="dodge")
151/468: sns.histplot(data=original_df, x="true_class_score", bins=20, kde=True, hue='true_class', multiple="dodge")
151/469: sns.histplot(data=original_df, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")
151/470:
wrong_pred = original_df[original_df.true_class != original_df.predicted_class]
wrong_pred
151/471:
wrong_pred = original_df[original_df.true_class != original_df.predicted_class].reset_index(drop=True)
wrong_pred
151/472:
wrong_pred = original_df[original_df.true_class != original_df.predicted_class].reset_index(drop=True)
wrong_pred.shape
151/473: sns.histplot(data=wrong_pred, x="true_class_score", bins=25, kde=True)
151/474: sns.histplot(data=wrong_pred, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")
151/475:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
151/476: data_dir
151/477:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
print(original)

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/478:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/479:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][10])
151/480:
def pre_process_images():
    original_pp = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in original:
        for i, img in enumerate(original[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            original_pp[breed].append(img)
            #original[breed][i] = img
    return original_pp
151/481: original_pp = pre_process_images()
151/482:
predictions_original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_original:
    for img in original_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_original[breed].append({'yhat': yhat, 'y': y})
151/483:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score += [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class += [item['yhat'][1] for item in predictions_original[breed]]
    
original_df = pd.DataFrame()
original_df['true_class_score'] = true_class_score
original_df['true_class'] = true_class
original_df['predicted_class_score'] = predicted_class_score
original_df['predicted_class'] = predicted_class
original_df
151/484: sns.histplot(data=original_df, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")
151/485:
wrong_pred = original_df[original_df.true_class != original_df.predicted_class].reset_index(drop=True)
wrong_pred.shape
151/486: sns.histplot(data=wrong_pred, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")
151/487:
original_beagles = original_df[original_df.true_class == 'beagle']
sns.histplot(data=original_beagles, x="true_class_score", bins=40, kde=True)
151/488:
wrong_pred_beagles = original_beagles[original_beagles.predicted_class != 'beagle']
wrong_pred_beagles.shape
151/489: sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=25, kde=True)
151/490: original
151/491:
def pre_process_compression():
    original_ppc = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in breeds:
        for img in original[breed]:
            img = image.img_to_array(img)
            img = img.reshape(224*224, 3)
            original_ppc[breed].append(img)
151/492: original_ppc
151/493: original_ppc = pre_process_compression()
151/494:
original_ppc = pre_process_compression()
original_ppc
151/495:
def pre_process_compression():
    original_ppc = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in breeds:
        for img in original[breed]:
            img = image.img_to_array(img)
            img = img.reshape(224*224, 3)
            original_ppc[breed].append(img)
    return original_ppc
151/496:
original_ppc = pre_process_compression()
original_ppc
151/497: original_ppc = pre_process_compression()
151/498:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(original_ppc[breeds[0]][0])
151/499:
import numpy as np
import os
from pathlib import Path
import pandas as pd
import seaborn as sns
import PIL
import tensorflow as tf
from sklearn.cluster import KMeans
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
151/500:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(original_ppc[breeds[0]][0])
151/501:
#Implement k-means clustering to form k clusters
kmeans = KMeans(n_clusters=4)
kmeans.fit(original_ppc[breeds[0]])
151/502:
def compress_imgs_kmeans(K):
    k_compressed_imgs = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    kmeans = KMeans(n_clusters=K)
    for breed in breeds:
        for img in original_ppc[breed]:
            kmeans.fit(img)
            compressed_img = kmeans.cluster_centers_[kmeans.labels_]
            compressed_img = np.clip(compressed_img.astype('uint8'), 0, 255)
            
            #Reshape the image to original dimension
            compressed_img = compressed_img.reshape(224, 224, 3)
            k_compressed_imgs[breed].append(compressed_img)
    
    return k_compressed_imgs
151/503: four_compressed_imgs = compress_imgs_kmeans(4)
151/504:
def compress_imgs_kmeans(K):
    k_compressed_imgs = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    kmeans = KMeans(n_clusters=K, n_init='auto')
    for breed in breeds:
        for img in original_ppc[breed]:
            kmeans.fit(img)
            compressed_img = kmeans.cluster_centers_[kmeans.labels_]
            compressed_img = np.clip(compressed_img.astype('uint8'), 0, 255)
            
            #Reshape the image to original dimension
            compressed_img = compressed_img.reshape(224, 224, 3)
            k_compressed_imgs[breed].append(compressed_img)
    
    return k_compressed_imgs
151/505: four_compressed_imgs = compress_imgs_kmeans(4)
151/506: plt.imshow(four_compressed_imgs[breeds[0]][0])
151/507: plt.imshow(four_compressed_imgs[breeds[1]][0])
151/508:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(four_compressed_imgs[breeds[i]][10])
151/509:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(four_compressed_imgs[breeds[i]][10])
151/510:
def pre_process_images(dict_imgs):
    dict_pp = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in dict_pp:
        for i, img in enumerate(dict_imgs[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            dict_pp[breed].append(img)

    return dict_pp
151/511: original_pp = pre_process_images(original)
151/512: four_compressed = compress_imgs_kmeans(4)
151/513:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(four_compressed[breeds[i]][10])
151/514: four_compressed_pp = pre_process_images(four_compressed)
151/515: four_compressed_pp
151/516:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=10)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/517:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=100)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/518:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=200)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/519:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=300)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/520:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=400)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/521:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=500)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/522:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=600)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/523:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=700)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/524:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=800)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/525:
predictions_four_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in predictions_four_compressed:
    for img in four_compressed_pp[breed]:
        pred = model.predict(img)
        decoded_pred = decode_predictions(pred, top=900)[0]
        yhat = decoded_pred[0]
        y = [item for item in decoded_pred if item[1] == breed][0]
        predictions_four_compressed[breed].append({'yhat': yhat, 'y': y})

predictions_four_compressed
151/526:
eight_compressed = compress_imgs_kmeans(8)
four_compressed = compress_imgs_kmeans(4)
two_compressed = compress_imgs_kmeans(2)
151/527:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(two_compressed[breeds[i]][10])
151/528:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(eight_compressed[breeds[i]][10])
151/529:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]

for i in range(3):
    for j in range(4):
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/530:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]

for i in range(3):
    for j in range(4):
        ax[i].xticks('a')
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/531:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]

for i in range(3):
    for j in range(4):
        ax[i].set_xticks('a')
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/532:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks('a')
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/533:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]

for i in range(3):
    for j in range(4):
        ax[i][j].set_xlabel('a')
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/534:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]

for i in range(3):
    for j in range(4):
        ax[i].set_xlabel('a')
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/535:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/536:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/537:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/538:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/539:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
fig.tight_layout(pad=5.0)
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/540:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
fig.tight_layout(pad=2.0)
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/541:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
fig.tight_layout(pad=1.0)
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/542:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
fig.tight_layout(pad=0.5)
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/543:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
fig.tight_layout()
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/544:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [eight_compressed, four_compressed, two_compressed]
ylabels = ['eight_compressed', 'four_compressed', 'two_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/545:
eight_compressed_pp = pre_process_images(eight_compressed)
four_compressed_pp = pre_process_images(four_compressed)
two_compressed_pp = pre_process_images(two_compressed)
151/546:
def predict_k_compressed(k_compressed_pp):
    preds_k_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

    for breed in preds_k_compressed:
        for img in k_compressed_pp[breed]:
            pred = model.predict(img)
            decoded_pred = decode_predictions(pred, top=900)[0]
            yhat = decoded_pred[0]
            y = [item for item in decoded_pred if item[1] == breed][0]
            preds_k_compressed[breed].append({'yhat': yhat, 'y': y})

    return preds_k_compressed
151/547: preds_k_compressed = predict_k_compressed(four_compressed_pp)
151/548: preds_k_compressed = predict_k_compressed(two_compressed_pp)
151/549:
preds_two_compressed = predict_k_compressed(two_compressed_pp)
preds_four_compressed = predict_k_compressed(four_compressed_pp)
preds_eight_compressed = predict_k_compressed(eight_compressed_pp)
151/550:
def build_dataframe(preds):
    true_class_score = []
    true_class = []
    predicted_class_score = []
    predicted_class = []

    for breed in breeds:
        true_class_score += [item['y'][2] for item in preds[breed]]
        true_class += [breed for i in range(147)]
        predicted_class_score += [item['yhat'][2] for item in preds[breed]]
        predicted_class += [item['yhat'][1] for item in preds[breed]]
    
    df = pd.DataFrame()
    df['true_class_score'] = true_class_score
    df['true_class'] = true_class
    df['predicted_class_score'] = predicted_class_score
    df['predicted_class'] = predicted_class
    
    return df
151/551:
two_compressed = build_dataframe(preds_two_compressed)
four_compressed = build_dataframe(preds_four_compressed)
eight_compressed = build_dataframe(preds_eight_compressed)
151/552: two_compressed
151/553: two_compressed[two_compressed.true_class == two_compressed.predicted_class]
151/554: two_compressed[two_compressed.true_class == two_compressed.predicted_class]
151/555: two_compressed[two_compressed.true_class == two_compressed.predicted_class]
151/556:

two_compressed[two_compressed.true_class == two_compressed.predicted_class]
151/557: four_compressed[four_compressed.true_class == four_compressed.predicted_class]
151/558: eight_compressed[eight_compressed.true_class == eight_compressed.predicted_class]
151/559: two_compressed[two_compressed.true_class != two_compressed.predicted_class]
151/560: two_compressed[two_compressed.true_class != two_compressed.predicted_class].shape
151/561: 588 / 4
151/562: four_compressed[four_compressed.true_class != four_compressed.predicted_class].shape
151/563: eight_compressed[eight_compressed.true_class != eight_compressed.predicted_class].shape
151/564:
fig, ax = plt.subplots(cols=3)

sns.histplot(data=two_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
151/565:
fig, ax = plt.subplots(ncols=3)

sns.histplot(data=two_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
151/566:
fig, ax = plt.subplots(ncols=3)

sns.histplot(data=two_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=four_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=eight_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])
151/567:
fig, ax = plt.subplots(ncols=3, figsize=(10,6))

sns.histplot(data=two_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=four_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=eight_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])
151/568:
fig, ax = plt.subplots(ncols=3, figsize=(10,8))

sns.histplot(data=two_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=four_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=eight_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])
151/569:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=two_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=four_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=eight_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])
151/570:
sixteen_compressed = compress_imgs_kmeans(16)
eight_compressed = compress_imgs_kmeans(8)
four_compressed = compress_imgs_kmeans(4)
151/571:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [sixteen_compressed, eight_compressed, four_compressed]
ylabels = ['sixteen_compressed', 'eight_compressed', 'four_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][10])
151/572:
sixteen_compressed_pp = pre_process_images(sixteen_compressed)
eight_compressed_pp = pre_process_images(eight_compressed)
four_compressed_pp = pre_process_images(four_compressed)
151/573:
def predict_k_compressed(k_compressed_pp):
    preds_k_compressed = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

    for breed in preds_k_compressed:
        for img in k_compressed_pp[breed]:
            pred = model.predict(img)
            decoded_pred = decode_predictions(pred, top=900)[0]
            yhat = decoded_pred[0]
            y = [item for item in decoded_pred if item[1] == breed][0]
            preds_k_compressed[breed].append({'yhat': yhat, 'y': y})

    return preds_k_compressed
151/574:
preds_sixteen_compressed = predict_k_compressed(sixteen_compressed_pp)
preds_eight_compressed = predict_k_compressed(eight_compressed_pp)
preds_four_compressed = predict_k_compressed(four_compressed_pp)
151/575:
def build_dataframe(preds):
    true_class_score = []
    true_class = []
    predicted_class_score = []
    predicted_class = []

    for breed in breeds:
        true_class_score += [item['y'][2] for item in preds[breed]]
        true_class += [breed for i in range(147)]
        predicted_class_score += [item['yhat'][2] for item in preds[breed]]
        predicted_class += [item['yhat'][1] for item in preds[breed]]
    
    df = pd.DataFrame()
    df['true_class_score'] = true_class_score
    df['true_class'] = true_class
    df['predicted_class_score'] = predicted_class_score
    df['predicted_class'] = predicted_class
    
    return df
151/576:
two_compressed = build_dataframe(preds_sixteen_compressed)
eight_compressed = build_dataframe(preds_eight_compressed)
four_compressed = build_dataframe(preds_four_compressed)
151/577:
sixteen_compressed = build_dataframe(preds_sixteen_compressed)
eight_compressed = build_dataframe(preds_eight_compressed)
four_compressed = build_dataframe(preds_four_compressed)
151/578:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])
151/579:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=20, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])
151/580:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])
151/581:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])
151/582:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
151/583:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
plt.title("ResNet50 true class scores considering different image /compressions")
151/584:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
plt.title("ResNet50 true class scores considering different image compressions")
151/585:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
plt.title("ResNet50 true class scores considering different image compressions")
plt.show()
151/586:
fig, ax = plt.subplots(ncols=3, figsize=(16,6))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
plt.show()
151/587: sixteen_compressed[sixteen_compressed.true_class != sixteen_compressed.predicted_class].shape
151/588: eight_compressed[eight_compressed.true_class != eight_compressed.predicted_class].shape
151/589: four_compressed[four_compressed.true_class != four_compressed.predicted_class].shape
151/590:
fig, ax = plt.subplots(ncols=3, figsize=(16,4))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
plt.show()
151/591:
fig, ax = plt.subplots(ncols=3, figsize=(16,5))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
plt.show()
151/592:
wrong_sixteen = sixteen_compressed[sixteen_compressed.true_class != sixteen_compressed.predicted_class]
wrong_eight   = eight_compressed[eight_compressed.true_class != eight_compressed.predicted_class]
wrong_four    = four_compressed[four_compressed.true_class != four_compressed.predicted_class]
151/593:
wrong_sixteen = sixteen_compressed[sixteen_compressed.true_class != sixteen_compressed.predicted_class].reset_index(drop=True)
wrong_eight   = eight_compressed[eight_compressed.true_class != eight_compressed.predicted_class].reset_index(drop=True)
wrong_four    = four_compressed[four_compressed.true_class != four_compressed.predicted_class].reset_index(drop=True)
151/594:
fig, ax = plt.subplots(ncols=3, figsize=(16,5))

sns.histplot(data=wrong_sixteen, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=wrong_eight, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=wrong_four, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression\nmisclassified cases')
ax[1].set_title('Eight-clustered compression\nmisclassified cases')
ax[2].set_title('Four-clustered compression\nmisclassified cases')
plt.show()
151/595:
fig, ax = plt.subplots(ncols=3, figsize=(16,5))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
plt.show()
151/596:
fig, ax = plt.subplots(ncols=3, figsize=(16,5))

sns.histplot(data=wrong_sixteen, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=wrong_eight, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=wrong_four, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression\nmisclassified cases')
ax[1].set_title('Eight-clustered compression\nmisclassified cases')
ax[2].set_title('Four-clustered compression\nmisclassified cases')
plt.show()
151/597: original_acc = original_df.shape
151/598: original_acc = original_df[original_df.true_class == original_df.predicted_class].shape[0] / original_df.shape[0]
151/599: original_acc
151/600: original_df[original_df.true_class == original_df.predicted_class].shape[0]
151/601: original_acc = 1 - (wrong_pred.shape[0] / original_df.shape[0])
151/602: original_acc
151/603: wrong_pred.shape[0
151/604: wrong_pred.shape[0]
151/605: wrong_pred.shape[0] - 588
151/606: sixteen_compressed.shape[0]
151/607:
original_acc = 1 - (wrong_pred.shape[0] / 588)
sixteen_acc = 1 - (wrong_sixteen.shape[0] / 588)
eight_acc = 1 - (wrong_eight.shape[0] / 588)
four_acc = 1 - (wrong_four.shape[0] / 588)
151/608: original_acc
151/609: sixteen_acc
151/610: eight_acc
151/611: four_acc
151/612:
data = [[original_acc, 'Original'], [sixteen_acc, 'Sixteen'], [eight_acc, 'Eight'], [four_acc, 'Four']]
data
151/613: acc = pd.DataFrame(data)
151/614: acc
151/615: acc.columns = ['Accuracy', 'Clusters']
151/616: acc
151/617: acc.hist()
151/618: sns.histplot(data=acc, x="Clusters", y='Accuracy', bins=10, kde=True, multiple="dodge")
151/619: sns.histplot(data=acc, x="Clusters", y='Accuracy', bins=10, kde=True)
151/620: sns.histplot(data=acc, x="Clusters", y='Accuracy', bins=10)
151/621: acc.hist(x='Clusters', y='Accuracy')
151/622: acc.plot.hist(x='Clusters', y='Accuracy')
151/623: sns.barplot(data=acc, x="Clusters", y="Accuracy")
151/624: plt.bar(acc.Clusters, acc.Accuracy, edgecolor = 'red')
151/625: sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='red')
151/626: sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')
151/627:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentual of correct predictions considering all four classes under the 4 scenarios of evaluation")
151/628:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentual of correct predictions considering\nall four classes under the 4 scenarios of evaluation")
151/629:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentual of correct predictions considering\nall four classes under the 4 scenarios of evaluation")
plt.show()
151/630:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentual of ResNet50 correct predictions considering\nall four classes under the 4 scenarios of evaluation")
plt.show()
151/631:
data = [[original_acc, 'Original image'], [sixteen_acc, 'Sixteen'], [eight_acc, 'Eight'], [four_acc, 'Four']]
data
151/632: acc = pd.DataFrame(data)
151/633: acc.columns = ['Accuracy', 'Clusters']
151/634: acc
151/635:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentual of ResNet50 correct predictions considering\nall four classes under the 4 scenarios of evaluation")
plt.show()
151/636:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black', kde=True)

plt.title("Percentual of ResNet50 correct predictions considering\nall four classes under the 4 scenarios of evaluation")
plt.show()
151/637:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentual of ResNet50 correct predictions considering\nall four classes under the 4 scenarios of evaluation")
plt.show()
151/638:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
151/639:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
151/640:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][11])
151/641:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][14])
151/642:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][15])
151/643:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][16])
151/644:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][18])
151/645:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][28])
151/646:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][38])
151/647:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][35])
151/648:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][32])
151/649:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][22])
151/650:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][72])
151/651:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][62])
151/652:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][61])
151/653:
def predict_ResNet50(dict_pp):
    preds = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

    for breed in preds:
        for img in dict_pp[breed]:
            pred = model.predict(img)
            decoded_pred = decode_predictions(pred, top=900)[0]
            yhat = decoded_pred[0]
            y = [item for item in decoded_pred if item[1] == breed][0]
            preds[breed].append({'yhat': yhat, 'y': y})

    return preds
151/654: original_pp = pre_process_images(original)
151/655: predictions_original = predict_ResNet50(original_pp)
151/656:
true_class_score = []
true_class = []
predicted_class_score = []
predicted_class = []

for breed in breeds:
    true_class_score += [item['y'][2] for item in predictions_original[breed]]
    true_class += [breed for i in range(147)]
    predicted_class_score += [item['yhat'][2] for item in predictions_original[breed]]
    predicted_class += [item['yhat'][1] for item in predictions_original[breed]]
    
original_df = pd.DataFrame()
original_df['true_class_score'] = true_class_score
original_df['true_class'] = true_class
original_df['predicted_class_score'] = predicted_class_score
original_df['predicted_class'] = predicted_class
original_df
151/657:
original_df = build_dataframe(predictions_original)
original_df
151/658:
sns.histplot(data=original_df, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")

plt.title("Score distribution for the expected classes after model prediction")
plt.show()
151/659:
wrong_pred = original_df[original_df.true_class != original_df.predicted_class].reset_index(drop=True)
wrong_pred.shape
151/660:
sns.histplot(data=wrong_pred, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")

plt.title("Score distribution for the expected classes after model misprediction")
plt.show()
151/661:
sns.histplot(data=wrong_pred, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")

plt.title("Misclassified cases' score distribution for the expected classes")
plt.show()
151/662:
original_beagles = original_df[original_df.true_class == 'beagle']
sns.histplot(data=original_beagles, x="true_class_score", bins=40, kde=True)

plt.title("Score distribution for the expected beagle class after model prediction")
plt.show()
151/663:
original_beagles = original_df[original_df.true_class == 'beagle']
sns.histplot(data=original_beagles, x="true_class_score", bins=40, kde=True)

plt.title("Score distribution for the expected BEAGLE class after model prediction")
plt.show()
151/664:
sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=25, kde=True)

plt.title("Beagles misclassified cases' score distribution for the expected classes")
plt.show()
151/665:
sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=25, kde=True)

plt.title("Beagles misclassified cases' true score distribution")
plt.show()
151/666:
fig, ax = plt.subplots(ncols=3, figsize=(16,5))

sns.histplot(data=wrong_sixteen, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=wrong_eight, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=wrong_four, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression\nmisclassified cases')
ax[1].set_title('Eight-clustered compression\nmisclassified cases')
ax[2].set_title('Four-clustered compression\nmisclassified cases')
plt.show()
151/667:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions considering\nall four classes under the 4 scenarios of evaluation")
plt.show()
151/668:
print("ResNet correct classification percentage over original images: ", original_acc)
print("ResNet correct classification percentage over sixteen-clustered compressed images: ", original_acc)
print("ResNet correct classification percentage over eight-clustered compressed images: ", original_acc)
print("ResNet correct classification percentage over four-clustered compressed images: ", original_acc)
151/669:
print("ResNet correct classification percentage under original images: ", original_acc)
print("ResNet correct classification percentage under sixteen-clustered compressed images: ", original_acc)
print("ResNet correct classification percentage under eight-clustered compressed images: ", original_acc)
print("ResNet correct classification percentage under four-clustered compressed images: ", original_acc)
151/670:
print("ResNet correct classification percentage under original images: ", original_acc)
print("ResNet correct classification percentage under sixteen-clustered compressed images: ", sixteen_acc)
print("ResNet correct classification percentage under eight-clustered compressed images: ", eight_acc)
print("ResNet correct classification percentage under four-clustered compressed images: ", four_acc)
151/671:
print("ResNet correct classification percentage under original images: ", round(original_acc * 100, 2), "%")
print("ResNet correct classification percentage under sixteen-clustered compressed images: ", sixteen_acc)
print("ResNet correct classification percentage under eight-clustered compressed images: ", eight_acc)
print("ResNet correct classification percentage under four-clustered compressed images: ", four_acc)
151/672:
print("ResNet correct classification percentage under original images:", round(original_acc * 100, 2), "%")
print("ResNet correct classification percentage under sixteen-clustered compressed images: ", sixteen_acc)
print("ResNet correct classification percentage under eight-clustered compressed images: ", eight_acc)
print("ResNet correct classification percentage under four-clustered compressed images: ", four_acc)
151/673:
print("ResNet correct classification percentage under original images:", round(original_acc * 100, 2), "%")
print("ResNet correct classification percentage under sixteen-clustered compressed images: ", round(sixteen_acc * 100, 2), "%")
print("ResNet correct classification percentage under eight-clustered compressed images: ", round(eight_acc * 100, 2), "%")
print("ResNet correct classification percentage under four-clustered compressed images: ", round(four_acc * 100, 2), "%")
151/674: acc = pd.DataFrame([[original_acc, 'Original image'], [sixteen_acc, 'Sixteen'], [eight_acc, 'Eight'], [four_acc, 'Four']])
151/675: acc.columns = ['Accuracy', 'Clusters']
151/676: acc
151/677:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions considering\nall cases from all four classes under the 4 scenarios of evaluation")
plt.show()
151/678:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions considering\nall cases from all four classes\nunder the 4 scenarios of evaluation")
plt.show()
151/679:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions considering\nall cases from all four classes under the 4\nscenarios of evaluation")
plt.show()
151/680:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions\nconsidering\nall cases from all four classes under the 4\nscenarios of evaluation")
plt.show()
151/681:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions\nconsidering all cases from all four classes under the 4\nscenarios of evaluation")
plt.show()
151/682:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions\nconsidering all cases from all four classes under\nthe 4\nscenarios of evaluation")
plt.show()
151/683:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions\nconsidering all cases from all four classes under\nthe 4 scenarios of evaluation")
plt.show()
152/1:
import numpy as np
import os
from pathlib import Path
import pandas as pd
import seaborn as sns
import PIL
import tensorflow as tf
from sklearn.cluster import KMeans
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
152/2:
def pre_process_images(dict_imgs):
    dict_pp = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in dict_pp:
        for i, img in enumerate(dict_imgs[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            dict_pp[breed].append(img)

    return dict_pp
152/3:
def pre_process_compression():
    original_ppc = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in breeds:
        for img in original[breed]:
            img = image.img_to_array(img)
            img = img.reshape(224*224, 3)
            original_ppc[breed].append(img)
    return original_ppc
152/4:
def compress_imgs_kmeans(K):
    k_compressed_imgs = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    kmeans = KMeans(n_clusters=K, n_init='auto')
    for breed in breeds:
        for img in original_ppc[breed]:
            kmeans.fit(img)
            compressed_img = kmeans.cluster_centers_[kmeans.labels_]
            compressed_img = np.clip(compressed_img.astype('uint8'), 0, 255)
            
            #Reshape the image to original dimension
            compressed_img = compressed_img.reshape(224, 224, 3)
            k_compressed_imgs[breed].append(compressed_img)
    
    return k_compressed_imgs
152/5:
def predict_ResNet50(dict_pp):
    preds = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

    for breed in preds:
        for img in dict_pp[breed]:
            pred = model.predict(img)
            decoded_pred = decode_predictions(pred, top=900)[0]
            yhat = decoded_pred[0]
            y = [item for item in decoded_pred if item[1] == breed][0]
            preds[breed].append({'yhat': yhat, 'y': y})

    return preds
152/6:
def build_dataframe(preds):
    true_class_score = []
    true_class = []
    predicted_class_score = []
    predicted_class = []

    for breed in breeds:
        true_class_score += [item['y'][2] for item in preds[breed]]
        true_class += [breed for i in range(147)]
        predicted_class_score += [item['yhat'][2] for item in preds[breed]]
        predicted_class += [item['yhat'][1] for item in preds[breed]]
    
    df = pd.DataFrame()
    df['true_class_score'] = true_class_score
    df['true_class'] = true_class
    df['predicted_class_score'] = predicted_class_score
    df['predicted_class'] = predicted_class
    
    return df
152/7:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
152/8: data_dir
152/9:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
152/10:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
152/11:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][61])
152/12: model = ResNet50(weights='imagenet')
152/13: original_pp = pre_process_images(original)
152/14: predictions_original = predict_ResNet50(original_pp)
153/1:
import numpy as np
import os
from pathlib import Path
import pandas as pd
import seaborn as sns
import PIL
import tensorflow as tf
from sklearn.cluster import KMeans
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
153/2:
def pre_process_images(dict_imgs):
    dict_pp = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in dict_pp:
        for i, img in enumerate(dict_imgs[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            dict_pp[breed].append(img)

    return dict_pp
153/3:
def pre_process_compression():
    original_ppc = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in breeds:
        for img in original[breed]:
            img = image.img_to_array(img)
            img = img.reshape(224*224, 3)
            original_ppc[breed].append(img)
    return original_ppc
153/4:
def compress_imgs_kmeans(K):
    k_compressed_imgs = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    kmeans = KMeans(n_clusters=K, n_init='auto')
    for breed in breeds:
        for img in original_ppc[breed]:
            kmeans.fit(img)
            compressed_img = kmeans.cluster_centers_[kmeans.labels_]
            compressed_img = np.clip(compressed_img.astype('uint8'), 0, 255)
            
            #Reshape the image to original dimension
            compressed_img = compressed_img.reshape(224, 224, 3)
            k_compressed_imgs[breed].append(compressed_img)
    
    return k_compressed_imgs
153/5:
def predict_ResNet50(dict_pp):
    preds = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

    for breed in preds:
        for img in dict_pp[breed]:
            pred = model.predict(img)
            decoded_pred = decode_predictions(pred, top=900)[0]
            yhat = decoded_pred[0]
            y = [item for item in decoded_pred if item[1] == breed][0]
            preds[breed].append({'yhat': yhat, 'y': y})

    return preds
153/6:
def build_dataframe(preds):
    true_class_score = []
    true_class = []
    predicted_class_score = []
    predicted_class = []

    for breed in breeds:
        true_class_score += [item['y'][2] for item in preds[breed]]
        true_class += [breed for i in range(147)]
        predicted_class_score += [item['yhat'][2] for item in preds[breed]]
        predicted_class += [item['yhat'][1] for item in preds[breed]]
    
    df = pd.DataFrame()
    df['true_class_score'] = true_class_score
    df['true_class'] = true_class
    df['predicted_class_score'] = predicted_class_score
    df['predicted_class'] = predicted_class
    
    return df
153/7:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
153/8: data_dir
153/9:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
153/10:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
153/11:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][61])
153/12: model = ResNet50(weights='imagenet')
153/13: original_pp = pre_process_images(original)
153/14: predictions_original = predict_ResNet50(original_pp)
153/15:
original_df = build_dataframe(predictions_original)
original_df
153/16:
sns.histplot(data=original_df, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")

plt.title("Score distribution for the expected classes after model prediction")
plt.show()
153/17:
wrong_pred = original_df[original_df.true_class != original_df.predicted_class].reset_index(drop=True)
wrong_pred.shape
153/18:
sns.histplot(data=wrong_pred, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")

plt.title("Misclassified cases' score distribution for the expected classes")
plt.show()
153/19:
original_beagles = original_df[original_df.true_class == 'beagle']
sns.histplot(data=original_beagles, x="true_class_score", bins=40, kde=True)

plt.title("Score distribution for the expected BEAGLE class after model prediction")
plt.show()
153/20:
wrong_pred_beagles = original_beagles[original_beagles.predicted_class != 'beagle']
wrong_pred_beagles.shape
153/21:
sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=25, kde=True)

plt.title("Beagles misclassified cases' true score distribution")
plt.show()
153/22: original_ppc = pre_process_compression()
153/23:
sixteen_compressed = compress_imgs_kmeans(16)
eight_compressed = compress_imgs_kmeans(8)
four_compressed = compress_imgs_kmeans(4)
153/24:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [sixteen_compressed, eight_compressed, four_compressed]
ylabels = ['sixteen_compressed', 'eight_compressed', 'four_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][61])
153/25:
sixteen_compressed_pp = pre_process_images(sixteen_compressed)
eight_compressed_pp = pre_process_images(eight_compressed)
four_compressed_pp = pre_process_images(four_compressed)
153/26:
preds_sixteen_compressed = predict_k_compressed(sixteen_compressed_pp)
preds_eight_compressed = predict_k_compressed(eight_compressed_pp)
preds_four_compressed = predict_k_compressed(four_compressed_pp)
154/1:
import numpy as np
import os
from pathlib import Path
import pandas as pd
import seaborn as sns
import PIL
import tensorflow as tf
from sklearn.cluster import KMeans
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
154/2:
def pre_process_images(dict_imgs):
    dict_pp = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in dict_pp:
        for i, img in enumerate(dict_imgs[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            dict_pp[breed].append(img)

    return dict_pp
154/3:
def pre_process_compression():
    original_ppc = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in breeds:
        for img in original[breed]:
            img = image.img_to_array(img)
            img = img.reshape(224*224, 3)
            original_ppc[breed].append(img)
    return original_ppc
154/4:
def compress_imgs_kmeans(K):
    k_compressed_imgs = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    kmeans = KMeans(n_clusters=K, n_init='auto')
    for breed in breeds:
        for img in original_ppc[breed]:
            kmeans.fit(img)
            compressed_img = kmeans.cluster_centers_[kmeans.labels_]
            compressed_img = np.clip(compressed_img.astype('uint8'), 0, 255)
            
            #Reshape the image to original dimension
            compressed_img = compressed_img.reshape(224, 224, 3)
            k_compressed_imgs[breed].append(compressed_img)
    
    return k_compressed_imgs
154/5:
def predict_ResNet50(dict_pp):
    preds = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

    for breed in preds:
        for img in dict_pp[breed]:
            pred = model.predict(img)
            decoded_pred = decode_predictions(pred, top=900)[0]
            yhat = decoded_pred[0]
            y = [item for item in decoded_pred if item[1] == breed][0]
            preds[breed].append({'yhat': yhat, 'y': y})

    return preds
154/6:
def build_dataframe(preds):
    true_class_score = []
    true_class = []
    predicted_class_score = []
    predicted_class = []

    for breed in breeds:
        true_class_score += [item['y'][2] for item in preds[breed]]
        true_class += [breed for i in range(147)]
        predicted_class_score += [item['yhat'][2] for item in preds[breed]]
        predicted_class += [item['yhat'][1] for item in preds[breed]]
    
    df = pd.DataFrame()
    df['true_class_score'] = true_class_score
    df['true_class'] = true_class
    df['predicted_class_score'] = predicted_class_score
    df['predicted_class'] = predicted_class
    
    return df
154/7:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
154/8: data_dir
154/9:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
154/10:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
154/11:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][61])
154/12: model = ResNet50(weights='imagenet')
154/13: original_pp = pre_process_images(original)
154/14: predictions_original = predict_ResNet50(original_pp)
154/15:
original_df = build_dataframe(predictions_original)
original_df
154/16:
sns.histplot(data=original_df, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")

plt.title("Score distribution for the expected classes after model prediction")
plt.show()
154/17:
wrong_pred = original_df[original_df.true_class != original_df.predicted_class].reset_index(drop=True)
wrong_pred.shape
154/18:
sns.histplot(data=wrong_pred, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")

plt.title("Misclassified cases' score distribution for the expected classes")
plt.show()
154/19:
original_beagles = original_df[original_df.true_class == 'beagle']
sns.histplot(data=original_beagles, x="true_class_score", bins=40, kde=True)

plt.title("Score distribution for the expected BEAGLE class after model prediction")
plt.show()
154/20:
wrong_pred_beagles = original_beagles[original_beagles.predicted_class != 'beagle']
wrong_pred_beagles.shape
154/21:
sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=25, kde=True)

plt.title("Beagles misclassified cases' true score distribution")
plt.show()
154/22: original_ppc = pre_process_compression()
154/23:
sixteen_compressed = compress_imgs_kmeans(16)
eight_compressed = compress_imgs_kmeans(8)
four_compressed = compress_imgs_kmeans(4)
154/24:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [sixteen_compressed, eight_compressed, four_compressed]
ylabels = ['sixteen_compressed', 'eight_compressed', 'four_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][61])
154/25:
sixteen_compressed_pp = pre_process_images(sixteen_compressed)
eight_compressed_pp = pre_process_images(eight_compressed)
four_compressed_pp = pre_process_images(four_compressed)
154/26:
preds_sixteen_compressed = predict_ResNet50(sixteen_compressed_pp)
preds_eight_compressed = predict_ResNet50(eight_compressed_pp)
preds_four_compressed = predict_ResNet50(four_compressed_pp)
154/27:
sixteen_compressed = build_dataframe(preds_sixteen_compressed)
eight_compressed = build_dataframe(preds_eight_compressed)
four_compressed = build_dataframe(preds_four_compressed)
154/28:
fig, ax = plt.subplots(ncols=3, figsize=(16,5))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
plt.show()
154/29:
wrong_sixteen = sixteen_compressed[sixteen_compressed.true_class != sixteen_compressed.predicted_class].reset_index(drop=True)
wrong_eight   = eight_compressed[eight_compressed.true_class != eight_compressed.predicted_class].reset_index(drop=True)
wrong_four    = four_compressed[four_compressed.true_class != four_compressed.predicted_class].reset_index(drop=True)
154/30:
fig, ax = plt.subplots(ncols=3, figsize=(16,5))

sns.histplot(data=wrong_sixteen, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=wrong_eight, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=wrong_four, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression\nmisclassified cases')
ax[1].set_title('Eight-clustered compression\nmisclassified cases')
ax[2].set_title('Four-clustered compression\nmisclassified cases')
plt.show()
154/31:
original_acc = 1 - (wrong_pred.shape[0] / 588)
sixteen_acc = 1 - (wrong_sixteen.shape[0] / 588)
eight_acc = 1 - (wrong_eight.shape[0] / 588)
four_acc = 1 - (wrong_four.shape[0] / 588)
154/32:
print("ResNet correct classification percentage under original images:", round(original_acc * 100, 2), "%")
print("ResNet correct classification percentage under sixteen-clustered compressed images: ", round(sixteen_acc * 100, 2), "%")
print("ResNet correct classification percentage under eight-clustered compressed images: ", round(eight_acc * 100, 2), "%")
print("ResNet correct classification percentage under four-clustered compressed images: ", round(four_acc * 100, 2), "%")
154/33: acc = pd.DataFrame([[original_acc, 'Original image'], [sixteen_acc, 'Sixteen'], [eight_acc, 'Eight'], [four_acc, 'Four']])
154/34: acc.columns = ['Accuracy', 'Clusters']
154/35: acc
154/36:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions\nconsidering all cases from all four classes under\nthe 4 scenarios of evaluation")
plt.show()
155/1:
import numpy as np
import os
from pathlib import Path
import pandas as pd
import seaborn as sns
import PIL
import tensorflow as tf
from sklearn.cluster import KMeans
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from keras.applications.imagenet_utils import decode_predictions
import matplotlib.pyplot as plt
155/2:
# set the images according to the ResNet50 input shape
def pre_process_images(dict_imgs):
    dict_pp = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in dict_pp:
        for i, img in enumerate(dict_imgs[breed]):
            img = image.img_to_array(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            dict_pp[breed].append(img)

    return dict_pp
155/3:
# set the images according to the sklearn K-Means input shape
def pre_process_compression():
    original_ppc = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    for breed in breeds:
        for img in original[breed]:
            img = image.img_to_array(img)
            img = img.reshape(224*224, 3)
            original_ppc[breed].append(img)
    return original_ppc
155/4:
# applies K-Means to the input image in order to compress it
def compress_imgs_kmeans(K):
    k_compressed_imgs = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}
    kmeans = KMeans(n_clusters=K, n_init='auto')
    for breed in breeds:
        for img in original_ppc[breed]:
            kmeans.fit(img)
            compressed_img = kmeans.cluster_centers_[kmeans.labels_]
            compressed_img = np.clip(compressed_img.astype('uint8'), 0, 255)
            
            # Reshape the image to original dimension
            compressed_img = compressed_img.reshape(224, 224, 3)
            k_compressed_imgs[breed].append(compressed_img)
    
    return k_compressed_imgs
155/5:
# runs ResNet50 to predict both true class scores and predicted class scores
def predict_ResNet50(dict_pp):
    preds = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

    for breed in preds:
        for img in dict_pp[breed]:
            pred = model.predict(img)
            decoded_pred = decode_predictions(pred, top=900)[0]
            yhat = decoded_pred[0]
            y = [item for item in decoded_pred if item[1] == breed][0]
            preds[breed].append({'yhat': yhat, 'y': y})

    return preds
155/6:
# builds DataFrame with the four desired columns using the desired prediction dictionary
def build_dataframe(preds):
    true_class_score = []
    true_class = []
    predicted_class_score = []
    predicted_class = []

    for breed in breeds:
        true_class_score += [item['y'][2] for item in preds[breed]]
        true_class += [breed for i in range(147)]
        predicted_class_score += [item['yhat'][2] for item in preds[breed]]
        predicted_class += [item['yhat'][1] for item in preds[breed]]
    
    df = pd.DataFrame()
    df['true_class_score'] = true_class_score
    df['true_class'] = true_class
    df['predicted_class_score'] = predicted_class_score
    df['predicted_class'] = predicted_class
    
    return df
155/7:
data_dir = Path('../dataset/Images')
total_breeds = os.listdir(data_dir)
breeds = [total_breeds[1], total_breeds[8], total_breeds[9], total_breeds[13]]
print(breeds)
155/8: data_dir
155/9:
original = {breeds[0]: [], breeds[1]: [], breeds[2]: [], breeds[3]: []}

for breed in breeds:
    breed_dir = Path('../dataset/Images/' + breed)
    breed_imgs = os.listdir(breed_dir)
    
    for breed_img in breed_imgs:
        breed_img_dir = Path('../dataset/Images/' + breed + '/' + breed_img)
        original[breed].append(image.load_img(breed_img_dir, target_size=(224, 224)))
155/10:
for key in original:
    print('Total of', key, 'images:', len(original[key]))
155/11:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))

for i in range(4):
    ax[i].set_title(breeds[i])
    ax[i].imshow(original[breeds[i]][61])
155/12: model = ResNet50(weights='imagenet')
155/13: original_pp = pre_process_images(original)
155/14: predictions_original = predict_ResNet50(original_pp)
155/15:
original_df = build_dataframe(predictions_original)
original_df
155/16:
sns.histplot(data=original_df, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")

plt.title("Score distribution for the expected classes after model prediction")
plt.show()
155/17:
wrong_pred = original_df[original_df.true_class != original_df.predicted_class].reset_index(drop=True)
wrong_pred.shape
155/18:
sns.histplot(data=wrong_pred, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge")

plt.title("Misclassified cases' score distribution for the expected classes")
plt.show()
155/19:
original_beagles = original_df[original_df.true_class == 'beagle']
sns.histplot(data=original_beagles, x="true_class_score", bins=40, kde=True)

plt.title("Score distribution for the expected BEAGLE class after model prediction")
plt.show()
155/20:
wrong_pred_beagles = original_beagles[original_beagles.predicted_class != 'beagle']
wrong_pred_beagles.shape
155/21:
sns.histplot(data=wrong_pred_beagles, x="true_class_score", bins=25, kde=True)

plt.title("Beagles misclassified cases' true score distribution")
plt.show()
155/22: original_ppc = pre_process_compression()
155/23:
sixteen_compressed = compress_imgs_kmeans(16)
eight_compressed = compress_imgs_kmeans(8)
four_compressed = compress_imgs_kmeans(4)
155/24:
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16,16))
compressed_imgs = [sixteen_compressed, eight_compressed, four_compressed]
ylabels = ['sixteen_compressed', 'eight_compressed', 'four_compressed']

for i in range(3):
    for j in range(4):
        ax[i][j].set_xticks([])
        ax[i][j].set_yticks([])
        ax[i][0].set_ylabel(ylabels[i])
        ax[i][j].set_title(breeds[j])
        ax[i][j].imshow(compressed_imgs[i][breeds[j]][61])
155/25:
sixteen_compressed_pp = pre_process_images(sixteen_compressed)
eight_compressed_pp = pre_process_images(eight_compressed)
four_compressed_pp = pre_process_images(four_compressed)
155/26:
preds_sixteen_compressed = predict_ResNet50(sixteen_compressed_pp)
preds_eight_compressed = predict_ResNet50(eight_compressed_pp)
preds_four_compressed = predict_ResNet50(four_compressed_pp)
155/27:
sixteen_compressed = build_dataframe(preds_sixteen_compressed)
eight_compressed = build_dataframe(preds_eight_compressed)
four_compressed = build_dataframe(preds_four_compressed)
155/28:
fig, ax = plt.subplots(ncols=3, figsize=(16,5))

sns.histplot(data=sixteen_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=eight_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=four_compressed, x="true_class_score", bins=15, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression')
ax[1].set_title('Eight-clustered compression')
ax[2].set_title('Four-clustered compression')
plt.show()
155/29:
wrong_sixteen = sixteen_compressed[sixteen_compressed.true_class != sixteen_compressed.predicted_class].reset_index(drop=True)
wrong_eight   = eight_compressed[eight_compressed.true_class != eight_compressed.predicted_class].reset_index(drop=True)
wrong_four    = four_compressed[four_compressed.true_class != four_compressed.predicted_class].reset_index(drop=True)
155/30:
fig, ax = plt.subplots(ncols=3, figsize=(16,5))

sns.histplot(data=wrong_sixteen, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[0])
sns.histplot(data=wrong_eight, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[1])
sns.histplot(data=wrong_four, x="true_class_score", bins=10, kde=True, hue='true_class', multiple="dodge", ax=ax[2])

ax[0].set_title('Sixteen-clustered compression\nmisclassified cases')
ax[1].set_title('Eight-clustered compression\nmisclassified cases')
ax[2].set_title('Four-clustered compression\nmisclassified cases')
plt.show()
155/31:
original_acc = 1 - (wrong_pred.shape[0] / 588)
sixteen_acc = 1 - (wrong_sixteen.shape[0] / 588)
eight_acc = 1 - (wrong_eight.shape[0] / 588)
four_acc = 1 - (wrong_four.shape[0] / 588)
155/32:
print("ResNet correct classification percentage under original images:", round(original_acc * 100, 2), "%")
print("ResNet correct classification percentage under sixteen-clustered compressed images: ", round(sixteen_acc * 100, 2), "%")
print("ResNet correct classification percentage under eight-clustered compressed images: ", round(eight_acc * 100, 2), "%")
print("ResNet correct classification percentage under four-clustered compressed images: ", round(four_acc * 100, 2), "%")
155/33: acc = pd.DataFrame([[original_acc, 'Original image'], [sixteen_acc, 'Sixteen'], [eight_acc, 'Eight'], [four_acc, 'Four']])
155/34: acc.columns = ['Accuracy', 'Clusters']
155/35: acc
155/36:
sns.barplot(data=acc, x="Clusters", y="Accuracy", edgecolor='black')

plt.title("Percentage of ResNet50 correct predictions\nconsidering all cases from all four classes under\nthe 4 scenarios of evaluation")
plt.show()
156/1:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
156/2: base = pd.read_csv("../dataset/observations.csv")
156/3: base.shape
156/4: base.head()
156/5:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
156/6:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
156/7:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
156/8: base.head()
156/9:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
156/10:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
156/11: base = base.drop(columns=['class'])
156/12:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
156/13:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
156/14:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
156/15: X_train.shape
156/16: X_test.shape
156/17: model = MLPClassifier(hidden_layer_sizes=np.array([5]), activation='tanh', max_iter=300, solver='lbfgs')
156/18: model.fit(X_train, y_train)
156/19: model.predict(X_test)
156/20: model.score(X_test, y_test)
156/21: model.score(X_train, y_train)
156/22: model = MLPClassifier()
156/23: model.fit(X_train, y_train)
156/24: model = MLPClassifier(activation='tanh', max_iter=300, solver='lbfgs')
156/25: model.fit(X_train, y_train)
156/26: model.predict(X_test)
156/27: model.score(X_test, y_test)
156/28: model.score(X_train, y_train)
156/29: model.score(X_test, y_test)
156/30: model.score(X_train, y_train)
156/31: model.fit(X_train, y_train)
156/32: model.predict(X_test)
156/33: model.score(X_test, y_test)
156/34: model.score(X_train, y_train)
156/35: model.fit(X_train, y_train)
156/36: model.predict(X_test)
156/37: model.score(X_test, y_test)
156/38: model.score(X_train, y_train)
156/39: model.fit(X_train, y_train)
156/40: model.predict(X_test)
156/41: model.score(X_test, y_test)
156/42: model.score(X_train, y_train)
156/43: model = MLPClassifier(hidden_layer_sizes=(5,), activation='tanh', max_iter=300, solver='lbfgs')
156/44: model.fit(X_train, y_train)
156/45: model.predict(X_test)
156/46: model.score(X_test, y_test)
156/47: model.score(X_train, y_train)
156/48: model = MLPClassifier(hidden_layer_sizes=(5,5,), activation='tanh', max_iter=300, solver='lbfgs')
156/49: model.fit(X_train, y_train)
156/50: model.predict(X_test)
156/51: model.score(X_test, y_test)
156/52: model.score(X_train, y_train)
156/53: model.predict(X_test)
156/54: model.score(X_test, y_test)
156/55: model.score(X_train, y_train)
156/56: model.fit(X_train, y_train)
156/57: model.predict(X_test)
156/58: model.score(X_test, y_test)
156/59: model.score(X_train, y_train)
156/60: model.fit(X_train, y_train)
156/61: model.predict(X_test)
156/62: model.score(X_test, y_test)
156/63: model.score(X_train, y_train)
156/64: model.fit(X_train, y_train)
156/65: model.predict(X_test)
156/66: model.score(X_test, y_test)
156/67: model = MLPClassifier(hidden_layer_sizes=(5,5,5), activation='tanh', max_iter=300, solver='lbfgs')
156/68: model.fit(X_train, y_train)
156/69: model.predict(X_test)
156/70: model.score(X_test, y_test)
156/71: model.score(X_train, y_train)
156/72: model.predict(X_test)
156/73: model.score(X_test, y_test)
156/74: model.score(X_train, y_train)
156/75: model.fit(X_train, y_train)
156/76: model.predict(X_test)
156/77: model.score(X_test, y_test)
156/78: model.score(X_train, y_train)
156/79: model.fit(X_train, y_train)
156/80: model.predict(X_test)
156/81: model.score(X_test, y_test)
156/82: model.score(X_train, y_train)
156/83: model = MLPClassifier(hidden_layer_sizes=(16,8,4), activation='tanh', max_iter=300, solver='lbfgs')
156/84: model.fit(X_train, y_train)
156/85: model.predict(X_test)
156/86: model.score(X_test, y_test)
156/87: model.score(X_train, y_train)
156/88: model = MLPClassifier(hidden_layer_sizes=(16,8,4), activation='tanh', max_iter=300, solver='lbfgs')
156/89:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
156/90: base = pd.read_csv("../dataset/observations.csv")
156/91: base.shape
156/92: base.head()
156/93:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
156/94:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
156/95:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
156/96: base.head()
156/97:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
156/98:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
156/99: base = base.drop(columns=['class'])
156/100:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
156/101:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
156/102:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
156/103: X_train.shape
156/104: X_test.shape
156/105: model = MLPClassifier(hidden_layer_sizes=(16,8,4), activation='tanh', max_iter=300, solver='lbfgs')
156/106: model.fit(X_train, y_train)
156/107: model.predict(X_test)
156/108: model.score(X_test, y_test)
156/109: model.score(X_train, y_train)
156/110: model = MLPClassifier(hidden_layer_sizes=(16,8,4), activation='tanh', max_iter=300, solver='lbfgs')
156/111: model.fit(X_train, y_train)
156/112: model.predict(X_test)
156/113: model.score(X_test, y_test)
156/114: model.score(X_train, y_train)
156/115: model = MLPClassifier(hidden_layer_sizes=(16,8,4), activation='tanh', max_iter=300, solver='lbfgs')
156/116: model.fit(X_train, y_train)
156/117: model = MLPClassifier(hidden_layer_sizes=(8,4,2), activation='tanh', max_iter=300, solver='lbfgs')
156/118: model.fit(X_train, y_train)
156/119:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
156/120: base = pd.read_csv("../dataset/observations.csv")
156/121: base.shape
156/122: base.head()
156/123:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
156/124:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
156/125:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
156/126: base.head()
156/127:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
156/128:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
156/129: base = base.drop(columns=['class'])
156/130:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
156/131:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
156/132:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
156/133: X_train.shape
156/134: X_test.shape
156/135: model = MLPClassifier(hidden_layer_sizes=(8,4,2), activation='tanh', max_iter=300, solver='lbfgs')
156/136: model.fit(X_train, y_train)
156/137: model.predict(X_test)
156/138: model.score(X_test, y_test)
156/139: model.score(X_train, y_train)
156/140: model = MLPClassifier(hidden_layer_sizes=(8,4,2), activation='tanh', max_iter=300, solver='lbfgs')
156/141: model.fit(X_train, y_train)
156/142: model = MLPClassifier(hidden_layer_sizes=(8,4,2), activation='tanh', max_iter=300)
156/143: model.fit(X_train, y_train)
156/144: model = MLPClassifier(hidden_layer_sizes=(8,4,2), activation='relu', max_iter=300)
156/145: model.fit(X_train, y_train)
156/146: model = MLPClassifier(hidden_layer_sizes=(8,4,2), max_iter=300)
156/147: model.fit(X_train, y_train)
156/148: model = MLPClassifier(hidden_layer_sizes=(8,4,2), max_iter=200)
156/149: model.fit(X_train, y_train)
156/150: model = MLPClassifier(hidden_layer_sizes=(8,4,2), max_iter=500)
156/151: model.fit(X_train, y_train)
156/152: model = MLPClassifier(hidden_layer_sizes=(8,4,2))
156/153: model.fit(X_train, y_train)
156/154: model = MLPClassifier()
156/155: model.fit(X_train, y_train)
156/156: model = MLPClassifier(activation='tanh')
156/157: model.fit(X_train, y_train)
156/158: model = MLPClassifier(activation='tanh', max_iter=300)
156/159: model.fit(X_train, y_train)
156/160: model = MLPClassifier(activation='tanh', max_iter=300, solver='lbfgs')
156/161: model.fit(X_train, y_train)
156/162: model.predict(X_test)
156/163: model.score(X_test, y_test)
156/164: model.score(X_train, y_train)
156/165: model = MLPClassifier(activation='tanh', max_iter=300, solver='lbfgs')
156/166: model.fit(X_train, y_train)
156/167: model.predict(X_test)
156/168: model.score(X_test, y_test)
156/169: model.score(X_train, y_train)
156/170: model = MLPClassifier(activation='tanh', max_iter=300, solver='lbfgs')
156/171: model.fit(X_train, y_train)
156/172: model.predict(X_test)
156/173: model.score(X_test, y_test)
156/174: model.score(X_train, y_train)
156/175: model.fit(X_train, y_train)
156/176: model.predict(X_test)
156/177: model.score(X_test, y_test)
156/178: model.score(X_train, y_train)
156/179: model.fit(X_train, y_train)
156/180: model.predict(X_test)
156/181: model.score(X_test, y_test)
156/182: model.score(X_train, y_train)
156/183: model.fit(X_train, y_train)
156/184: model.predict(X_test)
156/185: model.score(X_test, y_test)
156/186: model.score(X_train, y_train)
156/187: X_train.var(axis=0)
156/188:
X_train_v = X_train.copy()
X_train_v.var(axis=0)
156/189: X_train_v.drop(columns=['degree_spondylolisthesis'])
156/190: model = MLPClassifier(activation='tanh', max_iter=300, solver='lbfgs')
156/191: model.fit(X_train_v, y_train)
156/192:
X_train_v.drop(columns=['degree_spondylolisthesis'])
X_test_v.drop(columns=['degree_spondylolisthesis'])
156/193:
X_train_v = X_train.copy()
X_test_v = X_test.copy()

X_train_v.var(axis=0)
156/194:
X_train_v.drop(columns=['degree_spondylolisthesis'])
X_test_v.drop(columns=['degree_spondylolisthesis'])
156/195: model = MLPClassifier(activation='tanh', max_iter=300, solver='lbfgs')
156/196: model.fit(X_train_v, y_train)
156/197: model.predict(X_test_v)
156/198: model.score(X_test, y_test)
156/199: model.score(X_train, y_train)
156/200: model.score(X_train_v, y_train)
156/201: model.predict(X_test_v)
156/202: model.score(X_test_v, y_test)
156/203: model.score(X_train_v, y_train)
156/204: model = MLPClassifier(activation='tanh', max_iter=300, solver='lbfgs')
156/205: model.fit(X_train_v, y_train)
156/206: model.predict(X_test_v)
156/207: model.score(X_test_v, y_test)
156/208: model.score(X_train_v, y_train)
156/209: model.fit(X_train_v, y_train)
156/210: model.predict(X_test_v)
156/211: model.score(X_test_v, y_test)
156/212: model.score(X_train_v, y_train)
156/213:
from sklearn.feature_selection import RFE

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator=dt, n_features_to_select=4, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/214:
from sklearn.feature_selection import RFE

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator='dt', n_features_to_select=4, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/215:
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator=dt, n_features_to_select=4, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/216: X_train_v3.columns[RFE_selector.support_]
156/217:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_train_v3
156/218:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_test_v3 = RFE_selector.transform(X_test_v3)
dt.fit(sel_X_train_v3, y_train_v3)
RFE_preds = dt.predict(sel_X_test_v3)
rfe_f1_score = round(f1_score(y_test_v3, RFE_preds, average='weighted'),3)
print(rfe_f1_score)
156/219:
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score

dt = DecisionTreeClassifier(random_state=42)

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator=dt, n_features_to_select=4, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/220: X_train_v3.columns[RFE_selector.support_]
156/221:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_test_v3 = RFE_selector.transform(X_test_v3)
dt.fit(sel_X_train_v3, y_train_v3)
RFE_preds = dt.predict(sel_X_test_v3)
rfe_f1_score = round(f1_score(y_test_v3, RFE_preds, average='weighted'),3)
print(rfe_f1_score)
156/222:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_test_v3 = RFE_selector.transform(X_test_v3)
dt.fit(sel_X_train_v3, y_train_v3)
RFE_preds = dt.predict(sel_X_test_v3)
score = dt.score(X_test_v3, y_test_v3)
print(score)
156/223:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_test_v3 = RFE_selector.transform(X_test_v3)
dt.fit(sel_X_train_v3, y_train_v3)
RFE_preds = dt.predict(sel_X_test_v3)
score = dt.score(sel_X_test_v3, y_test_v3)
print(score)
156/224:
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score

dt = DecisionTreeClassifier(random_state=42)

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator=dt, n_features_to_select=4, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/225: X_train_v3.columns[RFE_selector.support_]
156/226:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_test_v3 = RFE_selector.transform(X_test_v3)
dt.fit(sel_X_train_v3, y_train_v3)
RFE_preds = dt.predict(sel_X_test_v3)
score = dt.score(sel_X_test_v3, y_test_v3)
print(score)
156/227:
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score

dt = DecisionTreeClassifier(random_state=42)

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator=dt, n_features_to_select=6, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/228: X_train_v3.columns[RFE_selector.support_]
156/229:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_test_v3 = RFE_selector.transform(X_test_v3)
dt.fit(sel_X_train_v3, y_train_v3)
RFE_preds = dt.predict(sel_X_test_v3)
score = dt.score(sel_X_test_v3, y_test_v3)
print(score)
156/230:
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score

dt = DecisionTreeClassifier(random_state=42)

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator=dt, n_features_to_select=8, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/231: X_train_v3.columns[RFE_selector.support_]
156/232:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_test_v3 = RFE_selector.transform(X_test_v3)
dt.fit(sel_X_train_v3, y_train_v3)
RFE_preds = dt.predict(sel_X_test_v3)
score = dt.score(sel_X_test_v3, y_test_v3)
print(score)
156/233:
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score

dt = DecisionTreeClassifier(random_state=42)

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator=dt, n_features_to_select=10, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/234: X_train_v3.columns[RFE_selector.support_]
156/235:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_test_v3 = RFE_selector.transform(X_test_v3)
dt.fit(sel_X_train_v3, y_train_v3)
RFE_preds = dt.predict(sel_X_test_v3)
score = dt.score(sel_X_test_v3, y_test_v3)
print(score)
156/236:
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score

dt = DecisionTreeClassifier(random_state=42)

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator=dt, n_features_to_select=6, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/237: X_train_v3.columns[RFE_selector.support_]
156/238:
sel_X_train_v3 = RFE_selector.transform(X_train_v3)
sel_X_test_v3 = RFE_selector.transform(X_test_v3)
dt.fit(sel_X_train_v3, y_train_v3)
RFE_preds = dt.predict(sel_X_test_v3)
score = dt.score(sel_X_test_v3, y_test_v3)
print(score)
156/239:
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score

dt = DecisionTreeClassifier(random_state=42)
model = MLPClassifier(activation='tanh', max_iter=300, solver='lbfgs')

X_train_v3, X_test_v3, y_train_v3, y_test_v3 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()
RFE_selector = RFE(estimator=model, n_features_to_select=6, step=1)
RFE_selector.fit(X_train_v3, y_train_v3)
156/240: model = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="relu", random_state=1)
156/241: base = base.drop(columns=['class'])
156/242:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
156/243: base = pd.read_csv("../dataset/observations.csv")
156/244: base.shape
156/245: base.head()
156/246:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
156/247:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
156/248:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
156/249: base.head()
156/250:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
156/251:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
156/252: base = base.drop(columns=['class'])
156/253:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
156/254:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
156/255:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
156/256: X_train.shape
156/257: X_test.shape
156/258: model = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="relu", random_state=1)
156/259: model.fit(X_train, y_train)
156/260: model = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="tanh", random_state=1)
156/261: model.fit(X_train, y_train)
156/262: model.predict(X_test_v)
156/263: model.score(X_test_v, y_test)
156/264: model.score(X_train_v, y_train)
156/265: model = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="tanh", random_state=1)
156/266: model.fit(X_train, y_train)
156/267: model.predict(X_test_v)
156/268: model.score(X_test, y_test)
156/269: model.score(X_train, y_train)
156/270: model = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="tanh", random_state=1)
157/1:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/2: base = pd.read_csv("../dataset/observations.csv")
157/3: base.shape
157/4: base.head()
157/5:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/6:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/7:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/8: base.head()
157/9:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/10:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/11: base = base.drop(columns=['class'])
157/12:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/13:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/14:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
157/15: X_train.shape
157/16: X_test.shape
157/17: model = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="tanh", random_state=1)
157/18: model.fit(X_train, y_train)
157/19: model.predict(X_test)
157/20: model.score(X_test, y_test)
157/21: model.score(X_train, y_train)
157/22:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
157/23:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)

    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("\n\n")
157/24: model = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="tanh", random_state=1)
157/25: model.fit(X_train, y_train)
157/26: plot_confusion_matrix(model, X_test, y_test)
157/27: model.predict(X_test)
157/28: model = MLPClassifier(hidden_layer_sizes=(256,128,64,32), activation="tanh", random_state=42)
157/29: model.fit(X_train, y_train)
157/30: plot_confusion_matrix(model, X_test, y_test)
157/31: model = MLPClassifier(hidden_layer_sizes=(32,16,8,4), activation="tanh", random_state=42)
157/32: model.fit(X_train, y_train)
157/33: plot_confusion_matrix(model, X_test, y_test)
157/34: plot_confusion_matrix(model, X_test, y_test)
157/35: plot_confusion_matrix(model, X_test, y_test)
157/36: plot_confusion_matrix(model, X_test, y_test)
157/37: plot_confusion_matrix(model, X_test, y_test)
157/38: model.fit(X_train, y_train)
157/39: plot_confusion_matrix(model, X_test, y_test)
157/40: model = MLPClassifier(hidden_layer_sizes=(5,5,5,5), activation="tanh", random_state=42)
157/41: model.fit(X_train, y_train)
157/42: plot_confusion_matrix(model, X_test, y_test)
157/43: model = MLPClassifier(hidden_layer_sizes=(5), activation="tanh", random_state=42)
157/44: model.fit(X_train, y_train)
157/45: plot_confusion_matrix(model, X_test, y_test)
157/46: model = MLPClassifier(hidden_layer_sizes=(16), activation="tanh", random_state=42)
157/47: model.fit(X_train, y_train)
157/48: plot_confusion_matrix(model, X_test, y_test)
157/49: model = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42)
157/50: model.fit(X_train, y_train)
157/51: plot_confusion_matrix(model, X_test, y_test)
157/52: model = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42)
157/53: model.fit(X_train, y_train)
157/54: plot_confusion_matrix(model, X_test, y_test)
157/55: model = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42)
157/56: model.fit(X_train, y_train)
157/57: plot_confusion_matrix(model, X_test, y_test)
157/58: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42)
157/59: model.fit(X_train, y_train)
157/60: plot_confusion_matrix(model, X_test, y_test)
157/61: model = MLPClassifier(hidden_layer_sizes=(32,32,8), activation="tanh", random_state=42)
157/62: model.fit(X_train, y_train)
157/63: plot_confusion_matrix(model, X_test, y_test)
157/64: model = MLPClassifier(hidden_layer_sizes=(32,32,32), activation="tanh", random_state=42)
157/65: model.fit(X_train, y_train)
157/66: plot_confusion_matrix(model, X_test, y_test)
157/67: model = MLPClassifier(hidden_layer_sizes=(8,8,), activation="tanh", random_state=42)
157/68: model.fit(X_train, y_train)
157/69: plot_confusion_matrix(model, X_test, y_test)
157/70: model = MLPClassifier(hidden_layer_sizes=(8,8,8), activation="tanh", random_state=42)
157/71: model.fit(X_train, y_train)
157/72: plot_confusion_matrix(model, X_test, y_test)
157/73: model = MLPClassifier(hidden_layer_sizes=(16,16,16), activation="tanh", random_state=42)
157/74: model.fit(X_train, y_train)
157/75: plot_confusion_matrix(model, X_test, y_test)
157/76: model = MLPClassifier(hidden_layer_sizes=(32,16,16), activation="tanh", random_state=42)
157/77: model.fit(X_train, y_train)
157/78: plot_confusion_matrix(model, X_test, y_test)
157/79: model = MLPClassifier(hidden_layer_sizes=(32,32,16), activation="tanh", random_state=42)
157/80: model.fit(X_train, y_train)
157/81: plot_confusion_matrix(model, X_test, y_test)
157/82: model = MLPClassifier(hidden_layer_sizes=(32,32,32), activation="tanh", random_state=42)
157/83: model.fit(X_train, y_train)
157/84: plot_confusion_matrix(model, X_test, y_test)
157/85: model = MLPClassifier(hidden_layer_sizes=(64,32,32), activation="tanh", random_state=42)
157/86: model.fit(X_train, y_train)
157/87: plot_confusion_matrix(model, X_test, y_test)
157/88: model = MLPClassifier(hidden_layer_sizes=(64,64,32), activation="tanh", random_state=42)
157/89: model.fit(X_train, y_train)
157/90: plot_confusion_matrix(model, X_test, y_test)
157/91:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/92: model = MLPClassifier(hidden_layer_sizes=(64,64,32), activation="tanh", random_state=42)
157/93: model.fit(X_train, y_train)
157/94: plot_confusion_matrix(model, X_test, y_test)
157/95: model = MLPClassifier(hidden_layer_sizes=(32,64,32), activation="tanh", random_state=42)
157/96: model.fit(X_train, y_train)
157/97: plot_confusion_matrix(model, X_test, y_test)
157/98: model = MLPClassifier(hidden_layer_sizes=(32,32,32), activation="tanh", random_state=42)
157/99: model.fit(X_train, y_train)
157/100: plot_confusion_matrix(model, X_test, y_test)
157/101: model = MLPClassifier(hidden_layer_sizes=(32,16,32), activation="tanh", random_state=42)
157/102: model.fit(X_train, y_train)
157/103: plot_confusion_matrix(model, X_test, y_test)
157/104: model = MLPClassifier(hidden_layer_sizes=(32,16,16), activation="tanh", random_state=42)
157/105: model.fit(X_train, y_train)
157/106: plot_confusion_matrix(model, X_test, y_test)
157/107: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42)
157/108: model.fit(X_train, y_train)
157/109: plot_confusion_matrix(model, X_test, y_test)
157/110: model = MLPClassifier(hidden_layer_sizes=(32,16,64), activation="tanh", random_state=42)
157/111: model.fit(X_train, y_train)
157/112: plot_confusion_matrix(model, X_test, y_test)
157/113: model = MLPClassifier(hidden_layer_sizes=(32,16,16), activation="tanh", random_state=42)
157/114: model.fit(X_train, y_train)
157/115: plot_confusion_matrix(model, X_test, y_test)
157/116: model = MLPClassifier(hidden_layer_sizes=(32,32,16), activation="tanh", random_state=42)
157/117: model.fit(X_train, y_train)
157/118: plot_confusion_matrix(model, X_test, y_test)
157/119: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42)
157/120: model.fit(X_train, y_train)
157/121: plot_confusion_matrix(model, X_test, y_test)
157/122: model = MLPClassifier(hidden_layer_sizes=(32,32,16), activation="tanh", random_state=42)
157/123: model.fit(X_train, y_train)
157/124: plot_confusion_matrix(model, X_test, y_test)
157/125: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42)
157/126: model.fit(X_train, y_train)
157/127: plot_confusion_matrix(model, X_test, y_test)
157/128: model = MLPClassifier(hidden_layer_sizes=(32,32,16), activation="tanh", random_state=42)
157/129: model.fit(X_train, y_train)
157/130: plot_confusion_matrix(model, X_test, y_test)
157/131: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42)
157/132: model.fit(X_train, y_train)
157/133: plot_confusion_matrix(model, X_test, y_test)
157/134: model = MLPClassifier(hidden_layer_sizes=(128,64,32,16), activation="tanh", random_state=42)
157/135: model.fit(X_train, y_train)
157/136: plot_confusion_matrix(model, X_test, y_test)
157/137: model = MLPClassifier(hidden_layer_sizes=(128,64,32,16), activation="tanh", random_state=42)
157/138: model.fit(X_train, y_train)
157/139: plot_confusion_matrix(model, X_test, y_test)
157/140: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42)
157/141: model.fit(X_train, y_train)
157/142: plot_confusion_matrix(model, X_test, y_test)
157/143: model = MLPClassifier(hidden_layer_sizes=(128,64,32,16), activation="tanh", random_state=42)
157/144: model.fit(X_train, y_train)
157/145: plot_confusion_matrix(model, X_test, y_test)
157/146: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42)
157/147: model.fit(X_train, y_train)
157/148: plot_confusion_matrix(model, X_test, y_test)
157/149: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=300)
157/150: model.fit(X_train, y_train)
157/151: plot_confusion_matrix(model, X_test, y_test)
157/152: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42)
157/153: model.fit(X_train, y_train)
157/154: plot_confusion_matrix(model, X_test, y_test)
157/155: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=300)
157/156: model.fit(X_train, y_train)
157/157: plot_confusion_matrix(model, X_test, y_test)
157/158: model.fit(X_train, y_train)
157/159: plot_confusion_matrix(model, X_test, y_test)
157/160: model.fit(X_train, y_train)
157/161: plot_confusion_matrix(model, X_test, y_test)
157/162: model = MLPClassifier(hidden_layer_sizes=(32,32,16), activation="tanh", random_state=42, max_iter=300)
157/163: model.fit(X_train, y_train)
157/164: plot_confusion_matrix(model, X_test, y_test)
157/165: model = MLPClassifier(hidden_layer_sizes=(32,32,32), activation="tanh", random_state=42, max_iter=300)
157/166: model.fit(X_train, y_train)
157/167: plot_confusion_matrix(model, X_test, y_test)
157/168: model = MLPClassifier(hidden_layer_sizes=(64,32,32,32), activation="tanh", random_state=42, max_iter=300)
157/169: model.fit(X_train, y_train)
157/170: plot_confusion_matrix(model, X_test, y_test)
157/171: model = MLPClassifier(hidden_layer_sizes=(32,32), activation="tanh", random_state=42, max_iter=300)
157/172: model.fit(X_train, y_train)
157/173: plot_confusion_matrix(model, X_test, y_test)
157/174: model = MLPClassifier(hidden_layer_sizes=(32,32, 16), activation="tanh", random_state=42, max_iter=300)
157/175: model.fit(X_train, y_train)
157/176: plot_confusion_matrix(model, X_test, y_test)
157/177: model = MLPClassifier(hidden_layer_sizes=(32,32, 16), activation="tanh", random_state=42, max_iter=250)
157/178: model.fit(X_train, y_train)
157/179: plot_confusion_matrix(model, X_test, y_test)
157/180: model = MLPClassifier(hidden_layer_sizes=(32,32, 16), activation="tanh", random_state=42, max_iter=300)
157/181: model.fit(X_train, y_train)
157/182: plot_confusion_matrix(model, X_test, y_test)
157/183: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=300)
157/184: model.fit(X_train, y_train)
157/185: plot_confusion_matrix(model, X_test, y_test)
157/186: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=200)
157/187: model.fit(X_train, y_train)
157/188: plot_confusion_matrix(model, X_test, y_test)
157/189: model = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=200)
157/190: model.fit(X_train, y_train)
157/191: plot_confusion_matrix(model, X_test, y_test)
157/192: model = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=200)
157/193: model.fit(X_train, y_train)
157/194: plot_confusion_matrix(model, X_test, y_test)
157/195: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=200)
157/196: model.fit(X_train, y_train)
157/197: plot_confusion_matrix(model, X_test, y_test)
157/198: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=280)
157/199: model.fit(X_train, y_train)
157/200: plot_confusion_matrix(model, X_test, y_test)
157/201: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=260)
157/202: model.fit(X_train, y_train)
157/203: plot_confusion_matrix(model, X_test, y_test)
157/204: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42)
157/205: model.fit(X_train, y_train)
157/206: plot_confusion_matrix(model, X_test, y_test)
157/207: model = MLPClassifier(hidden_layer_sizes=(64,32,16,16), activation="tanh", random_state=42)
157/208: model.fit(X_train, y_train)
157/209: model = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42)
157/210: model.fit(X_train, y_train)
157/211: plot_confusion_matrix(model, X_test, y_test)
157/212: model = MLPClassifier(hidden_layer_sizes=(128,64,32,16), activation="tanh", random_state=42)
157/213: model.fit(X_train, y_train)
157/214: plot_confusion_matrix(model, X_test, y_test)
157/215: model = MLPClassifier(hidden_layer_sizes=(128,64,32,16), activation="tanh", random_state=42, max_iter=300)
157/216: model.fit(X_train, y_train)
157/217: plot_confusion_matrix(model, X_test, y_test)
157/218: model = MLPClassifier(hidden_layer_sizes=(128,64,32), activation="tanh", random_state=42, max_iter=300)
157/219: model.fit(X_train, y_train)
157/220: plot_confusion_matrix(model, X_test, y_test)
157/221: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=300)
157/222: model.fit(X_train, y_train)
157/223: plot_confusion_matrix(model, X_test, y_test)
157/224: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=32, max_iter=300)
157/225: model.fit(X_train, y_train)
157/226: plot_confusion_matrix(model, X_test, y_test)
157/227: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=300)
157/228: model.fit(X_train, y_train)
157/229: plot_confusion_matrix(model, X_test, y_test)
157/230: model = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=300)
157/231: model.fit(X_train, y_train)
157/232: plot_confusion_matrix(model, X_test, y_test)
157/233: model = MLPClassifier(hidden_layer_sizes=(64,64), activation="tanh", random_state=42, max_iter=300)
157/234: model.fit(X_train, y_train)
157/235: plot_confusion_matrix(model, X_test, y_test)
157/236: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=300)
157/237: model.fit(X_train, y_train)
157/238: plot_confusion_matrix(model, X_test, y_test)
157/239: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=200)
157/240: model.fit(X_train, y_train)
157/241: plot_confusion_matrix(model, X_test, y_test)
157/242:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/243: base = pd.read_csv("../dataset/observations.csv")
157/244: base.shape
157/245: base.head()
157/246:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/247:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/248:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/249: base.head()
157/250:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/251:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/252: base = base.drop(columns=['class'])
157/253:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/254:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/255:
X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']
157/256: X_train.shape
157/257: X_test.shape
157/258:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/259: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=200)
157/260: model.fit(X_train, y_train)
157/261: plot_confusion_matrix(model, X_test, y_test)
157/262: model.predict(X_test)
157/263: model.score(X_test, y_test)
157/264: model.score(X_train, y_train)
157/265: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=270)
157/266: model.fit(X_train, y_train)
157/267: plot_confusion_matrix(model, X_test, y_test)
157/268: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=260)
157/269: model.fit(X_train, y_train)
157/270: model = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=265)
157/271: model.fit(X_train, y_train)
157/272: plot_confusion_matrix(model, X_test, y_test)
157/273: model = MLPClassifier(hidden_layer_sizes=(32,32,16), activation="tanh", random_state=42, max_iter=265)
157/274: model.fit(X_train, y_train)
157/275: plot_confusion_matrix(model, X_test, y_test)
157/276: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=265)
157/277: model.fit(X_train, y_train)
157/278: plot_confusion_matrix(model, X_test, y_test)
157/279: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=300)
157/280: model.fit(X_train, y_train)
157/281: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=400)
157/282: model.fit(X_train, y_train)
157/283: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/284: model.fit(X_train, y_train)
157/285: plot_confusion_matrix(model, X_test, y_test)
157/286: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=450)
157/287: model_2.fit(X_train, y_train)
157/288: plot_confusion_matrix(model, X_test, y_test)
157/289: plot_confusion_matrix(model_2, X_test, y_test)
157/290: model_2 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=450)
157/291: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=450)
157/292: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=450)
157/293: model_2.fit(X_train, y_train)
157/294: plot_confusion_matrix(model_2, X_test, y_test)
157/295: model_3.fit(X_train, y_train)
157/296: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=500)
157/297: model_3.fit(X_train, y_train)
157/298: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=550)
157/299: model_3.fit(X_train, y_train)
157/300: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=600)
157/301: model_3.fit(X_train, y_train)
157/302: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=650)
157/303: model_3.fit(X_train, y_train)
157/304: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700)
157/305: model_3.fit(X_train, y_train)
157/306: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=750)
157/307: model_3.fit(X_train, y_train)
157/308: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=800)
157/309: model_3.fit(X_train, y_train)
157/310: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=850)
157/311: model_3.fit(X_train, y_train)
157/312: plot_confusion_matrix(model_3, X_test, y_test)
157/313: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=850)
157/314: model_3.fit(X_train, y_train)
157/315: plot_confusion_matrix(model_3, X_test, y_test)
157/316: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=850)
157/317: model_4.fit(X_train, y_train)
157/318: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=900)
157/319: model_4.fit(X_train, y_train)
157/320: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=950)
157/321: model_4.fit(X_train, y_train)
157/322: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1200)
157/323: model_4.fit(X_train, y_train)
157/324: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1100)
157/325: model_4.fit(X_train, y_train)
157/326: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1150)
157/327: model_4.fit(X_train, y_train)
157/328: plot_confusion_matrix(model_4, X_test, y_test)
157/329:
# correlation df
base
157/330:
# correlation df
base.corr()
157/331:
# correlation df
corr_base = base.T
corr_base
157/332:
# correlation df
corr_base = base.T
corr_base.TARGET
157/333:
# correlation df
corr_base = base.TARGET
157/334:
# correlation df
corr_base = base.TARGET
corr_base.corr()
157/335:
# correlation df
base.corr()
157/336:
# correlation df
base.corr().TARGET
157/337:
# correlation df
base.corr().TARGET
157/338: base = base.drop(columns=['pelvic_slope', 'direct_tilt', 'thoracic_slope', 'sacrum_angle', 'scoliosis_slope'])
157/339:
base = base.drop(columns=['pelvic_slope', 'direct_tilt', 'thoracic_slope', 'sacrum_angle', 'scoliosis_slope'])
base
157/340:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/341:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/342: base = pd.read_csv("../dataset/observations.csv")
157/343: base.shape
157/344: base.head()
157/345:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/346:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/347:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/348: base.head()
157/349:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/350:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/351:
# correlation df
base.corr().TARGET
157/352: base = base.drop(columns=['pelvic_slope', 'direct_tilt', 'thoracic_slope', 'sacrum_angle', 'scoliosis_slope'])
157/353: base
157/354: base = base.drop(columns=['class'])
157/355:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/356:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/357:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/358: X_train.shape
157/359: X_test.shape
157/360: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/361: model.fit(X_train, y_train)
157/362: plot_confusion_matrix(model, X_test, y_test)
157/363: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=450)
157/364: model_2.fit(X_train, y_train)
157/365: plot_confusion_matrix(model_2, X_test, y_test)
157/366: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=500)
157/367: model_2.fit(X_train, y_train)
157/368: plot_confusion_matrix(model_2, X_test, y_test)
157/369: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=850)
157/370: model_3.fit(X_train, y_train)
157/371: plot_confusion_matrix(model_3, X_test, y_test)
157/372: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1150)
157/373: model_4.fit(X_train, y_train)
157/374: plot_confusion_matrix(model_4, X_test, y_test)
157/375:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/376:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/377: base = pd.read_csv("../dataset/observations.csv")
157/378: base.shape
157/379: base.head()
157/380:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/381:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/382:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/383: base.head()
157/384:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/385:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/386:
# correlation df
base.corr(numeric_only=True).TARGET
157/387: #base = base.drop(columns=['pelvic_slope', 'direct_tilt', 'thoracic_slope', 'sacrum_angle', 'scoliosis_slope'])
157/388: base
157/389: base = base.drop(columns=['class'])
157/390:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/391:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/392:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/393: X_train.shape
157/394: X_test.shape
157/395: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/396: model.fit(X_train, y_train)
157/397: plot_confusion_matrix(model, X_test, y_test)
157/398: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=500)
157/399: model_2.fit(X_train, y_train)
157/400: plot_confusion_matrix(model_2, X_test, y_test)
157/401: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=850)
157/402: model_3.fit(X_train, y_train)
157/403: plot_confusion_matrix(model_3, X_test, y_test)
157/404: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1150)
157/405: model_4.fit(X_train, y_train)
157/406: plot_confusion_matrix(model_4, X_test, y_test)
157/407: base = base.drop(columns=['pelvic_slope', 'direct_tilt', 'thoracic_slope', 'sacrum_angle', 'scoliosis_slope'])
157/408: base
157/409: base = base.drop(columns=['class'])
157/410:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/411:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/412: base = pd.read_csv("../dataset/observations.csv")
157/413: base.shape
157/414: base.head()
157/415:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/416:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/417:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/418: base.head()
157/419:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/420:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/421:
# correlation df
base.corr(numeric_only=True).TARGET
157/422: base = base.drop(columns=['pelvic_slope', 'direct_tilt', 'thoracic_slope', 'sacrum_angle', 'scoliosis_slope'])
157/423: base
157/424: base = base.drop(columns=['class'])
157/425:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/426:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/427:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/428: X_train.shape
157/429: X_test.shape
157/430: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/431: model.fit(X_train, y_train)
157/432: plot_confusion_matrix(model, X_test, y_test)
157/433: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=500)
157/434: model_2.fit(X_train, y_train)
157/435: plot_confusion_matrix(model_2, X_test, y_test)
157/436: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=850)
157/437: model_3.fit(X_train, y_train)
157/438: plot_confusion_matrix(model_3, X_test, y_test)
157/439: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1150)
157/440: model_4.fit(X_train, y_train)
157/441: plot_confusion_matrix(model_4, X_test, y_test)
157/442:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/443:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/444: base = pd.read_csv("../dataset/observations.csv")
157/445: base.shape
157/446: base.head()
157/447:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/448:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/449:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/450: base.head()
157/451:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/452:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/453:
# correlation df
base.corr(numeric_only=True).TARGET
157/454: base = base.drop(columns=['direct_tilt', 'sacrum_angle'])
157/455: base
157/456: base = base.drop(columns=['class'])
157/457:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/458:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/459:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/460: X_train.shape
157/461: X_test.shape
157/462: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/463: model.fit(X_train, y_train)
157/464: plot_confusion_matrix(model, X_test, y_test)
157/465: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=500)
157/466: model_2.fit(X_train, y_train)
157/467: plot_confusion_matrix(model_2, X_test, y_test)
157/468: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=850)
157/469: model_3.fit(X_train, y_train)
157/470: plot_confusion_matrix(model_3, X_test, y_test)
157/471: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1150)
157/472: model_4.fit(X_train, y_train)
157/473: plot_confusion_matrix(model_4, X_test, y_test)
157/474:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/475:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/476: base = pd.read_csv("../dataset/observations.csv")
157/477: base.shape
157/478: base.head()
157/479:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/480:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/481:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/482: base.head()
157/483:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/484:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/485:
# correlation df
base.corr(numeric_only=True).TARGET
157/486: base = base.drop(columns=['sacrum_angle'])
157/487: base
157/488: base = base.drop(columns=['class'])
157/489:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/490:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/491:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/492: X_train.shape
157/493: X_test.shape
157/494: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/495: model.fit(X_train, y_train)
157/496: plot_confusion_matrix(model, X_test, y_test)
157/497: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=500)
157/498: model_2.fit(X_train, y_train)
157/499: plot_confusion_matrix(model_2, X_test, y_test)
157/500: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=850)
157/501: model_3.fit(X_train, y_train)
157/502: plot_confusion_matrix(model_3, X_test, y_test)
157/503: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1150)
157/504: model_4.fit(X_train, y_train)
157/505: plot_confusion_matrix(model_4, X_test, y_test)
157/506: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1100)
157/507: model_4.fit(X_train, y_train)
157/508: plot_confusion_matrix(model_4, X_test, y_test)
157/509: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1200)
157/510: model_4.fit(X_train, y_train)
157/511: plot_confusion_matrix(model_4, X_test, y_test)
157/512: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1250)
157/513: model_4.fit(X_train, y_train)
157/514: plot_confusion_matrix(model_4, X_test, y_test)
157/515:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/516:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/517: base = pd.read_csv("../dataset/observations.csv")
157/518: base.shape
157/519: base.head()
157/520:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/521:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/522:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/523: base.head()
157/524:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/525:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/526:
# correlation df
base.corr(numeric_only=True).TARGET
157/527: base = base.drop(columns=['sacrum_angle', 'direc_tilt', 'thoracic_slope'])
157/528:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/529:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/530: base = pd.read_csv("../dataset/observations.csv")
157/531: base.shape
157/532: base.head()
157/533:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/534:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/535:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/536: base.head()
157/537:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/538:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/539:
# correlation df
base.corr(numeric_only=True).TARGET
157/540: base = base.drop(columns=['sacrum_angle', 'direct_tilt', 'thoracic_slope'])
157/541: base
157/542: base = base.drop(columns=['class'])
157/543:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/544:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/545:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/546: X_train.shape
157/547: X_test.shape
157/548: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/549: model.fit(X_train, y_train)
157/550: plot_confusion_matrix(model, X_test, y_test)
157/551: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=500)
157/552: model_2.fit(X_train, y_train)
157/553: plot_confusion_matrix(model_2, X_test, y_test)
157/554: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=850)
157/555: model_3.fit(X_train, y_train)
157/556: plot_confusion_matrix(model_3, X_test, y_test)
157/557: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1100)
157/558: model_4.fit(X_train, y_train)
157/559: plot_confusion_matrix(model_4, X_test, y_test)
157/560: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=600)
157/561: model_2.fit(X_train, y_train)
157/562: plot_confusion_matrix(model_2, X_test, y_test)
157/563: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=500)
157/564: model_2.fit(X_train, y_train)
157/565: plot_confusion_matrix(model_2, X_test, y_test)
157/566: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550)
157/567: model_2.fit(X_train, y_train)
157/568: plot_confusion_matrix(model_2, X_test, y_test)
157/569: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900)
157/570: model_3.fit(X_train, y_train)
157/571: plot_confusion_matrix(model_3, X_test, y_test)
157/572: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1200)
157/573: model_4.fit(X_train, y_train)
157/574: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1250)
157/575: model_4.fit(X_train, y_train)
157/576: plot_confusion_matrix(model_4, X_test, y_test)
157/577:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/578:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/579: base = pd.read_csv("../dataset/observations.csv")
157/580: base.shape
157/581: base.head()
157/582:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/583:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/584:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/585: base.head()
157/586:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/587:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/588:
# correlation df
base.corr(numeric_only=True).TARGET
157/589: base = base.drop(columns=['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope'])
157/590: base
157/591: base = base.drop(columns=['class'])
157/592:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/593:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/594:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/595: X_train.shape
157/596: X_test.shape
157/597: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/598: model.fit(X_train, y_train)
157/599: plot_confusion_matrix(model, X_test, y_test)
157/600: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550)
157/601: model_2.fit(X_train, y_train)
157/602: plot_confusion_matrix(model_2, X_test, y_test)
157/603: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900)
157/604: model_3.fit(X_train, y_train)
157/605: plot_confusion_matrix(model_3, X_test, y_test)
157/606: model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1250)
157/607: model_4.fit(X_train, y_train)
157/608: plot_confusion_matrix(model_4, X_test, y_test)
157/609: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1250)
157/610: model_4.fit(X_train, y_train)
157/611: plot_confusion_matrix(model_4, X_test, y_test)
157/612: model_4 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1250)
157/613: model_4.fit(X_train, y_train)
157/614: plot_confusion_matrix(model_4, X_test, y_test)
157/615: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1250)
157/616: model_4.fit(X_train, y_train)
157/617: plot_confusion_matrix(model_4, X_test, y_test)
157/618: model_4 = MLPClassifier(hidden_layer_sizes=(5,3), activation="tanh", random_state=42, max_iter=1250)
157/619: model_4.fit(X_train, y_train)
157/620: plot_confusion_matrix(model_4, X_test, y_test)
157/621: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1250)
157/622: model_4.fit(X_train, y_train)
157/623: plot_confusion_matrix(model_4, X_test, y_test)
157/624:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/625:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/626: base = pd.read_csv("../dataset/observations.csv")
157/627: base.shape
157/628: base.head()
157/629:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/630:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/631:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/632: base.head()
157/633:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/634:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/635:
# correlation df
base.corr(numeric_only=True).TARGET
157/636: #base = base.drop(columns=['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope'])
157/637: base
157/638: base = base.drop(columns=['class'])
157/639:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/640:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/641:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/642: X_train.shape
157/643: X_test.shape
157/644: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/645: model.fit(X_train, y_train)
157/646: plot_confusion_matrix(model, X_test, y_test)
157/647: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550)
157/648: model_2.fit(X_train, y_train)
157/649: plot_confusion_matrix(model_2, X_test, y_test)
157/650: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900)
157/651: model_3.fit(X_train, y_train)
157/652: plot_confusion_matrix(model_3, X_test, y_test)
157/653: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1250)
157/654: model_4.fit(X_train, y_train)
157/655: plot_confusion_matrix(model_4, X_test, y_test)
157/656: sns.heatmap(dataframe.corr());
157/657: sns.heatmap(base.corr());
157/658: #base = base.drop(columns=['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope'])
157/659: sns.heatmap(base.corr(), annot=True);
157/660: sns.heatmap(base.corr(), annot=True, vmin=-1, vmax=1);
157/661:
plt.figure(figsize=(16, 6))
sns.heatmap(base.corr(), annot=True, vmin=-1, vmax=1);
157/662:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/663:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
irrel_columns= ['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope']
157/664:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/665:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/666: base = pd.read_csv("../dataset/observations.csv")
157/667: base.shape
157/668: base.head()
157/669:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/670:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/671:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/672: base.head()
157/673:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/674:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/675:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/676:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
irrel_columns= ['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope']
157/677: base = base.drop(columns=['class'])
157/678:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/679:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/680:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/681: X_train.shape
157/682: X_test.shape
157/683: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/684: model.fit(X_train, y_train)
157/685: plot_confusion_matrix(model, X_test, y_test)
157/686: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550)
157/687: model_2.fit(X_train, y_train)
157/688: plot_confusion_matrix(model_2, X_test, y_test)
157/689: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900)
157/690: model_3.fit(X_train, y_train)
157/691: plot_confusion_matrix(model_3, X_test, y_test)
157/692: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1250)
157/693: model_4.fit(X_train, y_train)
157/694: plot_confusion_matrix(model_4, X_test, y_test)
157/695:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/696:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/697: base = pd.read_csv("../dataset/observations.csv")
157/698: base.shape
157/699: base.head()
157/700:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/701:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/702:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/703: base.head()
157/704:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/705:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/706:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_onlye=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/707:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/708:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/709: base = pd.read_csv("../dataset/observations.csv")
157/710: base.shape
157/711: base.head()
157/712:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/713:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/714:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/715: base.head()
157/716:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/717:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/718:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/719:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
irrel_columns= ['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope']
157/720: base = base.drop(columns=['class'])
157/721:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/722:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/723:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/724: X_train.shape
157/725: X_test.shape
157/726: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/727: model.fit(X_train, y_train)
157/728: plot_confusion_matrix(model, X_test, y_test)
157/729: model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550)
157/730: model_2.fit(X_train, y_train)
157/731: plot_confusion_matrix(model_2, X_test, y_test)
157/732: model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900)
157/733: model_3.fit(X_train, y_train)
157/734: plot_confusion_matrix(model_3, X_test, y_test)
157/735: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1250)
157/736: model_4.fit(X_train, y_train)
157/737: plot_confusion_matrix(model_4, X_test, y_test)
157/738: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1300)
157/739: model_4.fit(X_train, y_train)
157/740: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1350)
157/741: model_4.fit(X_train, y_train)
157/742: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1400)
157/743: model_4.fit(X_train, y_train)
157/744: model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1450)
157/745: model_4.fit(X_train, y_train)
157/746: plot_confusion_matrix(model_4, X_test, y_test)
157/747:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_, ax=ax)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/748: fig, ax = plt.subplots(nrows=2, ncols=4)
157/749: fig, ax = plt.subplots(nrows=2, ncols=2)
157/750:
fig, ax = plt.subplots(nrows=2, ncols=2)
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
157/751:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, plot_confusion_matrix

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_, ax=ax)
    display.plot()
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/752:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    plt.title(type(model).__name__)
    plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/753:
fig, ax = plt.subplots(nrows=2, ncols=2)
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
157/754:
fig, ax = plt.subplots(nrows=2, ncols=2)
plt.figure(figsize=(16,16))
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
157/755:
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16,16))
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
157/756:
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,8))
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
157/757:
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,8))
plot_confusion_matrix(model, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_2, X_test, y_test, ax[0][1])
plot_confusion_matrix(model_3, X_test, y_test, ax[1][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[1][1])
157/758:
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,8))
plot_confusion_matrix(model, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_2, X_test, y_test, ax[0][1])
plot_confusion_matrix(model_3, X_test, y_test, ax[1][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[1][1])
plt.show()
157/759:
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,8))
plot_confusion_matrix(model, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_2, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_3, X_test, y_test, ax[1][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[1][1])
plt.show()
157/760:
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,8))
plot_confusion_matrix(model, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_2, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_3, X_test, y_test, ax[1][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plt.show()
157/761:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
157/762:
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,8))
plot_confusion_matrix(model, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_2, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_3, X_test, y_test, ax[1][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[0][0])
plt.show()
157/763:
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,8))
plot_confusion_matrix(model, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_2, X_test, y_test, ax[0][1])
plot_confusion_matrix(model_3, X_test, y_test, ax[1][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[1][1])
plt.show()
157/764:
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8,8))
ax[0][0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0][0])
plot_confusion_matrix(model_2, X_test, y_test, ax[0][1])
plot_confusion_matrix(model_3, X_test, y_test, ax[1][0])
plot_confusion_matrix(model_4, X_test, y_test, ax[1][1])
plt.show()
157/765:
fig, ax = plt.subplots(ncols=4, figsize=(8,8))
ax[0][0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/766:
fig, ax = plt.subplots(ncols=4, figsize=(8,8))
ax[0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/767:
fig, ax = plt.subplots(ncols=4, figsize=(10,8))
ax[0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/768:
fig, ax = plt.subplots(ncols=4, figsize=(16,8))
ax[0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/769:
fig, ax = plt.subplots(ncols=4, figsize=(16,16))
ax[0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/770:
fig, ax = plt.subplots(ncols=4, figsize=(6,16))
ax[0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/771:
fig, ax = plt.subplots(ncols=4, figsize=(6,16))
fig.tight_layout()
ax[0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/772:
fig, ax = plt.subplots(ncols=4, figsize=(6,6))
fig.tight_layout()
ax[0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/773:
fig, ax = plt.subplots(ncols=4, figsize=(20,6))
fig.tight_layout()
ax[0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/774:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()
ax[0].set_title('aaaa')
plot_confusion_matrix(model, X_test, y_test, ax[0])
plot_confusion_matrix(model_2, X_test, y_test, ax[1])
plot_confusion_matrix(model_3, X_test, y_test, ax[2])
plot_confusion_matrix(model_4, X_test, y_test, ax[3])
plt.show()
157/775:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]

for i in range(4):
    plot_confusion_matrix(models[i], X_test, y_test, ax[i])
    ax[i].set_title('model_' + str(i))

plt.show()
157/776:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
    
    return data
157/777:
a = {
    'a': 2,
    'aa': 2
}
157/778:
b = {
    'a': 3,
    'aa': 5
}
157/779: a + b
157/780: [a] + [b]
157/781: pd.DataFrame([a] + [b])
157/782: model.coefs_
157/783: model.n_layers_
157/784: model_2.n_layers_
157/785: model_2.hidden_layer_sizes
157/786: model_3.hidden_layer_sizes
157/787:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
    
    return data
157/788:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    print("Precision score: ", precision_score(y, y_pred))
    print("Recall score: ", recall_score(y, y_pred))
    print("F1 score: ", f1_score(y, y_pred))
    print("Accuracy score: ", accuracy_score(y, y_pred))
    print("ROC AUC score: ", roc_auc_score(y, y_pred_prob))
    print("\n\n")
    
    return data
157/789:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('model_' + str(i))

print(results)
plt.show()
157/790:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('model_' + str(i))

plt.show()
157/791: pd.DataFrame(results)
157/792:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/793:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/794:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/795: base = pd.read_csv("../dataset/observations.csv")
157/796: base.shape
157/797: base.head()
157/798:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/799:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/800:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/801: base.head()
157/802:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/803:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/804:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/805:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
irrel_columns= ['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope']
157/806: base = base.drop(columns=['class'])
157/807:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/808:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/809:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/810: X_train.shape
157/811: X_test.shape
157/812: model = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450)
157/813: model.fit(X_train, y_train)
157/814: plot_confusion_matrix(model, X_test, y_test)
157/815:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title(models[i].hidden_layers_size + str(i))

plt.show()
157/816:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title(models[i].hidden_layers_shape + str(i))

plt.show()
157/817:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title(models[i].hidden_layer_shape + str(i))

plt.show()
157/818:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title(models[i].hidden_layer_sizes + str(i))

plt.show()
157/819:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title(models[i].hidden_layer_sizes)

plt.show()
157/820:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ', models[i].hidden_layer_sizes)

plt.show()
157/821:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ', str(models[i].hidden_layer_sizes))

plt.show()
157/822:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/823: all_feat = pd.DataFrame(results)
157/824:
all_feat = pd.DataFrame(results)
all_feat
157/825:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=True)
all_feat
157/826:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/827: corr_base = base.loc[:, base.columns in irrel_columns]
157/828: corr_base = base.loc[:, irrel_columns]
157/829:
corr_base = base.loc[:, irrel_columns]
corr_base
157/830:
corr_base = base.loc[:, !irrel_columns]
corr_base
157/831:
corr_base = base.loc[:, not irrel_columns]
corr_base
157/832:
corr_base = base.loc[:, irrel_columns]
corr_base
157/833:
corr_base = base.loc[:, base.columns != irrel_columns]
corr_base
157/834:
corr_base = base.loc[:, base.columns - irrel_columns]
corr_base
157/835: ['a', 'b'] - ['a']
157/836:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_cols]]
corr_base
157/837:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base
157/838: irrel_columns
157/839:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/840:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/841: base = pd.read_csv("../dataset/observations.csv")
157/842: base.shape
157/843: base.head()
157/844:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/845:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/846:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc)

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/847: base.head()
157/848:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/849:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/850:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/851:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
irrel_columns= ['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope']
157/852: base = base.drop(columns=['class'])
157/853:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/854:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/855:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/856: X_train.shape
157/857: X_test.shape
157/858:
model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/859:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/860:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/861:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base
157/862:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/863:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/864:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1400).fit(X_train, y_train)
157/865:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/866:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/867:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/868:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/869:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/870:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/871:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/872:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/873:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/874:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/875:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/876:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/877:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/878: base = pd.read_csv("../dataset/observations.csv")
157/879: base.shape
157/880: base.head()
157/881:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/882:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/883:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, encoding_method='ordered')

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/884:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/885:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/886: base = pd.read_csv("../dataset/observations.csv")
157/887: base.shape
157/888: base.head()
157/889:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/890:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/891:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=['Abnormal', 'Normal'])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/892:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/893:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/894: base = pd.read_csv("../dataset/observations.csv")
157/895: base.shape
157/896: base.head()
157/897:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/898:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/899:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Abnormal', 'Normal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/900: base.head()
157/901:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/902:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/903: base = pd.read_csv("../dataset/observations.csv")
157/904: base.shape
157/905: base.head()
157/906:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/907:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/908:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/909: base.head()
157/910:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/911:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/912: base = pd.read_csv("../dataset/observations.csv")
157/913: base.shape
157/914: base.head()
157/915:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/916:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/917:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/918: base.head()
157/919:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/920:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/921:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/922:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
irrel_columns= ['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope']
157/923: base = base.drop(columns=['class'])
157/924:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/925:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/926:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/927: X_train.shape
157/928: X_test.shape
157/929:
model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/930:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/931:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/932:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base
157/933:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/934:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/935:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/936:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/937:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/938:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/939:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/940:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/941:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/942:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/943:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/944:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/945:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/946:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/947:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/948:
model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/949:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/950:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/951:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base
157/952:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/953:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/954:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/955:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/956:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/957: base = pd.read_csv("../dataset/observations.csv")
157/958: base.shape
157/959: base.head()
157/960:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/961:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/962:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/963: base.head()
157/964:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/965:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/966:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/967:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
#'pelvic_slope'
irrel_columns= ['sacrum_angle', 'direct_tilt', 'thoracic_slope']
157/968: base = base.drop(columns=['class'])
157/969:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/970:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/971:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/972: X_train.shape
157/973: X_test.shape
157/974:
model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/975:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/976:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/977:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base
157/978:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/979:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/980:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/981: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
157/982:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
#
irrel_columns= ['sacrum_angle', 'direct_tilt', 'thoracic_slope', 'pelvic_slope']
157/983: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
157/984:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
157/985: X_train_corr.shape
157/986: X_test_corr.shape
157/987:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
157/988:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/989:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/990: X_train_corr
157/991:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/992:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/993: base = pd.read_csv("../dataset/observations.csv")
157/994: base.shape
157/995: base.head()
157/996:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/997:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/998:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/999: base.head()
157/1000:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/1001:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/1002:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/1003:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
# 'pelvic_slope'
irrel_columns= ['sacrum_angle', 'direct_tilt', 'thoracic_slope']
157/1004: base = base.drop(columns=['class'])
157/1005:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/1006:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/1007:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/1008: X_train.shape
157/1009: X_test.shape
157/1010:
model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1011:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1012:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1013: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
157/1014:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
157/1015: X_train_corr.shape
157/1016: X_test_corr.shape
157/1017:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
157/1018:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1019:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1020:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1021:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/1022:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/1023: base = pd.read_csv("../dataset/observations.csv")
157/1024: base.shape
157/1025: base.head()
157/1026:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/1027:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/1028:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/1029: base.head()
157/1030:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/1031:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/1032:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/1033:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
# 'pelvic_slope', 'thoracic_slope'
irrel_columns= ['sacrum_angle', 'direct_tilt']
157/1034: base = base.drop(columns=['class'])
157/1035:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/1036:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/1037:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/1038: X_train.shape
157/1039: X_test.shape
157/1040:
model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1041:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1042:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1043: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
157/1044:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
157/1045: X_train_corr.shape
157/1046: X_test_corr.shape
157/1047:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
157/1048:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1049:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1050:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1051:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np
157/1052:
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/1053: base = pd.read_csv("../dataset/observations.csv")
157/1054: base.shape
157/1055: base.head()
157/1056:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/1057:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/1058:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/1059: base.head()
157/1060:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/1061:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/1062:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/1063:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
# 'pelvic_slope', 'thoracic_slope', 'direct_tilt'
irrel_columns= ['sacrum_angle']
157/1064: base = base.drop(columns=['class'])
157/1065:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/1066:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/1067:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/1068: X_train.shape
157/1069: X_test.shape
157/1070:
model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1071:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1072:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1073: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
157/1074:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
157/1075: X_train_corr.shape
157/1076: X_test_corr.shape
157/1077:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
157/1078:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1079:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1080:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1081:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(85,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1082:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1083:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1084:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1085:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1086:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1087:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1088:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1089:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1090:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1091:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1092:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1093:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(12,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1094:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1095:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1096:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1097:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1098:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1099:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(4,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1100:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1101:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1102:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(6,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1103:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1104:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1105:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1106:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1107:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1108:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1109:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1110:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1111:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1112:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1113:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1114:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(4,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1115:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1116:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1117:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(4,2), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1118:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1119:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1120:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(5,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1121:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1122:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1123:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
157/1124:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1125:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1126: sns.barplot(data=corr_feat, y='model_shape', x='roc')
157/1127: sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
157/1128:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xticks(range(100))
157/1129:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xticks(range(100))
plt.show()
157/1130:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xticks(range(1))
plt.show()
157/1131:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xticks(range(0, 1, 0.1))
plt.show()
157/1132:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xticks(np.arange(0, 1, 0.1))
plt.show()
157/1133:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xticks(np.arange(0, 1, 0.05))
plt.show()
157/1134:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xticks(np.arange(0, 1, 0.005))
plt.show()
157/1135:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', n_bootint=10)
plt.xticks(np.arange(0, 1, 0.005))
plt.show()
157/1136:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', n_boot=10)
plt.xticks(np.arange(0, 1, 0.005))
plt.show()
157/1137:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', n_boot=1000)
plt.xticks(np.arange(0, 1, 0.005))
plt.show()
157/1138:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=10)
plt.xticks(np.arange(0, 1, 0.005))
plt.show()
157/1139:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xticks(np.arange(0, 1, 0.005))
plt.show()
157/1140:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1).scale()
plt.xticks(np.arange(0, 1, 0.005))
plt.show()
157/1141:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xticks(np.arange(0.5, 1, 0.005))
plt.show()
157/1142:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xticks(np.arange(0.5, 1, 0.005))
plt.xlim((0.5,1))
plt.show()
157/1143:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xlim((0.5,1))
plt.show()
157/1144:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xlim((0.9,1))
plt.show()
157/1145:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xlim((0.91,1))
plt.show()
157/1146:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xlim((0.90001,1))
plt.show()
157/1147:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xlim((0.93,1))
plt.show()
157/1148:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xlim((0.94,1))
plt.show()
157/1149:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black', width=1)
plt.xlim((0.92,1))
plt.show()
157/1150:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.92,1))
plt.show()
157/1151:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.941,1))
plt.show()
157/1152:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.9,1))
plt.show()
157/1153:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xticks(np.arange(0, 1, 0.5))
plt.xlim((0.9,1))
plt.show()
157/1154:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xticks(np.arange(0.9, 1, 0.5))
plt.xlim((0.9,1))
plt.show()
157/1155:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')

plt.xlim((0.9,1))
plt.xticks(np.arange(0.9, 1, 0.5))
plt.show()
157/1156:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')

plt.xlim((0.9,1, 0.05))
plt.show()
157/1157:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')

plt.xlim((0.9,1))
plt.show()
157/1158:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')

plt.xlim((0,1))
plt.show()
157/1159:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')

plt.xlim((0.9234575,1))
plt.show()
157/1160:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')

plt.xlim((0.9235344575,1))
plt.show()
157/1161:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))
plt.xlim((0.9235344575,1))
plt.show()
157/1162:
from matplotlib.ticker import StrMethodFormatter
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))
plt.xlim((0.9235344575,1))
plt.show()
157/1163:
from matplotlib.ticker import StrMethodFormatter
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))
plt.xlim((0.9235344575,1))
plt.show()
157/1164:
from matplotlib.ticker import StrMethodFormatter
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))
plt.show()
157/1165:
from matplotlib.ticker import StrMethodFormatter
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.5f}'))
plt.show()
157/1166:
from matplotlib.ticker import StrMethodFormatter
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.5f}'))
plt.show()
157/1167:
from matplotlib.ticker import StrMethodFormatter
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))
plt.show()
157/1168:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))
plt.xlim((0.9235344575,1))
plt.show()
157/1169:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))
plt.xlim((0.9235344575,1))
plt.show()
157/1170:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))
plt.xlim((0.9235344575,1.01))
plt.show()
157/1171:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.3f}'))
plt.xlim((0.9235344575,1.121))
plt.show()
157/1172:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.9,1.121))
plt.show()
157/1173:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.9,0.955))
plt.show()
157/1174:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.9,0.9555))
plt.show()
157/1175:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.9,0.9535))
plt.show()
157/1176:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.9123,0.9535))
plt.show()
157/1177:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91233,0.9535))
plt.show()
157/1178:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91233,0.95235))
plt.show()
157/1179:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.912233,0.952325))
plt.show()
157/1180:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.912233,0.9523225))
plt.show()
157/1181:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.9123233,0.9523225))
plt.show()
157/1182:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.9523225))
plt.show()
157/1183:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.9523225))
plt.show()
157/1184:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
157/1185:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="logistic", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHOR MODELO
157/1186:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1187:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1188:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="logistic", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="logistic", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="logistic", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="logistic", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHOR MODELO
157/1189:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1190:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1191:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHOR MODELO
157/1192:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1193:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1194:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450, verbose=True).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1195:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1196:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1197:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1198:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
157/1199: np.linspace(0.1, 1.0, 10)
157/1200: train_sizes, train_scores, test_scores = learning_curve(estimator=corr_model_4, X=X_train_corr, y=y_train_corr, cv=10, train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=1)
157/1201:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.neural_network import MLPClassifier
import numpy as np
157/1202: train_sizes, train_scores, test_scores = learning_curve(estimator=corr_model_4, X=X_train_corr, y=y_train_corr, cv=10, train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=1)
157/1203:
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
#
# Plot the learning curve
#
plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')
plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')
plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')
plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')
plt.title('Learning Curve')
plt.xlabel('Training Data Size')
plt.ylabel('Model accuracy')
plt.grid()
plt.legend(loc='lower right')
plt.show()
157/1204:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450)
#.fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1205: train_sizes, train_scores, test_scores = learning_curve(estimator=corr_model_4, X=X_train_corr, y=y_train_corr, cv=10, train_sizes=np.linspace(0.1, 1.0, 10))
157/1206:
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
#
# Plot the learning curve
#
plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')
plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')
plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')
plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')
plt.title('Learning Curve')
plt.xlabel('Training Data Size')
plt.ylabel('Model accuracy')
plt.grid()
plt.legend(loc='lower right')
plt.show()
157/1207: train_sizes, train_scores, test_scores = learning_curve(estimator=corr_model_4, X=X_train_corr, y=y_train_corr, cv=5, train_sizes=np.linspace(0.1, 5.0, 10))
157/1208: train_sizes, train_scores, test_scores = learning_curve(estimator=corr_model_4, X=X_train_corr, y=y_train_corr, cv=5, train_sizes=np.linspace(0.1, 1.0, 10))
157/1209:
corr_model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1210:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1211:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1212:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
157/1213: corr_model_4.loss_
157/1214: corr_model_4.best_loss_
157/1215: corr_model_4.loss_curve
157/1216: corr_model_4.loss_curve_
157/1217: len(corr_model_4.loss_curve_)
157/1218: losses = corr_model_4.loss_curve_
157/1219: losses_4 = corr_model_4.loss_curve_
157/1220: sns.lineplot(losses_4)
157/1221: corr_model_4.validation_scores_
157/1222: corr_model_4.validation_scores_
157/1223: type(corr_model_4.validation_scores_)
157/1224:
losses_4 = corr_model_4.loss_curve_
losses_3, corr_model_3.loss_curve_
157/1225:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
157/1226:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
157/1227:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve
losses = corr_model.loss_curve
157/1228:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve
losses = corr_model.loss_curve_
157/1229:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve_
losses = corr_model.loss_curve_
157/1230:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_3)
157/1231:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_2)
sns.lineplot(losses)
157/1232:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, plot_roc_curve
from sklearn.neural_network import MLPClassifier
import numpy as np
157/1233:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay
from sklearn.neural_network import MLPClassifier
import numpy as np
157/1234:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
157/1235:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='MLP')
display.plot()
157/1236:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='MLP')
display.plot()
print(roc_auc)
157/1237:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='MLP')
display.plot()
print(fpr)
157/1238:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
#display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='MLP')
#display.plot()
plt.plot(fpr, tpr)
print(fpr)
157/1239:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
#display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='MLP')
#display.plot()
plt.plot(fpr, tpr, label="MLP (" + corr_model_4.hidden_layer_sizes + "), AUC="+str(auc))
print(fpr)
157/1240:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
#display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='MLP')
#display.plot()
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(auc))
print(fpr)
157/1241:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(auc))

plt.show()
157/1242:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(auc))

y_pred_prob = corr_model_3.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(auc))

plt.show()
157/1243:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(auc))

y_pred_prob = corr_model_3.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_3.hidden_layer_sizes) + "), AUC="+str(auc))

plt.show()
157/1244:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(auc))

y_pred_prob = corr_model_3.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_3.hidden_layer_sizes) + "), AUC="+str(auc))

plt.legend()
plt.show()
157/1245:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = corr_model_3.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_3.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.show()
157/1246:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = corr_model_2.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_2.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.show()
157/1247:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = corr_model_2.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.show()
157/1248:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.show()
157/1249:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.show()
157/1250:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = model.predict_proba(X_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.show()
157/1251:
y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = model.predict_proba(X_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.show()
157/1252:
y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = model.predict_proba(X_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.show()
print(len(y_pred_prob))
157/1253:
plt.figure(0).clf()
y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = model.predict_proba(X_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.show()
print(len(y_pred_prob))
157/1254:

y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = model.predict_proba(X_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.figure(figsize=(16,6))
plt.show()
print(len(y_pred_prob))
157/1255:

y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = model.predict_proba(X_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.figure(figsize=(20,6))
plt.show()
print(len(y_pred_prob))
157/1256:

y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

y_pred_prob = model.predict_proba(X_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(model.hidden_layer_sizes) + "), AUC="+str(roc_auc))

plt.legend()
plt.figure(figsize=(20,6))
plt.show()
print(len(fpr))
157/1257:

y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.figure(figsize=(20,6))
plt.show()
print(len(fpr))
157/1258:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1259:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1260:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1261:
model   = MLPClassifier(hidden_layer_sizes=(32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1262:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1263:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1264:
model   = MLPClassifier(hidden_layer_sizes=(128,64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1265:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1266:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1267:
model   = MLPClassifier(hidden_layer_sizes=(64,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1268:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1269:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1270:
model   = MLPClassifier(hidden_layer_sizes=(64,32,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1271:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1272:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1273:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1274:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1275:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1276:
model   = MLPClassifier(hidden_layer_sizes=(32,32,16), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1277:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1278:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1279:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1280:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1281:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1282:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1283:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1284:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1285:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1286:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1287:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1288:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(8,4,2), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1289:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1290:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1291:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1292:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1293:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1294:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1295:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1296:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1297:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1298:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1299:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1300:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1301:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1302:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
157/1303:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve_
losses = corr_model.loss_curve_
157/1304:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_2)
sns.lineplot(losses)
157/1305:
y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.figure(figsize=(20,6))
plt.show()
print(len(fpr))
157/1306:
y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.figure(figsize=(20,6))
plt.show()
157/1307:
y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.show()
157/1308:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
157/1309:


def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
157/1310: base = pd.read_csv("../dataset/observations.csv")
157/1311: base.shape
157/1312: base.head()
157/1313:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
157/1314:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
157/1315:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
157/1316: base.head()
157/1317:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
157/1318:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
157/1319:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
157/1320:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
# 'pelvic_slope', 'thoracic_slope', 'direct_tilt'
irrel_columns= ['sacrum_angle']
157/1321: base = base.drop(columns=['class'])
157/1322:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
157/1323:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
157/1324:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
157/1325: X_train.shape
157/1326: X_test.shape
157/1327:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1328:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1329:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1330: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
157/1331:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
157/1332: X_train_corr.shape
157/1333: X_test_corr.shape
157/1334:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
157/1335:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1336:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1337:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1338:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
157/1339:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve_
losses = corr_model.loss_curve_
157/1340:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_2)
sns.lineplot(losses)
157/1341:
y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.show()
157/1342:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(8), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1343:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1344:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1345:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(12), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1346:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1347:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1348:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(16), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
157/1349:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1350:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1351:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(16), activation="tanh", random_state=42, max_iter=1500).fit(X_train, y_train)
157/1352:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1353:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(16), activation="tanh", random_state=42, max_iter=1550).fit(X_train, y_train)
157/1354:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1355:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
157/1356:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1357:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1358:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
157/1359:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1360:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1361:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(64), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
157/1362:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1363:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1364:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
157/1365:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
157/1366:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
157/1367:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1368:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1369:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1370:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1371:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1372:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1373:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1374:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1375:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1376:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1377:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1378:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1379:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
157/1380:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
157/1381: pearsonr(base.sacrum_angle, base.TARGET)
157/1382:
for col in base.columns:
    print(pearsonr(base['col'], base.TARGET))
157/1383:
for col in base.columns:
    print(pearsonr(base[col], base.TARGET))
157/1384:
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    print(type(res))
157/1385:
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    print(res.statistic)
157/1386:
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    print(res)
157/1387:
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        print(res)
157/1388:
irrel_columns=[]
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns += col
irrel_columns
157/1389:
irrel_columns=[""]
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns += col
irrel_columns
157/1390:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
# 'pelvic_slope', 'thoracic_slope', 'direct_tilt'
irrel_columns= ['sacrum_angle']
[] + ""
157/1391:
irrel_columns=[""]
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
157/1392:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
# 'pelvic_slope', 'thoracic_slope', 'direct_tilt'
irrel_columns= ['sacrum_angle']
157/1393:
irrel_columns=[]
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
157/1394:
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
157/1395: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
157/1396:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
157/1397: X_train_corr.shape
157/1398: X_test_corr.shape
157/1399:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
157/1400:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
#corr_model_4 = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1401:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1402:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1403:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
#corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(8), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1404:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1405:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1406:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
#corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1407:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1408:
corr_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
corr_feat
157/1409:
corr_feat = pd.DataFrame(results).sort_values(by=['roc', 'accuracy'], ascending=False)
corr_feat
157/1410:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
#corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1411:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1412:
corr_feat = pd.DataFrame(results).sort_values(by=['roc', 'accuracy'], ascending=False)
corr_feat
157/1413:
corr_feat = pd.DataFrame(results).sort_values(by=['accuracy','roc'], ascending=False)
corr_feat
157/1414:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
#corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1415:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1416:
corr_feat = pd.DataFrame(results).sort_values(by=['accuracy','roc'], ascending=False)
corr_feat
157/1417:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
157/1418:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
#corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=((16)), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1419:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1420:
corr_feat = pd.DataFrame(results).sort_values(by=['accuracy','roc'], ascending=False)
corr_feat
157/1421:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
157/1422:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
#corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
157/1423:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
157/1424:
corr_feat = pd.DataFrame(results).sort_values(by=['accuracy','roc'], ascending=False)
corr_feat
157/1425:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
158/1:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
158/2:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
158/3:


def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
158/4: base = pd.read_csv("../dataset/observations.csv")
158/5: base.shape
158/6: base.head()
158/7:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
158/8:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
158/9:
# Ordinal encoding for target feature
# 0 --> Abnormal
# 1 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
158/10: base.head()
158/11:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
158/12:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
158/13:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
158/14:
# features whose correlation coefficient regarding TARGET is such that abs(cc) < 0.06
# 'pelvic_slope', 'thoracic_slope', 'direct_tilt'
irrel_columns= ['sacrum_angle']
158/15:
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
159/1:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
159/2:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
159/3:


def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
159/4: base = pd.read_csv("../dataset/observations.csv")
159/5: base.shape
159/6: base.head()
159/7:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
159/8:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
159/9:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
base = base.drop(columns=['class'])
159/10: base.head()
159/11:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
159/12:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
160/1:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
160/2:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
160/3:


def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
160/4: base = pd.read_csv("../dataset/observations.csv")
160/5: base.shape
160/6: base.head()
160/7:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
160/8:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
160/9:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
160/10: base.head()
160/11:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
160/12:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j == 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
160/13: base = base.drop(columns=['class'])
160/14:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
160/15:
# features whose correlation coefficient are close to 0 and statistically significant
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
160/16:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
160/17:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
160/18:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
160/19: X_train.shape
160/20: X_test.shape
160/21:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
160/22:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
160/23:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
160/24: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
160/25:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
160/26: X_train_corr.shape
160/27: X_test_corr.shape
160/28:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
160/29:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
#corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
160/30:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
160/31:
corr_feat = pd.DataFrame(results).sort_values(by=['accuracy','roc'], ascending=False)
corr_feat
160/32:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
160/33:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve_
losses = corr_model.loss_curve_
160/34:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_2)
sns.lineplot(losses)
160/35:
y_pred_prob = corr_model.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.show()
160/36:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.show()
162/1:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
162/2:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
162/3:
def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
162/4: base = pd.read_csv("../dataset/observations.csv")
162/5: base.shape
162/6: base.head()
162/7:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
162/8:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
162/9:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
162/10: base.head()
162/11:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
162/12:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j >= 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 2

plt.show()
162/13:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j >= 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 2

plt.show()
162/14:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j >= 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 2

plt.show()
162/15:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j >= 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 2

plt.show()
162/16:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j >= 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j+2])
    j += 1

plt.show()
162/17:
fig, ax = plt.subplots(nrows=11, ncols=6, figsize=(34,34))

i=0
j=0
for pair in pairs:
    if j >= 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
162/18:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=2
for k in range(0, len(pairs), 2):
    print(k)
    
for k, pair in enumerate(pairs):
    
    if j >= 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
162/19:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=2
for k in range(0, len(pairs)+2, 2):
    print(k)
    
for k, pair in enumerate(pairs):
    
    if j >= 6:
        j=0
        i += 1
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
162/20:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=2
for k in range(0, len(pairs)+2, 2):
    if j >= 3:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/21:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(0, len(pairs)+2, 2):
    if j >= 3:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/22:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(0, len(pairs)+1, 2):
    if j >= 3:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/23:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(0, len(pairs)+1, 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/24:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(0, len(pairs), 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/25:
fig, ax = plt.subplots(nrows=11, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(1, len(pairs), 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/26:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(1, 15, 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/27:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(1, 30, 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/28:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(0, 30, 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/29:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(5, 35, 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/30:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(10, 40, 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/31:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(8, 38, 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/32:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(4, 34, 2):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/33:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 3:
        j=0
        i += 1
    print(k)
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/34:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(34,34))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 3:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/35:
fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(10,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 3:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/36:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(10,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/37:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(60,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/38:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(16,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/39:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(10,16))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/40:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(20,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/41:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/42: sns.histplot(data=base, x="pelvic_incidence", bins=10, kde=True)
162/43: sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/44: #sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/45: base.shape
162/46:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for col in base.columns:
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=col, bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/47:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for col in base.columns:
    print(i, j)
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=col, bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/48:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for col in base.columns:
    if j >= 4:
        j = 0
        i += 1
    print(i, j)
    sns.histplot(data=base, x=col, bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/49:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for i in range(12):
    if j >= 4:
        j = 0
        i += 1
    print(i, j)
    sns.histplot(data=base, x=base.columns[i], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/50:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    print(i, j)
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/51:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/52: base.describe()
162/53:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
162/54:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
162/55:
def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
162/56: base = pd.read_csv("../dataset/observations.csv")
162/57: base.shape
162/58: base.head()
162/59:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
162/60:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
162/61:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
162/62: base.describe()
162/63:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
162/64:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
162/65:
def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
162/66: base = pd.read_csv("../dataset/observations.csv")
162/67: base.shape
162/68: base.head()
162/69:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
162/70:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
162/71:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
162/72: base.describe()
162/73:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
162/74:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
162/75:
def plot_confusion_matrix(model, X, y, ax):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
162/76: base = pd.read_csv("../dataset/observations.csv")
162/77: base.shape
162/78: base.head()
162/79:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
162/80:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
162/81:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
162/82: base.describe()
162/83:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
162/84:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/85:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/86: base = base.drop(columns=['class'])
162/87:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
162/88:
# features whose correlation coefficient are close to 0 and statistically significant
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
162/89:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
162/90:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
162/91:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
162/92: X_train.shape
162/93: X_test.shape
162/94:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/95:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
162/96:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
162/97: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
162/98:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
162/99: X_train_corr.shape
162/100: X_test_corr.shape
162/101:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
162/102:
corr_model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
#corr_model_4 = MLPClassifier(hidden_layer_sizes=(10,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
corr_model_4 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/103:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

corr_models = [corr_model, corr_model_2, corr_model_3, corr_model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(corr_models[i], X_test_corr, y_test_corr, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(corr_models[i].hidden_layer_sizes))

plt.show()
162/104:
corr_feat = pd.DataFrame(results).sort_values(by=['accuracy','roc'], ascending=False)
corr_feat
162/105:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
162/106:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve_
losses = corr_model.loss_curve_
162/107:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_2)
sns.lineplot(losses)
162/108:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.show()
162/109:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=450).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/110:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=900).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/111:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=500).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/112:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=600).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/113:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=700).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/114:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/115:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=750).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/116:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/117:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=800).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=800).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/118: model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train)
162/119:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=850).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=800).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/120:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=900).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=800).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/121:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=950).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=800).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/122:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1000).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=800).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/123:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=800).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/124:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=1100).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/125:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=1000).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/126:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/127:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=800).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/128:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=850).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/129:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
162/130:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=850).fit(X_train, y_train)
162/131:
model   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/132:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/133:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/134: str(model_id)
162/135: model_id.__name__
162/136:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model_id, model_log, model_relu, model_tanh]
m_names = ['model_id', 'model_log', 'model_relu', 'model_tanh']
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/137: pd.DataFrame(results).sort_values(by='roc', ascending=False)
162/138:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/139:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model_id, model_log, model_relu, model_tanh]
m_names = ['model_id', 'model_log', 'model_relu', 'model_tanh']
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/140: pd.DataFrame(results).sort_values(by='roc', ascending=False)
162/141:
def plot_confusion_matrix(model, X, y, ax, model_name):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'accuracy': accuracy_score(y, y_pred),
        'roc': roc_auc_score(y, y_pred_prob)
    }
    
    return data
162/142:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/143:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model_id, model_log, model_relu, model_tanh]
m_names = ['model_id', 'model_log', 'model_relu', 'model_tanh']
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_name)]
    ax[i].set_title(m_names[i])

plt.show()
162/144:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model_id, model_log, model_relu, model_tanh]
m_names = ['model_id', 'model_log', 'model_relu', 'model_tanh']
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/145: pd.DataFrame(results).sort_values(by='roc', ascending=False)
162/146:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=300).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/147:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=400).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/148:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=500).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/149:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=600).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/150:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=700).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/151:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/152:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=750).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/153:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/154:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=700).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/155:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=800).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/156:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=850).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/157:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=900).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/158:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1000).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/159:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/160:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1050).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=300).fit(X_train, y_train)
162/161:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1050).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=700).fit(X_train, y_train)
162/162:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1050).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=800).fit(X_train, y_train)
162/163:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=300).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/164:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=400).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/165:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=500).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/166:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=600).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/167:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/168:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=900).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/169:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=1000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/170:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=1100).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/171:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=1200).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/172:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=1300).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/173:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/174:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=800).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/175:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=700).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/176:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=600).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/177:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=500).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/178:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=400).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/179:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=300).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/180:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300).fit(X_train, y_train)
162/181:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=700).fit(X_train, y_train)
162/182:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=800).fit(X_train, y_train)
162/183:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=850).fit(X_train, y_train)
162/184:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/185:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2500).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/186:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=4000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/187:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=3000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/188:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2000).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/189:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2500).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/190:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2750).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/191:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2800).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/192:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2900).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/193:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/194:
fig, ax = plt.subplots(ncols=3, figsize=(20,4))
fig.tight_layout()

models = [model_lbfgs, model_sgd, model_adam]
m_names = ['model_lbfgs', 'model_sgd', 'model_adam']
results = []

for i in range(3):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/195: pd.DataFrame(results).sort_values(by='roc', ascending=False)
162/196:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.1).fit(X_train, y_train)
162/197:
fig, ax = plt.subplots(ncols=3, figsize=(20,4))
fig.tight_layout()

models = [model_1, model_2, model_3]
m_names = ['model_1', 'model_2', 'model_3']
results = []

for i in range(3):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/198:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/199:
fig, ax = plt.subplots(ncols=3, figsize=(20,4))
fig.tight_layout()

models = [model_1, model_2, model_3]
m_names = ['model_1', 'model_2', 'model_3']
results = []

for i in range(3):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/200:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/201:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=400, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/202:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=350, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/203:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=325, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/204:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/205:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=325, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/206:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=400, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=325, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/207:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=500, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=325, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/208:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=600, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=325, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/209:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=700, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=325, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/210:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=800, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=325, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/211:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=325, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/212:
fig, ax = plt.subplots(ncols=3, figsize=(20,4))
fig.tight_layout()

models = [model_1, model_2, model_3]
m_names = ['model_1', 'model_2', 'model_3']
results = []

for i in range(3):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/213: pd.DataFrame(results).sort_values(by='roc', ascending=False)
162/214:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42, max_iter=1600, learning_rate_init=0.1).fit(X_train, y_train)
162/215:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
162/216:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
m_names = ['model', 'model_2', 'model_3', 'model_4']
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
162/217:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
162/218:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900, learning_rate_init=0.001).fit(X_train, y_train) 
model_2 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=325, learning_rate_init=0.01).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=300, learning_rate_init=0.1).fit(X_train, y_train)
162/219:
fig, ax = plt.subplots(ncols=3, figsize=(20,4))
fig.tight_layout()

models = [model_1, model_2, model_3]
m_names = ['model_1', 'model_2', 'model_3']
results = []

for i in range(3):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/220: pd.DataFrame(results).sort_values(by='roc', ascending=False)
162/221:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42, max_iter=1600, learning_rate_init=0.01).fit(X_train, y_train)
162/222:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
m_names = ['model', 'model_2', 'model_3', 'model_4']
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
162/223:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
162/224:
model   = MLPClassifier(hidden_layer_sizes=(64,32,16,8), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train) #melhores modelos
#model_2 = MLPClassifier(hidden_layer_sizes=(16,8,4), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
#model_2 = MLPClassifier(hidden_layer_sizes=(64,32), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
#model_4 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32), activation="tanh", random_state=42, max_iter=1600, learning_rate_init=0.001).fit(X_train, y_train)
162/225:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model, model_2, model_3, model_4]
m_names = ['model', 'model_2', 'model_3', 'model_4']
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title('hidden_layer_size: ' + str(models[i].hidden_layer_sizes))

plt.show()
162/226:
all_feat = pd.DataFrame(results).sort_values(by='roc', ascending=False)
all_feat
162/227:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/228:
fig, ax = plt.subplots(ncols=4, figsize=(20,4))
fig.tight_layout()

models = [model_id, model_log, model_relu, model_tanh]
m_names = ['model_id', 'model_log', 'model_relu', 'model_tanh']
results = []

for i in range(4):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/229: pd.DataFrame(results).sort_values(by='roc', ascending=False)
162/230:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/231:
fig, ax = plt.subplots(ncols=3, figsize=(20,4))
fig.tight_layout()

models = [model_lbfgs, model_sgd, model_adam]
m_names = ['model_lbfgs', 'model_sgd', 'model_adam']
results = []

for i in range(3):
    results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
    ax[i].set_title(m_names[i])

plt.show()
162/232: pd.DataFrame(results).sort_values(by='roc', ascending=False)
162/233:
def plot_model_stats():
    fig, ax = plt.subplots(ncols=3, figsize=(20,4))
    fig.tight_layout()

    models = [model_lbfgs, model_sgd, model_adam]
    m_names = ['model_lbfgs', 'model_sgd', 'model_adam']
    results = []

    for i in range(3):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
        ax[i].set_title(m_names[i])

    plt.show()
162/234:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/235: plot_model_stats()
162/236:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test):
    fig, ax = plt.subplots(ncols=3, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(3):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], m_names[i])]
        ax[i].set_title(m_names[i])

    plt.show()
    return results
162/237:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test):
    fig, ax = plt.subplots(ncols=3, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(3):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i])]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/238: plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/239: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/240: results
162/241: pd.DataFrame(results)
162/242: pd.DataFrame(results).sort_values(by=['recall', 'roc'], ascending=False)
162/243: pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
162/244: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/245:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test):
    fig, ax = plt.subplots(ncols=3, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i])]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/246:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/247: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/248:
def plot_model_stats(ncols, models, model_names, X_test, y_test):
    fig, ax = plt.subplots(ncols=3, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(ncols):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i])]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/249:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/250: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/251:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i])]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/252:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/253: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/254: pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
162/255:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/256: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/257: pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
162/258:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/259:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=550).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/260:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=650).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/261:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=890).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/262:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/263:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=470).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/264:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=700).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/265:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=800).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/266:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=850).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/267:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=8750).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/268:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=875).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/269:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/270:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=910).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/271:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=9250).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/272:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=925).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/273:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=950).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/274:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/275:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1000).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/276:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1100).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/277:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1200).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/278:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/279:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1400).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/280:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1300).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/281:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1200).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/282:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1250).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/283:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/284:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/285:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1300).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/286:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/287:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1450).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/288:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1550).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/289:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1690).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/290:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=450).fit(X_train, y_train)
162/291:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/292: results = plot_model_stats(3, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/293: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/294: pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
162/295: sns.barplot(data=results, x="model_shape", y="recall")
162/296: df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
162/297:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df
162/298: sns.barplot(data=df, x="model_shape", y="recall")
162/299: sns.barplot(data=df, x="model_shape", y="recall", edgecolor='black')
162/300: sns.barplot(data=df, x="model_shape", y=["recall", 'precision'], edgecolor='black')
162/301: sns.barplot(data=df, x="model_shape", y="recall", edgecolor='black', hue='precision')
162/302: df = df.melt('model_shape', var_name='score', value_name='Val')
162/303:
df = df.melt('model_shape', var_name='score', value_name='Val')
df
162/304:
df = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc'])
df
162/305:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df
162/306:
df = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc'])
df
162/307:
df = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name')
df
162/308:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df
162/309:
df = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name')
df
162/310:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df
162/311:
df = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/312:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/313: sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
162/314: sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', figsize=(10,5))
162/315:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', figsize=(10,5))
plt.figure(figsize=(10,5))
162/316:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.figure(figsize=(10,5))
162/317:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.figure(figsize=(16,5))
162/318:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.figsize(figsize=(16,5))
162/319:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.figsize((16,5))
162/320:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.figure().set_figwidth(15)
162/321:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.figure().set_figwidth(20)
162/322:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.xlim((0.7,0.95))
plt.figure().set_figwidth(20)
162/323:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,0.95))
plt.figure().set_figwidth(20)
162/324:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,0.99))
plt.figure().set_figwidth(20)
162/325:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.5))
plt.figure().set_figwidth(20)
162/326:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.figure().set_figwidth(20)
162/327:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.yticks(np.arange(0.7, 1))
plt.figure().set_figwidth(20)
162/328:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.yticks(np.arange(0.7, 1.))
plt.figure().set_figwidth(20)
162/329:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.yticks(np.arange(0.7, 1.0))
plt.figure().set_figwidth(20)
162/330:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.yticks(np.arange(0.7, 1.0, 0.5))
plt.figure().set_figwidth(20)
162/331:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.yticks(np.arange(0, 1, 0.5))
plt.figure().set_figwidth(20)
162/332:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.yticks(np.arange(0, 1.5, 0.5))
plt.figure().set_figwidth(20)
162/333:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.yticks(np.arange(0, 1.5, 0.25))
plt.figure().set_figwidth(20)
162/334:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.yticks(np.arange(0, 2.5, 0.25))
plt.figure().set_figwidth(20)
162/335:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.figure().set_figwidth(20)
162/336:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
162/337:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.figure().set_figwidth(20)
plt.show()
162/338:
sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.show()
162/339:
fig, ax = plt.subplots(ncols=2)

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable')
plt.ylim((0.7,1.1))
plt.show()
162/340:
fig, ax = plt.subplots(ncols=2)

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
plt.ylim((0.7,1.1))
plt.show()
162/341:
fig, ax = plt.subplots(ncols=2, figsize(12, 6))

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
plt.ylim((0.7,1.1))
plt.show()
162/342:
fig, ax = plt.subplots(ncols=2, figsize=(12, 6))

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
plt.ylim((0.7,1.1))
plt.show()
162/343:
fig, ax = plt.subplots(ncols=2, figsize=(6, 16))

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
plt.ylim((0.7,1.1))
plt.show()
162/344:
fig, ax = plt.subplots(ncols=2, figsize=(12, 8))

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
plt.ylim((0.7,1.1))
plt.show()
162/345:
fig, ax = plt.subplots(ncols=2, figsize=(18, 8))

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
plt.ylim((0.7,1.1))
plt.show()
162/346:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
plt.ylim((0.7,1.1))
plt.show()
162/347:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))
plt.show()
162/348:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(font_size)

plt.show()
162/349:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/350: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/351:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
162/352:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(font_size)

plt.show()
162/353:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(14)

plt.show()
162/354:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(8)

plt.show()
162/355:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/356:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/357:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.5, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/358:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/359:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 3, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/360:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/361:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/362:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/363:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df.precision = round(df.precision, 2)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
162/364:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/365:
def plot_confusion_matrix(model, X, y, ax, model_name):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'f1': round(f1_score(y, y_pred), 4),
        'accuracy': round(accuracy_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
162/366:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df.precision = round(df.precision, 2)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
162/367:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/368:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/369: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/370:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df.precision = round(df.precision, 2)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
162/371:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/372:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
162/373:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/374:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/375:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=[df.columns != 'model_name'])
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/376:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/377:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=[col for col in df.columns if col != 'model_name'])
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/378:
fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
ax[0].set_ylim((0.7,1.1))

bbox=[0, 0, 1.2, 1]
ax[1].axis('off')
mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
mpl_table.auto_set_font_size(False)
mpl_table.set_fontsize(10)

plt.show()
162/379:
#fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

#sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
#ax[0].set_ylim((0.7,1.1))

#bbox=[0, 0, 1.2, 1]
#ax[1].axis('off')
#mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
#mpl_table.auto_set_font_size(False)
#mpl_table.set_fontsize(10)

#plt.show()
162/380:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/381:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/382: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/383:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/384:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_5 = MLPClassifier(hidden_layer_sizes=(64,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/385: results = plot_model_stats(4, [model_1, model_2, model_3, model_4, model_5], ['model_1', 'model_2', 'model_3', 'model_4', 'model_5'], X_test, y_test)
162/386: results = plot_model_stats(5, [model_1, model_2, model_3, model_4, model_5], ['model_1', 'model_2', 'model_3', 'model_4', 'model_5'], X_test, y_test)
162/387:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/388: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/389:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/390: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/391:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/392:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1000).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/393:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1100).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/394:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/395:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1500).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/396:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1400).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/397:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1200).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/398:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1100).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/399:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/400: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/401:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/402:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/403: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/404:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/405:
#fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

#sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
#ax[0].set_ylim((0.7,1.1))

#bbox=[0, 0, 1.2, 1]
#ax[1].axis('off')
#mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
#mpl_table.auto_set_font_size(False)
#mpl_table.set_fontsize(10)

#plt.show()
162/406:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/407: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/408:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/409: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
162/410:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
162/411: X_train_corr.shape
162/412: X_test_corr.shape
162/413:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
162/414:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=450).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/415:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=500).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/416:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/417:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter9700).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/418:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/419:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/420:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1200).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/421:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/422:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/423:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/424:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/425:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/426:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1200).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/427:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/428:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/429:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/430:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/431:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/432:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/433:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/434:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1350).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/435:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/436:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/437:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr) # MELHORes MODELO
162/438: results = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/439: df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
162/440:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df
162/441:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/442:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=16400).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/443:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/444:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/445:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1200).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/446:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/447:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/448:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/449:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
162/450:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
162/451:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
162/452:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
162/453:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
162/454:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
162/455:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
162/456:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
162/457:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=800).fit(X_train_corr, y_train_corr)
162/458:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
162/459:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=600).fit(X_train_corr, y_train_corr)
162/460:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=500).fit(X_train_corr, y_train_corr)
162/461:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/462:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=600).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/463:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=650).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/464:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/465:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=800).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/466:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=900).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/467:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/468: results = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/469:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df
162/470:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.recall + df.f1) / 2
df
162/471:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/472: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/473:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.recall + df.f1) / 2
df
162/474:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/475: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/476:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.recall + df.f1) / 2
df
162/477:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/478: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/479:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.recall + df.f1 + df.precision + df.accuracy) / 4
df
162/480:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/481: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/482:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.recall + df.f1 + df.precision + df.accuracy) / 4
df
162/483:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/484: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/485:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.accuracy) / 2
df
162/486:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/487: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/488:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.accuracy) / 2
df
162/489:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/490: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/491:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.accuracy) / 2
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/492:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/493: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/494:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.accuracy) / 2
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/495:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/496: results = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/497:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.accuracy) / 2
df
162/498:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/499: results = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/500:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.accuracy) / 2
df
162/501:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/502: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/503:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall) / 2
df
162/504:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/505: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/506:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall) / 2
df
162/507:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/508: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/509:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.accuracy + df.recall) / 2
df
162/510:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/511: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/512:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.accuracy + df.recall) / 2
df
162/513:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/514: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/515:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.roc + df.recall) / 2
df
162/516:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/517: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/518:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.roc + df.recall) / 2
df
162/519:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/520: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/521:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.roc + df.recall) / 2
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/522:
#fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

#sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
#ax[0].set_ylim((0.7,1.1))

#bbox=[0, 0, 1.2, 1]
#ax[1].axis('off')
#mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
#mpl_table.auto_set_font_size(False)
#mpl_table.set_fontsize(10)

#plt.show()
162/523:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/524: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/525:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.roc + df.recall) / 2
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/526:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/527: results = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/528:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.roc + df.recall) / 2
df
162/529:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/530: results = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/531:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.roc + df.recall) / 2
df
162/532:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/533: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/534:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.accuracy + df.recall) / 2
df
162/535:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/536: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/537:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.accuracy + df.recall) / 2
df
162/538:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/539: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/540:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/541:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/542: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/543:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/544:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/545: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/546:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/547:
model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/548: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/549:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/550:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/551: results = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/552:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/553:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/554: results = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/555:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/556:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
162/557:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
162/558:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i])]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/559:
def plot_confusion_matrix(model, X, y, ax, model_name):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'f1': round(f1_score(y, y_pred), 4),
        'accuracy': round(accuracy_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
162/560: base = pd.read_csv("../dataset/observations.csv")
162/561: base.shape
162/562: base.head()
162/563:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
162/564:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
162/565:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
162/566: base.describe()
162/567:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
162/568:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/569:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/570: base = base.drop(columns=['class'])
162/571:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
162/572:
# features whose correlation coefficient are close to 0 and statistically significant
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
162/573:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
162/574:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
162/575:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
162/576: X_train.shape
162/577: X_test.shape
162/578:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/579: results = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/580:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/581:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/582: results = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/583:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/584:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/585: results = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/586:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/587:
#fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

#sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
#ax[0].set_ylim((0.7,1.1))

#bbox=[0, 0, 1.2, 1]
#ax[1].axis('off')
#mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
#mpl_table.auto_set_font_size(False)
#mpl_table.set_fontsize(10)

#plt.show()
162/588:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/589: results = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test)
162/590:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/591: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
162/592:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
162/593: X_train_corr.shape
162/594: X_test_corr.shape
162/595:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
162/596:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/597: results = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/598:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/599:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/600: results = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr)
162/601:
df = pd.DataFrame(results).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/602:
sns.barplot(data=corr_feat, y='model_shape', x='roc', edgecolor='black')
plt.xlim((0.91,0.95))
plt.show()
162/603:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve_
losses = corr_model.loss_curve_
162/604:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_2)
sns.lineplot(losses)
162/605:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.show()
162/606: total_results = None
162/607:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/608: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/609:
df = pd.DataFrame(results_1).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/610:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/611: results_2 = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/612:
df = pd.DataFrame(results_2).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/613:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/614: results_3 = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/615:
df = pd.DataFrame(results_3).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/616:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/617: results_4 = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test)
162/618:
df = pd.DataFrame(results_4).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df_plot = pd.melt(df, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
df
162/619:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/620: results_5 = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/621:
df = pd.DataFrame(results_5).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/622:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/623: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr)
162/624:
df = pd.DataFrame(results_6).sort_values(by=['roc', 'recall'], ascending=False)
df['metric'] = (df.precision + df.recall + df.roc) / 3
df
162/625:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)
total_results
162/626:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
162/627:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
162/628:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i])]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/629:
def plot_confusion_matrix(model, X, y, ax, model_name):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'f1': round(f1_score(y, y_pred), 4),
        'accuracy': round(accuracy_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
162/630: base = pd.read_csv("../dataset/observations.csv")
162/631: base.shape
162/632: base.head()
162/633:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
162/634:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
162/635:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
162/636: base.describe()
162/637:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
162/638:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/639:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/640: base = base.drop(columns=['class'])
162/641:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
162/642:
# features whose correlation coefficient are close to 0 and statistically significant
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
162/643:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
162/644:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
162/645:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
162/646: X_train.shape
162/647: X_test.shape
162/648: total_results = None
162/649:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/650: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test)
162/651:
results_1 = pd.DataFrame(results_1).sort_values(by=['roc', 'recall'], ascending=False)
results_1['metric'] = (results_1.precision + results_1.recall + results_1.roc) / 3
results_1
162/652:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/653: results_2 = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test)
162/654:
results_2 = pd.DataFrame(results_2).sort_values(by=['roc', 'recall'], ascending=False)
results_2['metric'] = (results_2.precision + results_2.recall + results_2.roc) / 3
results_2
162/655:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/656: results_3 = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test)
162/657:
results_3 = pd.DataFrame(results_3).sort_values(by=['roc', 'recall'], ascending=False)
results_3['metric'] = (results_3.precision + results_3.recall + results_3.roc) / 3
df_plot = pd.melt(results_3, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
results_3
162/658:
#fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

#sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
#ax[0].set_ylim((0.7,1.1))

#bbox=[0, 0, 1.2, 1]
#ax[1].axis('off')
#mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
#mpl_table.auto_set_font_size(False)
#mpl_table.set_fontsize(10)

#plt.show()
162/659:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/660: results_4 = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test)
162/661:
results_4 = pd.DataFrame(results_4).sort_values(by=['roc', 'recall'], ascending=False)
results_4['metric'] = (results_4.precision + results_4.recall + results_4.roc) / 3
df_plot = pd.melt(results_4, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
results_4
162/662: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
162/663:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
162/664: X_train_corr.shape
162/665: X_test_corr.shape
162/666:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
162/667:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/668: results_5 = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr)
162/669:
results_5 = pd.DataFrame(results_5).sort_values(by=['roc', 'recall'], ascending=False)
results_5['metric'] = (results_5.precision + results_5.recall + results_5.roc) / 3
results_5
162/670:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/671: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr)
162/672:
results_6 = pd.DataFrame(results_6).sort_values(by=['roc', 'recall'], ascending=False)
results_6['metric'] = (results_6.precision + results_6.recall + results_6.roc) / 3
results_6
162/673:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)
total_results
162/674:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve_
losses = corr_model.loss_curve_
162/675:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_2)
sns.lineplot(losses)
162/676:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.show()
162/677:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0).reset_index(drop=True)
total_results
162/678:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0).reset_index(drop=True)
total_results.shape
162/679:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='metric')
total_results
162/680:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='metric', ascending=False)
total_results
162/681:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'f1': round(f1_score(y, y_pred), 4),
        'accuracy': round(accuracy_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
162/682:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
162/683:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
162/684:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i])]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/685:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'f1': round(f1_score(y, y_pred), 4),
        'accuracy': round(accuracy_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
162/686: base = pd.read_csv("../dataset/observations.csv")
162/687: base.shape
162/688: base.head()
162/689:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
162/690:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
162/691:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
162/692: base.describe()
162/693:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
162/694:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/695:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/696: base = base.drop(columns=['class'])
162/697:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
162/698:
# features whose correlation coefficient are close to 0 and statistically significant
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
162/699:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
162/700:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
162/701:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
162/702: X_train.shape
162/703: X_test.shape
162/704: total_results = None
162/705:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/706: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
162/707:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/708:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
162/709:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
162/710:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/711:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'f1': round(f1_score(y, y_pred), 4),
        'accuracy': round(accuracy_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
162/712: base = pd.read_csv("../dataset/observations.csv")
162/713: base.shape
162/714: base.head()
162/715:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
162/716:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
162/717:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
162/718: base.describe()
162/719:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
162/720:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/721:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/722: base = base.drop(columns=['class'])
162/723:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
162/724:
# features whose correlation coefficient are close to 0 and statistically significant
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
162/725:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
162/726:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
162/727:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
162/728: X_train.shape
162/729: X_test.shape
162/730: total_results = None
162/731:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/732: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
162/733:
results_1 = pd.DataFrame(results_1).sort_values(by=['roc', 'recall'], ascending=False)
results_1['metric'] = (results_1.precision + results_1.recall + results_1.roc) / 3
results_1
162/734:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/735: results_2 = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test, 2)
162/736:
results_2 = pd.DataFrame(results_2).sort_values(by=['roc', 'recall'], ascending=False)
results_2['metric'] = (results_2.precision + results_2.recall + results_2.roc) / 3
results_2
162/737:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/738: results_3 = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test, 3)
162/739:
results_3 = pd.DataFrame(results_3).sort_values(by=['roc', 'recall'], ascending=False)
results_3['metric'] = (results_3.precision + results_3.recall + results_3.roc) / 3
df_plot = pd.melt(results_3, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
results_3
162/740:
#fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

#sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
#ax[0].set_ylim((0.7,1.1))

#bbox=[0, 0, 1.2, 1]
#ax[1].axis('off')
#mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
#mpl_table.auto_set_font_size(False)
#mpl_table.set_fontsize(10)

#plt.show()
162/741:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/742: results_4 = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test, 4)
162/743:
results_4 = pd.DataFrame(results_4).sort_values(by=['roc', 'recall'], ascending=False)
results_4['metric'] = (results_4.precision + results_4.recall + results_4.roc) / 3
df_plot = pd.melt(results_4, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
results_4
162/744: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
162/745:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
162/746: X_train_corr.shape
162/747: X_test_corr.shape
162/748:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
162/749:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/750: results_5 = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr, 5)
162/751:
results_5 = pd.DataFrame(results_5).sort_values(by=['roc', 'recall'], ascending=False)
results_5['metric'] = (results_5.precision + results_5.recall + results_5.roc) / 3
results_5
162/752:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/753: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr, 6)
162/754:
results_6 = pd.DataFrame(results_6).sort_values(by=['roc', 'recall'], ascending=False)
results_6['metric'] = (results_6.precision + results_6.recall + results_6.roc) / 3
results_6
162/755:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='metric', ascending=False)
total_results
162/756:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve_
losses = corr_model.loss_curve_
162/757:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_2)
sns.lineplot(losses)
162/758:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.show()
162/759:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=False)
total_results
162/760:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results
162/761:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results.groupby(by='leva').max()
162/762:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'metric']]
total_results.groupby(by='leva').max()
162/763:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'metric']]
total_results
162/764:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results
162/765:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results.groupby(by='leva').max()
162/766:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results
162/767:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results.groupby(by='leva').max('metric')
162/768:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results.groupby(by='leva').max()
162/769:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results.groupby(by='leva')
162/770:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results
162/771:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results.groupby(by='leva').max('metric')
162/772:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results.groupby(by='leva').agg({'metric':'max'})
162/773:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results.groupby(by='leva').agg({'metric':'max'})[['model_name', 'model_shape', 'leva', 'metric']]
162/774:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results.groupby(by=['leva', 'model_name']).agg({'metric':'max'})
162/775:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]
total_results.groupby(by='leva')['metric']
162/776:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
total_results = total_results[['model_name', 'model_shape', 'leva', 'metric']]

total_results.loc[total_results.groupby('leva')['metric'].idxmax()].reset_index(drop=True)
162/777:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)

total_results.loc[total_results.groupby('leva')['metric'].idxmax()].reset_index(drop=True)
162/778:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatest_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()].reset_index(drop=True)
greatest_leva
162/779: greatests = plot_model_stats(6, [model_tanh, model_adam, model_4, model_8, corr_model_4, corr_model_7], ['model_tanh', 'model_adam', 'model_4', 'model_8', 'corr_model_4', 'corr_model_7'], X_test_corr, y_test_corr, 7)6
162/780: greatests = plot_model_stats(6, [model_tanh, model_adam, model_4, model_8, corr_model_4, corr_model_7], ['model_tanh', 'model_adam', 'model_4', 'model_8', 'corr_model_4', 'corr_model_7'], X_test_corr, y_test_corr, 7)
162/781:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_leva
162/782:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
162/783:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
162/784:
# TODO plotar matriz de confusão considerando diferentes quantidades de neuronios e diferentes quantidades de camadas
# testar mudando funções de ativação
162/785:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
162/786:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
162/787:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
162/788: base = pd.read_csv("../dataset/observations.csv")
162/789: base.shape
162/790: base.head()
162/791:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
162/792:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
162/793:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
162/794: base.describe()
162/795:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
162/796:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
162/797:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
162/798: base = base.drop(columns=['class'])
162/799:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
162/800:
# features whose correlation coefficient are close to 0 and statistically significant
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
162/801:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
162/802:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
162/803:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
162/804: X_train.shape
162/805: X_test.shape
162/806: total_results = None
162/807:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
162/808: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
162/809:
results_1 = pd.DataFrame(results_1).sort_values(by=['roc', 'recall'], ascending=False)
results_1['metric'] = (results_1.precision + results_1.recall + results_1.roc) / 3
results_1
162/810:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
162/811: results_2 = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test, 2)
162/812:
results_2 = pd.DataFrame(results_2).sort_values(by=['roc', 'recall'], ascending=False)
results_2['metric'] = (results_2.precision + results_2.recall + results_2.roc) / 3
results_2
162/813:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/814: results_3 = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test, 3)
162/815:
results_3 = pd.DataFrame(results_3).sort_values(by=['roc', 'recall'], ascending=False)
results_3['metric'] = (results_3.precision + results_3.recall + results_3.roc) / 3
df_plot = pd.melt(results_3, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
results_3
162/816:
results_3 = pd.DataFrame(results_3).sort_values(by=['roc', 'recall'], ascending=False)
results_3['metric'] = (results_3.precision + results_3.recall + results_3.roc) / 3
#df_plot = pd.melt(results_3, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
results_3
162/817:
#fig, ax = plt.subplots(ncols=2, figsize=(18, 4))

#sns.barplot(data=df_plot, x="model_shape", y="value", edgecolor='black', hue='variable', ax=ax[0])
#ax[0].set_ylim((0.7,1.1))

#bbox=[0, 0, 1.2, 1]
#ax[1].axis('off')
#mpl_table = ax[1].table(cellText = df.values, rowLabels = df.model_name, bbox=bbox, colLabels=df.columns)
#mpl_table.auto_set_font_size(False)
#mpl_table.set_fontsize(10)

#plt.show()
162/818:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
162/819: results_4 = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test, 4)
162/820:
results_4 = pd.DataFrame(results_4).sort_values(by=['roc', 'recall'], ascending=False)
results_4['metric'] = (results_4.precision + results_4.recall + results_4.roc) / 3
#df_plot = pd.melt(results_4, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'f1', 'accuracy', 'roc']).sort_values(by='model_name').reset_index(drop=True)
results_4
162/821: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
162/822:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
162/823: X_train_corr.shape
162/824: X_test_corr.shape
162/825:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
162/826:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
162/827: results_5 = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr, 5)
162/828:
results_5 = pd.DataFrame(results_5).sort_values(by=['roc', 'recall'], ascending=False)
results_5['metric'] = (results_5.precision + results_5.recall + results_5.roc) / 3
results_5
162/829:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
162/830: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr, 6)
162/831:
results_6 = pd.DataFrame(results_6).sort_values(by=['roc', 'recall'], ascending=False)
results_6['metric'] = (results_6.precision + results_6.recall + results_6.roc) / 3
results_6
162/832:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
162/833:
losses_4 = corr_model_4.loss_curve_
losses_3 = corr_model_3.loss_curve_
losses_2 = corr_model_2.loss_curve_
losses = corr_model.loss_curve_
162/834:
sns.lineplot(losses_4)
sns.lineplot(losses_3)
sns.lineplot(losses_2)
sns.lineplot(losses)
162/835:
y_pred_prob = corr_model_4.predict_proba(X_test_corr)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test_corr, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label="MLP (" + str(corr_model_4.hidden_layer_sizes) + "), AUC="+str(roc_auc))


plt.legend()
plt.show()
162/836:
losses_corr_7 = corr_model_7.loss_curve_
losses_corr_4 = corr_model_4.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
162/837:
sns.lineplot(losses_corr_7)
sns.lineplot(losses_corr_4)
sns.lineplot(losses_8)
sns.lineplot(losses_4)
sns.lineplot(losses_tanh)
sns.lineplot(losses_adam)
162/838:
sns.lineplot(losses_corr_7, legend='auto')
sns.lineplot(losses_corr_4)
sns.lineplot(losses_8)
sns.lineplot(losses_4)
sns.lineplot(losses_tanh)
sns.lineplot(losses_adam)
162/839:
plt.plot(losses_corr_7)
sns.lineplot(losses_corr_4)
sns.lineplot(losses_8)
sns.lineplot(losses_4)
sns.lineplot(losses_tanh)
sns.lineplot(losses_adam)
162/840:
plt.plot(losses_corr_7, label='corr_model_7')
plt.plot(losses_corr_4, label='corr_model_4')
plt.plot(losses_8, label='model_8')
plt.plot(losses_4, label='model_4')
plt.plot(losses_tanh, label='model_tanh')
plt.plot(losses_adam, label='model_adam')
162/841:
plt.plot(losses_corr_7, label='corr_model_7')
plt.plot(losses_corr_4, label='corr_model_4')
plt.plot(losses_8, label='model_8')
plt.plot(losses_4, label='model_4')
plt.plot(losses_tanh, label='model_tanh')
plt.plot(losses_adam, label='model_adam')
plt.legend()
162/842:
plt.plot(losses_corr_7, label='corr_model_7')
plt.plot(losses_corr_4, label='corr_model_4')
plt.plot(losses_8, label='model_8')
plt.plot(losses_4, label='model_4')
plt.plot(losses_tanh, label='model_tanh')
plt.plot(losses_adam, label='model_adam')
plt.legend()
plt.show()
162/843:
plt.plot(losses_corr_7, label='corr_model_7')
plt.plot(losses_corr_4, label='corr_model_4')
plt.plot(losses_8, label='model_8')
plt.plot(losses_4, label='model_4')
plt.plot(losses_tanh, label='model_tanh')
plt.plot(losses_adam, label='model_adam')
plt.legend()
plt.yaxis('Loss')
plt.show()
162/844:
plt.plot(losses_corr_7, label='corr_model_7')
plt.plot(losses_corr_4, label='corr_model_4')
plt.plot(losses_8, label='model_8')
plt.plot(losses_4, label='model_4')
plt.plot(losses_tanh, label='model_tanh')
plt.plot(losses_adam, label='model_adam')
plt.legend()
plt.ylabel('Loss')
plt.show()
162/845:
plt.plot(losses_corr_7, label='corr_model_7')
plt.plot(losses_corr_4, label='corr_model_4')
plt.plot(losses_8, label='model_8')
plt.plot(losses_4, label='model_4')
plt.plot(losses_tanh, label='model_tanh')
plt.plot(losses_adam, label='model_adam')
plt.legend()
plt.ylabel('loss')
plt.xlabel('iteration')
plt.show()
162/846:
plt.plot(losses_corr_7, label='corr_model_7')
plt.plot(losses_corr_4, label='corr_model_4')
plt.plot(losses_8, label='model_8')
plt.plot(losses_4, label='model_4')
plt.plot(losses_tanh, label='model_tanh')
plt.plot(losses_adam, label='model_adam')
plt.legend()
plt.ylabel('loss')
plt.xlabel('iterations')
plt.title('Curvas de loss para os melhores modelos de cada leva')
plt.show()
162/847:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric']).sort_values(by='model_name').reset_index(drop=True)
162/848:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric']).sort_values(by='model_name').reset_index(drop=True)
greatests_plot
162/849:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric']).sort_values(by='model_name').reset_index(drop=True)
162/850:
fig, ax = plt.subplots(ncols=2)

plt.plot(losses_corr_7, label='corr_model_7', ax=ax[0])
plt.plot(losses_corr_4, label='corr_model_4')
plt.plot(losses_8, label='model_8')
plt.plot(losses_4, label='model_4')
plt.plot(losses_tanh, label='model_tanh')
plt.plot(losses_adam, label='model_adam')
plt.legend()
plt.ylabel('loss')
plt.xlabel('iterations')
plt.title('Curvas de loss para os melhores modelos de cada leva')
plt.show()
162/851:
fig, ax = plt.subplots(ncols=2)

ax[0].plot(losses_corr_7, label='corr_model_7')
plt.plot(losses_corr_4, label='corr_model_4')
plt.plot(losses_8, label='model_8')
plt.plot(losses_4, label='model_4')
plt.plot(losses_tanh, label='model_tanh')
plt.plot(losses_adam, label='model_adam')
plt.legend()
plt.ylabel('loss')
plt.xlabel('iterations')
plt.title('Curvas de loss para os melhores modelos de cada leva')
plt.show()
162/852:
fig, ax = plt.subplots(ncols=2)

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].ylabel('loss')
ax[0].xlabel('iterations')
ax[0].title('Curvas de loss para os melhores modelos de cada leva')
plt.show()
162/853:
fig, ax = plt.subplots(ncols=2)

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].xlabel('iterations')
ax[0].title('Curvas de loss para os melhores modelos de cada leva')
plt.show()
162/854:
fig, ax = plt.subplots(ncols=2)

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')
plt.show()
162/855:
fig, ax = plt.subplots(ncols=2, figsize=(16,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')
plt.show()
162/856:
fig, ax = plt.subplots(ncols=2, figsize=(16,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[0])

plt.show()
162/857:
fig, ax = plt.subplots(ncols=2, figsize=(16,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])

plt.show()
162/858:
fig, ax = plt.subplots(ncols=2, figsize=(16,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="value", y="model_name", edgecolor='black', hue='variable', ax=ax[1])

plt.show()
162/859:
fig, ax = plt.subplots(ncols=2, figsize=(16,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])

plt.show()
162/860:
fig, ax = plt.subplots(ncols=2, figsize=(16,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.7, 1.1)

plt.show()
162/861:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.7, 1.1)

plt.show()
162/862:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 1.11)

plt.show()
162/863:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 1.21)

plt.show()
162/864:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.97)

plt.show()
162/865:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.98)

plt.show()
162/866:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.985)

plt.show()
162/867:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.97)

plt.show()
162/868:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
162/869:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.97)

plt.show()
162/870:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric']).sort_values(by='model_name').reset_index(drop=True)
greatests_per_leva
162/871:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric']).sort_values(by='metric').reset_index(drop=True)
greatests_per_leva
162/872:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric']).sort_values(by='value').reset_index(drop=True)
greatests_per_leva
162/873:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
162/874:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.97)

plt.show()
162/875:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric']).sort_values(by='model_name').reset_index(drop=True)
greatests_per_leva
162/876:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
162/877:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.97)

plt.show()
162/878:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric']).sort_values(by='value').reset_index(drop=True)
greatests_per_leva
162/879:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
162/880:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.97)

plt.show()
162/881: greatests_plot
162/882:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric']).sort_values(by='value', ascending=False).reset_index(drop=True)
greatests_per_leva
162/883:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
162/884: greatests_plot
162/885:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.97)

plt.show()
162/886:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.98)

plt.show()
162/887:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.995)

plt.show()
162/888:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
162/889:
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric'])\
                   .sort_values(by='value', ascending=False)\
                   .reset_index(drop=True)
162/890:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
162/891:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.995)

plt.show()
162/892:
# features whose correlation coefficient are close to 0 and statistically significant
# TODO mostrar statistic e p-value
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
163/1:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['precision', 'roc', 'recall'], ascending=False)
    df['metric'] = (results_2.precision + results_2.recall + results_2.roc) / 3
    return df
163/2:
results_1 = build_dataframe(results_1)
results_1
163/3: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
163/4:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['precision', 'roc', 'recall'], ascending=False)
    df['metric'] = (results_2.precision + results_2.recall + results_2.roc) / 3
    return df
163/5:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
163/6:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
163/7:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
163/8: base = pd.read_csv("../dataset/observations.csv")
163/9: base.shape
163/10: base.head()
163/11:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
163/12:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
163/13:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
163/14: base.describe()
163/15:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
163/16:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
163/17:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
163/18: base = base.drop(columns=['class'])
163/19:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
163/20:
# features whose correlation coefficient are close to 0 and statistically significant
# TODO mostrar statistic e p-value
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
163/21:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
163/22:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
163/23:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
163/24: X_train.shape
163/25: X_test.shape
163/26: total_results = None
163/27:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
163/28: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
163/29:
results_1 = build_dataframe(results_1)
results_1
163/30:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['precision', 'roc', 'recall'], ascending=False)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    return df
163/31:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['precision', 'roc', 'recall'], ascending=False)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    return df
163/32:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
163/33:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
163/34:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
163/35: base = pd.read_csv("../dataset/observations.csv")
163/36: base.shape
163/37: base.head()
163/38:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
163/39:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
163/40:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
163/41: base.describe()
163/42:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
163/43:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
163/44:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
163/45: base = base.drop(columns=['class'])
163/46:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
163/47:
# features whose correlation coefficient are close to 0 and statistically significant
# TODO mostrar statistic e p-value
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        irrel_columns.append(col)
irrel_columns
163/48:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
163/49:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
163/50:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
163/51: X_train.shape
163/52: X_test.shape
163/53: total_results = None
163/54:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
163/55: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
163/56:
results_1 = build_dataframe(results_1)
results_1
163/57:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
163/58: results_2 = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test, 2)
163/59:
results_2 = build_dataframe(results_2)
results_2
163/60:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
163/61: results_3 = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test, 3)
163/62:
results_3 = build_dataframe(results_3)
results_3
163/63:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
163/64: results_4 = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test, 4)
163/65:
results_4 = build_dataframe(results_4)
results_4
163/66: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
163/67:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
163/68: X_train_corr.shape
163/69: X_test_corr.shape
163/70:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
163/71:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
163/72: results_5 = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr, 5)
163/73:
results_5 = build_dataframe(results_5)
results_5
163/74:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
163/75: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr, 6)
163/76:
results_6 = build_dataframe(results_6)
results_6
163/77:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
163/78:
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric'])\
                   .sort_values(by='value', ascending=False)\
                   .reset_index(drop=True)
163/79:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
163/80:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.995)

plt.show()
163/81:
plt.figure(figsize=(16, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
163/82:
plt.figure(figsize=(12, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
163/83:
irrel_columns = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        print(res)
        irrel_columns.append(col)
irrel_columns
163/84:
target_corr = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        print(res)
        target_corr.append({'feature': col, 'statistic': res.statistic, 'p-value': res.pvalue})
target_corr
163/85:
target_corr = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        target_corr.append({'feature': col, 'statistic': res.statistic, 'p-value': res.pvalue})
target_corr
163/86:
target_corr = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        target_corr.append({'feature': col, 'statistic': res.statistic, 'p-value': res.pvalue})
target_corr = pd.DataFrame(target_corr)
target_corr
163/87: irrel_columns = target_corr.feature
163/88:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['precision', 'roc', 'recall'], ascending=False)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    return df
163/89:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
163/90:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
163/91:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
163/92: base = pd.read_csv("../dataset/observations.csv")
163/93: base.shape
163/94: base.head()
163/95:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
163/96:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
163/97:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
163/98: base.describe()
163/99:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
163/100:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
163/101:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
163/102: base = base.drop(columns=['class'])
163/103:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
163/104:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
163/105:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
163/106: X_train.shape
163/107: X_test.shape
163/108: total_results = None
163/109:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
163/110: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
163/111:
results_1 = build_dataframe(results_1)
results_1
163/112:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
163/113: results_2 = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test, 2)
163/114:
results_2 = build_dataframe(results_2)
results_2
163/115:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
163/116: results_3 = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test, 3)
163/117:
results_3 = build_dataframe(results_3)
results_3
163/118:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
163/119: results_4 = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test, 4)
163/120:
results_4 = build_dataframe(results_4)
results_4
163/121:
plt.figure(figsize=(12, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
163/122:
target_corr = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        target_corr.append({'feature': col, 'statistic': res.statistic, 'p-value': res.pvalue})
target_corr = pd.DataFrame(target_corr)
target_corr
163/123: irrel_columns = target_corr.feature
163/124: corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
163/125:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
163/126: X_train_corr.shape
163/127: X_test_corr.shape
163/128:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
163/129:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
163/130: results_5 = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr, 5)
163/131:
results_5 = build_dataframe(results_5)
results_5
163/132:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=700).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
163/133: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr, 6)
163/134:
results_6 = build_dataframe(results_6)
results_6
163/135:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
163/136:
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric'])\
                   .sort_values(by='value', ascending=False)\
                   .reset_index(drop=True)
163/137:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
163/138:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.995)

plt.show()
163/139:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1000).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=800).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
163/140:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1100).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=800).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
163/141: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr, 6)
163/142:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base
163/143:
irrel_columns = target_corr.feature
irrel_columns
163/144:
irrel_columns = target_corr.feature
irrel_columns.columns
163/145: irrel_columns = target_corr.feature.to_numpy()
163/146:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base
163/147:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['precision', 'roc', 'recall'], ascending=False)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    return df
163/148:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
163/149:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
163/150:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
163/151: base = pd.read_csv("../dataset/observations.csv")
163/152: base.shape
163/153: base.head()
163/154:
# dropping useless column 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
163/155:
# renaming columns
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
163/156:
# Ordinal encoding for target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# transform data and add new integer target column to base
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
163/157: base.describe()
163/158:
pairs = []

for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
163/159:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1


plt.show()
163/160:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1

#sns.histplot(data=base, x="pelvic_incidence", bins=20, kde=True)
163/161: base = base.drop(columns=['class'])
163/162:
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
163/163:
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
163/164:
#X_train.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
#X_test.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'cervical_tilt']
163/165: X_train.shape
163/166: X_test.shape
163/167: total_results = None
163/168:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
163/169: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
163/170:
results_1 = build_dataframe(results_1)
results_1
163/171:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
163/172: results_2 = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test, 2)
163/173:
results_2 = build_dataframe(results_2)
results_2
163/174:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
163/175: results_3 = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test, 3)
163/176:
results_3 = build_dataframe(results_3)
results_3
163/177:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
163/178: results_4 = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test, 4)
163/179:
results_4 = build_dataframe(results_4)
results_4
163/180:
plt.figure(figsize=(12, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
163/181:
target_corr = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        target_corr.append({'feature': col, 'statistic': res.statistic, 'p-value': res.pvalue})
target_corr = pd.DataFrame(target_corr)
target_corr
163/182: irrel_columns = target_corr.feature.to_numpy()
163/183:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base
163/184:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
163/185: X_train_corr.shape
163/186: X_test_corr.shape
163/187:
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
163/188:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
163/189: results_5 = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr, 5)
163/190:
results_5 = build_dataframe(results_5)
results_5
163/191:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1100).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=800).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
163/192: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr, 6)
163/193:
results_6 = build_dataframe(results_6)
results_6
163/194:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
163/195:
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric'])\
                   .sort_values(by='value', ascending=False)\
                   .reset_index(drop=True)
163/196:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
163/197:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.995)

plt.show()
163/198:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base.shape
163/199:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.title('Distribuição das features relacionadas duas a duas e como influenciam no TARGET')
plt.show()
163/200:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
163/201:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['precision', 'roc', 'recall'], ascending=False)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    return df
163/202:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
163/203:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
163/204: base = pd.read_csv("../dataset/observations.csv")
163/205: base.shape
163/206: base.head()
163/207:
# Dropando coluna não necessária 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
163/208:
# Renomeando colunas (no Kaggle encontramos os nomes das features)
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
163/209:
# Ordinal encoding para a target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# Aplicação do tranform na base e adicionando coluna de inteiros TARGET
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
163/210: base.describe()
163/211:
pairs = []

# pairs tem todas as combinações das 12 features 2 a dois
for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
163/212:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.title('Distribuição das features relacionadas duas a duas e como influenciam no TARGET')
plt.show()
163/213:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.title('Distribuição das features relacionadas duas a duas e como influenciam no TARGET', loc='center')
plt.show()
163/214:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
163/215:
# 12 features initially
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1
163/216:
# Histograma das 12 features da base original pré-processada
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1
163/217:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    return df
163/218:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    return df
163/219:
# Realizando scaling nas bases de treino e teste
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
163/220:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base.head()
164/1:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
164/2:
def build_dataframe(results):
    df = pd.DataFrame(results).sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    return df
164/3:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
164/4:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
164/5: base = pd.read_csv("../dataset/observations.csv")
164/6: base.shape
164/7: base.head()
164/8:
# Dropando coluna não necessária 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
164/9:
# Renomeando colunas (no Kaggle encontramos os nomes das features)
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
164/10:
# Ordinal encoding para a target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# Aplicação do tranform na base e adicionando coluna de inteiros TARGET
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
164/11: base.describe()
164/12:
pairs = []

# pairs tem todas as combinações das 12 features 2 a dois
for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
164/13:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
164/14:
# Histograma das 12 features da base original pré-processada
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1
164/15: base = base.drop(columns=['class'])
164/16:
# Matriz de features X e vetor TARGET y
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

# Separando a base pré-processada em bases treino e teste
# seguindo uma proporção de 25% para a de teste
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
164/17:
# Inicializando scaler para realizar scaling nas
# matrizes de features de treino e teste
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
164/18: X_train.shape
164/19: X_test.shape
164/20: total_results = None
164/21:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
164/22: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
164/23:
results_1 = build_dataframe(results_1)
results_1
164/24:
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
165/1:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
165/2:
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
165/3:
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
165/4:
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    #plt.title(type(model).__name__)
    #plt.show()
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
165/5: base = pd.read_csv("../dataset/observations.csv")
165/6: base.shape
165/7: base.head()
165/8:
# Dropando coluna não necessária 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
165/9:
# Renomeando colunas (no Kaggle encontramos os nomes das features)
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
165/10:
# Ordinal encoding para a target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# Aplicação do tranform na base e adicionando coluna de inteiros TARGET
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
165/11: base.describe()
165/12:
pairs = []

# pairs tem todas as combinações das 12 features 2 a dois
for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
165/13:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
165/14:
# Histograma das 12 features da base original pré-processada
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1
165/15: base = base.drop(columns=['class'])
165/16:
# Matriz de features X e vetor TARGET y
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

# Separando a base pré-processada em bases treino e teste
# seguindo uma proporção de 25% para a de teste
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
165/17:
# Inicializando scaler para realizar scaling nas
# matrizes de features de treino e teste
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
165/18: X_train.shape
165/19: X_test.shape
165/20: total_results = None
165/21:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
165/22: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
165/23:
results_1 = build_dataframe(results_1)
results_1
165/24:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
165/25: results_2 = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test, 2)
165/26:
results_2 = build_dataframe(results_2)
results_2
165/27:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
165/28: results_3 = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test, 3)
165/29:
results_3 = build_dataframe(results_3)
results_3
165/30:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
165/31: results_4 = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test, 4)
165/32:
results_4 = build_dataframe(results_4)
results_4
165/33:
plt.figure(figsize=(12, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
165/34:
target_corr = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        target_corr.append({'feature': col, 'statistic': res.statistic, 'p-value': res.pvalue})
target_corr = pd.DataFrame(target_corr)
target_corr
165/35: irrel_columns = target_corr.feature.to_numpy()
165/36:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base.head()
165/37:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
165/38: X_train_corr.shape
165/39: X_test_corr.shape
165/40:
# Realizando scaling nas bases de treino e teste
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
165/41:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
165/42: results_5 = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr, 5)
165/43:
results_5 = build_dataframe(results_5)
results_5
165/44:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1100).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=800).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
165/45: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr, 6)
165/46:
results_6 = build_dataframe(results_6)
results_6
165/47:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
165/48:
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric'])\
                   .sort_values(by='value', ascending=False)\
                   .reset_index(drop=True)
165/49:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
165/50:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.995)

plt.show()
166/1:
import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.neural_network import MLPClassifier
import numpy as np
166/2:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
166/3:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
166/4:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
166/5: base = pd.read_csv("../dataset/observations.csv")
166/6: base.shape
166/7: base.head()
166/8:
# Dropando coluna não necessária 'Unnamed: 13'
base = base.drop(columns=['Unnamed: 13'])
166/9:
# Renomeando colunas (no Kaggle encontramos os nomes das features)
base.columns = ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'pelvic_slope', 'direct_tilt', 'thoracic_slope', 'cervical_tilt', 'sacrum_angle', 'scoliosis_slope', 'class']
166/10:
# Ordinal encoding para a target feature
# 1 --> Abnormal
# 0 --> Normal
oe = OrdinalEncoder(dtype=np.intc, categories=[['Normal', 'Abnormal']])

# Aplicação do tranform na base e adicionando coluna de inteiros TARGET
base['TARGET'] = oe.fit_transform(base['class'].to_numpy().reshape(-1,1))
166/11: base.describe()
166/12:
pairs = []

# pairs tem todas as combinações das 12 features 2 a dois
for i in range(12):
    for j in range(12):
        if i != j:
            coli = base.columns[i]
            colj = base.columns[j]
            if ([coli, colj] not in pairs) and ([colj, coli] not in pairs):
                pairs.append([coli, colj])
166/13:
fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(30,10))

i=0
j=0
for k in range(4, 49, 3):
    if j >= 5:
        j=0
        i += 1
    pair = pairs[k]
    data = base[[pair[0], pair[1], 'class']]
    sns.scatterplot(data=data, x=pair[0], y=pair[1], hue="class", ax=ax[i][j])
    j += 1

plt.show()
166/14:
# Histograma das 12 features da base original pré-processada
fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(30,10))

i = 0
j = 0
for k in range(12):
    if j >= 4:
        j = 0
        i += 1
    sns.histplot(data=base, x=base.columns[k], bins=20, kde=True, ax=ax[i][j])
    j += 1
166/15: base = base.drop(columns=['class'])
166/16:
# Matriz de features X e vetor TARGET y
X_ = base.loc[:, base.columns != 'TARGET']
y_ = base.TARGET

# Separando a base pré-processada em bases treino e teste
# seguindo uma proporção de 25% para a de teste
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
166/17:
# Inicializando scaler para realizar scaling nas
# matrizes de features de treino e teste
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
166/18: X_train.shape
166/19: X_test.shape
166/20: total_results = None
166/21:
model_id   = MLPClassifier(hidden_layer_sizes=(5,), activation="identity", random_state=42, max_iter=800).fit(X_train, y_train) 
model_log = MLPClassifier(hidden_layer_sizes=(5,), activation="logistic", random_state=42, max_iter=1100).fit(X_train, y_train)
model_relu = MLPClassifier(hidden_layer_sizes=(5,), activation="relu", random_state=42, max_iter=900).fit(X_train, y_train)
model_tanh = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
166/22: results_1 = plot_model_stats(4, [model_id, model_log, model_relu, model_tanh], ['model_id', 'model_log', 'model_relu', 'model_tanh'], X_test, y_test, 1)
166/23:
results_1 = build_dataframe(results_1)
results_1
166/24:
model_lbfgs   = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='lbfgs', max_iter=2950).fit(X_train, y_train) 
model_sgd = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='sgd', max_iter=350).fit(X_train, y_train)
model_adam = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, solver='adam', max_iter=900).fit(X_train, y_train)
166/25: results_2 = plot_model_stats(3, [model_lbfgs, model_sgd, model_adam], ['model_lbfgs', 'model_sgd', 'model_adam'], X_test, y_test, 2)
166/26:
results_2 = build_dataframe(results_2)
results_2
166/27:
model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=900).fit(X_train, y_train)
model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
166/28: results_3 = plot_model_stats(4, [model_1, model_2, model_3, model_4], ['model_1', 'model_2', 'model_3', 'model_4'], X_test, y_test, 3)
166/29:
results_3 = build_dataframe(results_3)
results_3
166/30:
model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1150).fit(X_train, y_train)
model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1275).fit(X_train, y_train)
model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=1600).fit(X_train, y_train)
166/31: results_4 = plot_model_stats(4, [model_5, model_6, model_7, model_8], ['model_5', 'model_6', 'model_7', 'model_8'], X_test, y_test, 4)
166/32:
results_4 = build_dataframe(results_4)
results_4
166/33:
plt.figure(figsize=(12, 6))
hm = sns.heatmap(base.corr(numeric_only=True), annot=True, vmin=-1, vmax=1);
hm.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
166/34:
target_corr = []
for col in base.columns:
    res = pearsonr(base[col], base.TARGET)
    if res.pvalue > 0.05:
        target_corr.append({'feature': col, 'statistic': res.statistic, 'p-value': res.pvalue})
target_corr = pd.DataFrame(target_corr)
target_corr
166/35: irrel_columns = target_corr.feature.to_numpy()
166/36:
corr_base = base.loc[:, [col for col in base.columns if col not in irrel_columns]]
corr_base.head()
166/37:
X_corr_ = corr_base.loc[:, corr_base.columns != 'TARGET']
y_corr_ = corr_base.TARGET

X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(
X_corr_, y_corr_, test_size=0.25, random_state=42)
166/38: X_train_corr.shape
166/39: X_test_corr.shape
166/40:
# Realizando scaling nas bases de treino e teste
X_train_corr = pd.DataFrame(scaler.fit_transform(X_train_corr))
X_test_corr = pd.DataFrame(scaler.fit_transform(X_test_corr))
166/41:
corr_model_1 = MLPClassifier(hidden_layer_sizes=(5,), activation="tanh", random_state=42, max_iter=1600).fit(X_train_corr, y_train_corr)
corr_model_2 = MLPClassifier(hidden_layer_sizes=(8,), activation="tanh", random_state=42, max_iter=1500).fit(X_train_corr, y_train_corr)
corr_model_3 = MLPClassifier(hidden_layer_sizes=(16,), activation="tanh", random_state=42, max_iter=1400).fit(X_train_corr, y_train_corr)
corr_model_4 = MLPClassifier(hidden_layer_sizes=(32,), activation="tanh", random_state=42, max_iter=1450).fit(X_train_corr, y_train_corr)
166/42: results_5 = plot_model_stats(4, [corr_model_1, corr_model_2, corr_model_3, corr_model_4], ['corr_model_1', 'corr_model_2', 'corr_model_3', 'corr_model_4'], X_test_corr, y_test_corr, 5)
166/43:
results_5 = build_dataframe(results_5)
results_5
166/44:
corr_model_5 = MLPClassifier(hidden_layer_sizes=(5,5), activation="tanh", random_state=42, max_iter=1300).fit(X_train_corr, y_train_corr)
corr_model_6 = MLPClassifier(hidden_layer_sizes=(8,4), activation="tanh", random_state=42, max_iter=1100).fit(X_train_corr, y_train_corr)
corr_model_7 = MLPClassifier(hidden_layer_sizes=(16,8), activation="tanh", random_state=42, max_iter=800).fit(X_train_corr, y_train_corr)
corr_model_8 = MLPClassifier(hidden_layer_sizes=(32,16), activation="tanh", random_state=42, max_iter=550).fit(X_train_corr, y_train_corr)
166/45: results_6 = plot_model_stats(4, [corr_model_5, corr_model_6, corr_model_7, corr_model_8], ['corr_model_5', 'corr_model_6', 'corr_model_7', 'corr_model_8'], X_test_corr, y_test_corr, 6)
166/46:
results_6 = build_dataframe(results_6)
results_6
166/47:
total_results = pd.concat([results_1, results_2, results_3, results_4, results_5, results_6], axis=0)\
                  .reset_index(drop=True)\
                  .sort_values(by='leva', ascending=True)
greatests_per_leva = total_results.loc[total_results.groupby('leva')['metric'].idxmax()]\
                              .reset_index(drop=True)\
                              .sort_values(by='metric', ascending=False)
greatests_per_leva
166/48:
greatests_plot = pd.melt(greatests_per_leva, id_vars=['model_name', 'model_shape'], value_vars=['precision', 'recall', 'roc', 'metric'])\
                   .sort_values(by='value', ascending=False)\
                   .reset_index(drop=True)
166/49:
losses_corr_4 = corr_model_4.loss_curve_
losses_corr_7 = corr_model_7.loss_curve_
losses_8 = model_8.loss_curve_
losses_4 = model_4.loss_curve_
losses_tanh = model_tanh.loss_curve_
losses_adam = model_adam.loss_curve_
166/50:
fig, ax = plt.subplots(ncols=2, figsize=(20,4))

ax[0].plot(losses_corr_4, label='corr_model_4')
ax[0].plot(losses_corr_7, label='corr_model_7')
ax[0].plot(losses_8, label='model_8')
ax[0].plot(losses_4, label='model_4')
ax[0].plot(losses_tanh, label='model_tanh')
ax[0].plot(losses_adam, label='model_adam')
ax[0].legend()
ax[0].set_ylabel('loss')
ax[0].set_xlabel('iterations')
ax[0].set_title('Curvas de loss para os melhores modelos de cada leva')

sns.barplot(data=greatests_plot, x="model_name", y="value", edgecolor='black', hue='variable', ax=ax[1])
ax[1].set_ylim(0.75, 0.995)

plt.show()
168/1:
import pandas as pd
import numpy as np
import seaborn as sns
168/2: train_df = pd.read_csv("../dataset/train.csv")
168/3: train_df
168/4: train_df.head()
168/5: train_df.describe()
168/6: train_df.value_counts()
168/7:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
168/8: test_df.head()
169/1: train_df.sample(5)
169/2:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
169/3:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
169/4: train_df.describe()
169/5: train_df.sample(5)
169/6: train_df.isnull()
169/7: train_df.isnull().mean()
169/8: train_df.isnull().sum()
169/9: train_df.isnull().sum() / train_df.shape[0]
169/10:
percent = train_df.isnull().mean()
columns_to_drop = percent[percent > 0.75].index
169/11:
percent = train_df.isnull().mean()
columns_to_drop = percent[percent > 0.75].index
columns_to_drop
169/12:
percent = train_df.isnull().mean()
columns_to_drop = percent[percent > 0.75].index
percent
169/13: train_df.Ticker.value_counts()
169/14: train_df.Ticket.value_counts()
169/15: train_df.Ticket.value_counts().shape
169/16: train_df.Ticket.value_counts()
169/17: train_df.Ticket.value_counts().head(10)
169/18: train_df.value_counts()
169/19: train_df.value_counts().sum()
169/20: train_df.Embarked.value_counts()
169/21: train_df.Ticket.value_counts().head(10)
169/22: train_df.Name.value_counts()
169/23: train_df.Parch.value_counts()
169/24: train_df.SibSp.value_counts()
169/25: train_df.SibSp.value_counts()
169/26: test_df.sample(5)
169/27:
train_df = train_df.drop(columns=['PassengerId', 'Name', 'Cabin'])
test_df = test_df.drop(columns=['PassengerId', 'Name', 'Cabin'])
169/28:
train_df = train_df.dropna()
test_df = test_df.dropna()
169/29:
oe = OrdinalEncoder(dtype=np.intc, categories=[['male', 'female']])

train_df['Sex'] = oe.fit_transform(train_df['Sex'].to_numpy().reshape(-1,1))
test_df['Sex'] = oe.fit_transform(test_df['Sex'].to_numpy().reshape(-1,1))
169/30:
y_train = pd.DataFrame(train_df.pop('Survived'))
y_train.columns = ['Survived']
x_train = train_df
169/31:
seed = 10
tree = DecisionTreeClassifier(criterion='gini',
                              min_samples_leaf=5,
                              min_samples_split=5,
                              max_depth=None,
                              random_state=seed)

tree.fit(x_train, y_train)
y_pred = tree.predict(test_df)
170/1:
from google.colab import drive
drive.mount('/content/drive')
170/2:
#from google.colab import drive
#drive.mount('/content/drive')
170/3:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
170/4:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
170/5: train_df = pd.read_csv("../dataset/train.csv")
170/6: train_df.head()
170/7: train_df.sample(5)
170/8: train_df.SibSp.value_counts()
170/9: sns.barplot(train_df.Sex)
170/10: sns.barplot(train_df.Sex.value_counts())
170/11: sns.barplot(train_df.Sex.value_counts(), hue='Sex')
170/12: sns.barplot(train_df.Sex.value_counts()/)
170/13: sns.barplot(train_df.Sex.value_counts()?)
170/14: sns.barplot(train_df.Sex.value_counts())
170/15: sns.barplot(train_df)
170/16: train_df = pd.read_csv("../dataset/train.csv")
170/17: train_df.sample(5)
170/18: train_df.describe()
170/19: sns.barplot(train_df)
170/20: sns.histplot(train_df.Age)
170/21: sns.histplot(train_df.Age, kde=True)
170/22: sns.histplot(train_df.Age, kde=True, hue='Sex')
170/23: sns.histplot(train_df.Age, kde=True)
170/24: sns.histplot(train_df.Fare, kde=True)
170/25:
males = train_df[:, train_df.Sex == 'male']
males
170/26:
males = train_df[train_df.Sex == 'male']
males
170/27:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
170/28: sns.boxplot(data=train_df, x='Sex', y='Age')
170/29: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.5)
170/30: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
170/31: sns.boxplot(data=train_df, x='Survived', y='Age', width=0.25)
170/32: sns.boxplot(data=train_df, x='Survived', y='Age', width=0.25, hue='Sex')
170/33: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
170/34: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
170/35: sns.barplot(x=train_df.Sex)
170/36: sns.barplot(x=train_df.Sex, y=train_df.Age)
170/37: sns.barplot(x=train_df.Sex, y=train_df.index)
170/38: train_df.shape
170/39: sns.barplot(x=train_df.Sex, y=train_df.index, hue='Survived')
170/40: sns.barplot(x=train_df.Sex, y=train_df.index, hue=train_df.Survived)
170/41: sns.barplot(x=train_df.Sex, y=train_df.Sex.value_counts(), hue=train_df.Survived)
170/42: train_df.Sex.value_counts()
170/43: train_df.Sex.value_counts().reset_index()
170/44: train_df.Sex.value_counts().reset_index(drop=True)
170/45: total_sex = train_df.Sex.value_counts().reset_index()
170/46: sns.barplot(data=total_sex, x='index', y='Sex')
170/47:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
males.shape
170/48:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
females.shape
170/49:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
170/50: sns.barplot(data=total_sex, x='index', y='Sex', width=0.25, edgecolor='black')
170/51:
total_sex = train_df.Sex.value_counts().reset_index()
total_sex.columns = ['Sex', 'Count']
170/52: sns.barplot(data=total_sex, x='index', y='Sex', width=0.25, edgecolor='black')
170/53: sns.barplot(data=total_sex, x='Sex', y='Count', width=0.25, edgecolor='black')
170/54: males.Survived.value_counts()
170/55: males.Survived.value_counts().reset_index()
170/56: males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
170/57:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived
170/58:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'
170/59:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_surviver, females_surviver], axis=0)
survived_per_sex
170/60:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
survived_per_sex
170/61:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
170/62: sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
170/63: train_df.groupby('Sex').sum()
170/64:
sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
170/65:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
170/66:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[1])
170/67:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0] + ax.containers[1])
170/68:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
170/69:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
170/70:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0] / males_survived.shape[0])
ax.bar_label(ax.containers[1])
170/71:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0][3] / males_survived.shape[0])
ax.bar_label(ax.containers[1])
170/72:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0][2] / males_survived.shape[0])
ax.bar_label(ax.containers[1])
170/73:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
170/74:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
print(ax.containers[0])
170/75:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
170/76:
dead = train_df[train_df.Survived == 0]
dead
170/77:
dead = train_df[train_df.Survived == 0][['Survived', 'Pclass']
dead
170/78:
dead = train_df[train_df.Survived == 0][['Survived', 'Pclass']]
dead
170/79:
dead = train_df[train_df.Survived == 0][['Survived', 'Pclass']]
dead.groupby('Pclass')
170/80:
dead = train_df[train_df.Survived == 0][['Survived', 'Pclass']]
dead.groupby('Pclass').sum()
170/81:
dead = train_df[train_df.Survived == 0][['Survived', 'Pclass']]
dead
170/82:
dead = train_df[train_df.Survived == 0][['Survived', 'Pclass']]
dead.Pclass.value_counts()
170/83:
dead = train_df[train_df.Survived == 0][['Survived', 'Pclass']]
dead.Pclass.value_counts().reset_index()
170/84:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0
dead
170/85:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
170/86: ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
170/87:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
170/88:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
170/89:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
170/90: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
170/91: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Sex')
170/92: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
170/93: sns.scatterplot(data=train_df, x='Age', y='SibSp', hue='Survived')
170/94: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
170/95: sns.scatterplot(data=train_df.Age)
170/96: sns.scatterplot(data=train_df.Age, hue='Survived')
170/97: sns.scatterplot(data=train_df, y='Age', hue='Survived')
170/98: sns.scatterplot(data=train_df, y='Age')
170/99: sns.scatterplot(data=train_df, y='Age', x=train_df.index)
170/100: sns.scatterplot(data=train_df, y='Age', x=train_df.index, hue='Survived')
170/101:
age_survived = train_df[train_df.Survived = 1].Age
sns.histplot(age_survived)
170/102:
age_survived = train_df[train_df.Survived == 1].Age
sns.histplot(age_survived)
170/103:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived)
sns.histplot(age_dead)
171/1:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived)
#sns.histplot(age_dead)
172/1:
#from google.colab import drive
#drive.mount('/content/drive')
172/2:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/3:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/4: train_df = pd.read_csv("../dataset/train.csv")
172/5: train_df.sample(5)
172/6: train_df.describe()
172/7:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/8:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/9:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/10:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/11:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/12: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/13:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived)
#sns.histplot(age_dead)
172/14: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/15: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/16: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/17: train_df.shape
172/18: sns.histplot(train_df.Age, kde=True)
172/19: sns.histplot(train_df.Fare, kde=True)
172/20:
train_df = train_df.drop(columns=['PassengerId', 'Name', 'Cabin', 'Ticket'])
test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/21:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived)
sns.histplot(age_dead)
172/22:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived)
#sns.histplot(age_dead)
172/23:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived)
sns.histplot(age_dead)
172/24:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived, kde=True)
sns.histplot(age_dead)
172/25:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived, kde=True)
sns.histplot(age_dead, kde=True)
172/26:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
ax = sns.histplot(age_survived, kde=True)
sns.histplot(age_dead, kde=True, ax=ax)
172/27:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived, kde=True)
sns.histplot(age_dead, kde=True)
172/28:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived, kde=True)
sns.histplot(age_dead, kde=True)
plt.legend(/)
172/29:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived, kde=True)
sns.histplot(age_dead, kde=True)
plt.legend()
172/30:
age_survived = train_df[train_df.Survived == 1].Age
age_dead = train_df[train_df.Survived == 0].Age
sns.histplot(age_survived, kde=True)
sns.histplot(age_dead, kde=True)
172/31: sns.histplot(data=train_df, x='Age')
172/32: sns.histplot(data=train_df, x='Age', hue='Survived')
172/33: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='stack')
172/34: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='dodge')
172/35: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='dodge', shrink=0.8)
172/36: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='dodge', shrink=0.5)
172/37: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True)
172/38: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=10)
172/39: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=20)
172/40: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=30)
172/41: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=20)
172/42: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='fill', shrink=0.5, kde=True, bins=20)
172/43: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20)
172/44: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, elemend='step')
172/45: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/46: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='stack', shrink=0.5, kde=True, bins=20, element='step')
172/47: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='stack', shrink=0.5, kde=True, bins=20, element='step')
172/48: sns.histplot(data=train_df, x='Age', hue=['Sex'], multiple='stack', shrink=0.5, kde=True, bins=20, element='step')
172/49: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='stack', shrink=0.5, kde=True, bins=20, element='step')
172/50: sns.hisplot(data=train_df, x='Sex', y='Survived')
172/51: sns.histplot(data=train_df, x='Sex', y='Survived')
172/52: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/53: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/54: sns.histplot(data=train_df, x='Age', y='Survived', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/55: sns.histplot(data=train_df, x='Age', y='Survived', hue='Sex', multiple='stack', shrink=0.5, kde=True, bins=20, element='step')
172/56: sns.histplot(data=train_df, x='Age', y='Survived', hue='Sex', multiple='dodge', shrink=0.5, kde=True, bins=20, element='step')
172/57: sns.histplot(data=train_df, x='Sex', y='Survived', hue='Age', multiple='dodge', shrink=0.5, kde=True, bins=20, element='step')
172/58: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=20, element='step')
172/59: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=20)
172/60: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=20, element='step')
172/61: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True)
172/62: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=2)
172/63: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=30)
172/64: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=50)
172/65: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=0.5, kde=True, bins=60)
172/66: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=0.5, bins=60)
172/67: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', bins=60)
172/68: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=1, bins=60)
172/69: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=1., bins=60)
172/70: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', multiple='dodge', shrink=1.6, bins=60)
172/71: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=60)
172/72: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=60, cbar=True)
172/73: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=20, cbar=True)
172/74: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=30, cbar=True)
172/75: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins430, cbar=True)
172/76: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=430, cbar=True)
172/77: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=80, cbar=True)
172/78: sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=100, cbar=True)
172/79:
fig, ax = plt.subplots(figsize=(20,20))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=100, cbar=True, ax=ax)
172/80:
fig, ax = plt.subplots(figsize=(10,20))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=100, cbar=True, ax=ax)
172/81:
fig, ax = plt.subplots(figsize=(10,10))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=100, cbar=True, ax=ax)
172/82:
fig, ax = plt.subplots(figsize=(10,8))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=100, cbar=True, ax=ax)
172/83:
fig, ax = plt.subplots(figsize=(10,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=100, cbar=True, ax=ax)
172/84:
fig, ax = plt.subplots(figsize=(12,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=100, cbar=True, ax=ax)
172/85:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=100, cbar=True, ax=ax)
172/86:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=90, cbar=True, ax=ax)
172/87:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=60, cbar=True, ax=ax)
172/88:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/89:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/90: train_df.shape
172/91:
train_df = train_df.drop(columns=['PassengerId', 'Name', 'Cabin', 'Ticket'])
test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/92: train_df.sample(1)
172/93: percent = train_df.isnull().mean()
172/94:
percent = train_df.isnull().mean()
percent
172/95:
percent = train_df.isnull().mean().reset_index(drop=True)
percent
172/96:
percent = train_df.isnull().mean()
percent
172/97:
#from google.colab import drive
#drive.mount('/content/drive')
172/98:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/99:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/100:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/101: train_df.sample(5)
172/102: train_df.describe()
172/103: train_df.shape
172/104:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/105:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/106:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/107:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/108:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/109: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/110: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/111: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/112:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/113: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/114: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/115: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/116:
percent = train_df.isnull().mean()
percent
172/117:
most_null_cols = percent[percent > 0.75]
train_df = train_df.drop(columns=most_null_cols)
172/118:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
172/119:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df
172/120:
#from google.colab import drive
#drive.mount('/content/drive')
172/121:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/122:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/123:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/124: train_df.sample(5)
172/125: train_df.describe()
172/126: train_df.shape
172/127:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/128:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/129:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/130:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/131:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/132: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/133: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/134: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/135:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/136: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/137: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/138: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/139:
percent = train_df.isnull().mean()
percent
172/140:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df
172/141:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/142:
#from google.colab import drive
#drive.mount('/content/drive')
172/143:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/144:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/145:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/146: train_df.sample(5)
172/147: train_df.describe()
172/148: train_df.shape
172/149:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/150:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/151:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/152:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/153:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/154: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/155: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/156: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/157:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/158: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/159: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/160: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/161:
percent = train_df.isnull().mean()
percent
172/162:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/163:
train_df = train_df.drop(columns=['PassengerId', 'Name', 'Cabin', 'Ticket'])
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/164:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket'])
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/165: train_df.sample(1)
172/166: CATEGORICAL_FEATURES = train_df[train_df.columns.dtype() == 'object']
172/167: train_df.columns
172/168: train_df.columns.dtype()
172/169: train_df.dtype()
172/170: train_df.type()
172/171: train_df.dtypes()
172/172: train_df.columns.dtypes()
172/173: train_df.dtypes()
172/174: train_df.dtypes
172/175: train_df.dtypes == 'object'
172/176: train_df[train_df.dtypes == 'object']
172/177: train_df.dtypes[:, 'object']
172/178: train_df.dtypes
172/179: train_df.dtypes.reset_index()
172/180:
types = train_df.dtypes.reset_index().rename(columns={'index': 'Feature', '0': 'dtype'})
types
172/181:
types = train_df.dtypes.reset_index(drop=True).rename(columns={'index': 'Feature', '0': 'dtype'})
types
172/182:
types = train_df.dtypes.reset_index(drop=True)
types
172/183:
types = train_df.dtypes.reset_index().rename(columns={'index': 'Feature', '0': 'dtype'})
types
172/184:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'dtype']
172/185:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'dtype']
types
172/186:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'dtype']
types
172/187: CATEGORICAL_FEATURES = [idx.Feature for idx in types if idx.dtype == 'object']
172/188: CATEGORICAL_FEATURES = [idx.Feature for idx in types if idx['dtype'] == 'object']
172/189: CATEGORICAL_FEATURES = [idx.Feature for idx in types]
172/190: CATEGORICAL_FEATURES = [idx for idx in types]
172/191:
CATEGORICAL_FEATURES = [idx for idx in types]
CATEGORICAL_FEATURES
172/192:
CATEGORICAL_FEATURES = [idx for idx in types.index]
CATEGORICAL_FEATURES
172/193:
CATEGORICAL_FEATURES = [idx for idx in types.value]
CATEGORICAL_FEATURES
172/194:
CATEGORICAL_FEATURES = [idx for idx in types.value()]
CATEGORICAL_FEATURES
172/195:
CATEGORICAL_FEATURES = [idx for idx in types.loc[:,:]]
CATEGORICAL_FEATURES
172/196:
CATEGORICAL_FEATURES = [idx for idx in types.Feature]
CATEGORICAL_FEATURES
172/197:
CATEGORICAL_FEATURES = [idx for idx in types.Feature if types[idx, 'dtype'] == 'object']
CATEGORICAL_FEATURES
172/198:
CATEGORICAL_FEATURES = [idx for idx in types.Feature if types[idx,:] == 'object']
CATEGORICAL_FEATURES
172/199:
CATEGORICAL_FEATURES = [idx for idx in types.Feature if types[idx,:].dtype == 'object']
CATEGORICAL_FEATURES
172/200:
CATEGORICAL_FEATURES = [idx for idx in types]
CATEGORICAL_FEATURES
172/201:
CATEGORICAL_FEATURES = [idx for idx in types.index]
CATEGORICAL_FEATURES
172/202:
CATEGORICAL_FEATURES = [types.iloc[idx] for idx in types.index]
CATEGORICAL_FEATURES
172/203:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index]
CATEGORICAL_FEATURES
172/204:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].dtype == 'object']
CATEGORICAL_FEATURES
172/205:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].dtype == 'object']
CATEGORICAL_FEATURES
172/206:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].dtype == 'int64']
CATEGORICAL_FEATURES
172/207:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].dtype == np.int64]
CATEGORICAL_FEATURES
172/208:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].dtype == object]
CATEGORICAL_FEATURES
172/209:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/210:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].type == 'object']
CATEGORICAL_FEATURES
172/211:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].type == 'object']
CATEGORICAL_FEATURES = [col for col in types.columns if types[col] == 'object']
172/212:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].type == 'object']
CATEGORICAL_FEATURES = [col for col in types.columns if types.loc[:,col].type == 'object']
172/213:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].type == 'object']
NUMERICAL_FEATURES = types.columns
172/214:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].type == 'object']
NUMERICAL_FEATURES = types.columns
NUMERICAL_FEATURES
172/215:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].type == 'object']
NUMERICAL_FEATURES = list(types.columns)
NUMERICAL_FEATURES
172/216:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].type == 'object']
NUMERICAL_FEATURES = types.columns
NUMERICAL_FEATURES
172/217:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].type == 'object']
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
172/218:
CATEGORICAL_FEATURES = [types.iloc[idx].Feature for idx in types.index if types.iloc[idx].type == 'object']
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
CATEGORICAL_FEATURES
172/219:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
CATEGORICAL_FEATURES
172/220:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if col not in CATEGORICAL_FEATURES]
172/221:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if col not in CATEGORICAL_FEATURES]
NUMERICAL_FEATURES
172/222:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype != 'object']
NUMERICAL_FEATURES
172/223:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype != 'object']
172/224:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')
172/225: train_df.fillna(np.NaN)
172/226: train_df = train_df.fillna(np.NaN)
172/227: train_df.isnull().sum()
172/228:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')

train_df = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES].values.reshape(-1,1))[:,0]
train_df
172/229:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')

train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES].values.reshape(-1,1))[:,0]
train_df
172/230:
#from google.colab import drive
#drive.mount('/content/drive')
172/231:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/232:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/233:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/234: train_df.sample(5)
172/235: train_df.describe()
172/236: train_df.shape
172/237:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/238:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/239:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/240:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/241:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/242: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/243: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/244: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/245:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/246: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/247: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/248: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/249:
percent = train_df.isnull().mean()
percent
172/250:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/251:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/252: train_df.sample(1)
172/253:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/254:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype != 'object']
172/255: #train_df = train_df.fillna(np.NaN)
172/256: train_df.isnull().sum()
172/257:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')

train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES].values.reshape(-1,1))[:,0]
train_df
172/258:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')

train_df = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES].values.reshape(-1,1))[:,0]
train_df
172/259:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')

train_df = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES].values.reshape(-1,1))[:,0]
train_df.shape
172/260:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')

train_df = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES].values.reshape(-1,1))[:,0]
172/261:
#from google.colab import drive
#drive.mount('/content/drive')
172/262:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/263:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/264:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/265: train_df.sample(5)
172/266: train_df.describe()
172/267: train_df.shape
172/268:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/269:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/270:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/271:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/272:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/273: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/274: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/275: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/276:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/277: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/278: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/279: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/280:
percent = train_df.isnull().mean()
percent
172/281:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/282:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/283: train_df.sample(1)
172/284:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/285:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype != 'object']
172/286: #train_df = train_df.fillna(np.NaN)
172/287: train_df.isnull().sum()
172/288:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')

train_df = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES].values.reshape(-1,1))[:,0]
172/289: train_df.shape
172/290:
#from google.colab import drive
#drive.mount('/content/drive')
172/291:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/292:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/293:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/294: train_df.sample(5)
172/295: train_df.describe()
172/296: train_df.shape
172/297:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/298:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/299:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/300:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/301:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/302: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/303: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/304: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/305:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/306: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/307: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/308: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/309:
percent = train_df.isnull().mean()
percent
172/310:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/311:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/312: train_df.sample(1)
172/313:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/314:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype != 'object']
172/315: #train_df = train_df.fillna(np.NaN)
172/316: train_df.isnull().sum()
172/317:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')

train_df = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES].to_numpy.reshape(-1,1))
172/318:
#from google.colab import drive
#drive.mount('/content/drive')
172/319:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/320:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/321:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/322: train_df.sample(5)
172/323: train_df.describe()
172/324: train_df.shape
172/325:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/326:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/327:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/328:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/329:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/330: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/331: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/332: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/333:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/334: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/335: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/336: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/337:
percent = train_df.isnull().mean()
percent
172/338:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/339:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/340: train_df.sample(1)
172/341:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/342:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype != 'object']
172/343: #train_df = train_df.fillna(np.NaN)
172/344: train_df.isnull().sum()
172/345:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')

train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES].to_numpy.reshape(-1,1))
172/346:
#from google.colab import drive
#drive.mount('/content/drive')
172/347:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/348:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/349:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/350: train_df.sample(5)
172/351: train_df.describe()
172/352: train_df.shape
172/353:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/354:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/355:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/356:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/357:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/358: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/359: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/360: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/361:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/362: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/363: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/364: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/365:
percent = train_df.isnull().mean()
percent
172/366:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/367:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/368: train_df.sample(1)
172/369:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/370:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype != 'object']
172/371: #train_df = train_df.fillna(np.NaN)
172/372: train_df.isnull().sum()
172/373:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES])
172/374: train_df.shape
172/375: train_df
172/376: train_df.isnull().sum()
172/377:
#from google.colab import drive
#drive.mount('/content/drive')
172/378:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/379:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/380:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/381: train_df.sample(5)
172/382: train_df.describe()
172/383: train_df.shape
172/384:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/385:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/386:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/387:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/388:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/389: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/390: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/391: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/392:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/393: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/394: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/395: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/396:
percent = train_df.isnull().mean()
percent
172/397:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/398:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/399: train_df.sample(1)
172/400:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/401:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype != 'object']
172/402: #train_df = train_df.fillna(np.NaN)
172/403: train_df.isnull().sum()
172/404:
from sklearn.impute import SimpleImputer

num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES])
train_df[CATEGORICAL_FEATURES] = cat_imputer.fit_transform(train_df[CATEGORICAL_FEATURES])
172/405: train_df.isnull().sum()
172/406: train_df.sample(5)
172/407:
oe = OrdinalEncoder()

train_df[CATEGORICAL_FEATURES] = oe.fit_transform(train_df[CATEGORICAL_FEATURES])
172/408: train_df.sample(5)
172/409:
# Matriz de features X e vetor TARGET y
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived

# Separando a base pré-processada em bases treino e teste
# seguindo uma proporção de 25% para a de teste
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
172/410:
# Inicializando scaler para realizar scaling nas
# matrizes de features de treino e teste
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
172/411: X_train.shape
172/412:
seed = 10
tree = DecisionTreeClassifier(criterion='gini',
                              min_samples_leaf=5,
                              min_samples_split=5,
                              max_depth=None,
                              random_state=seed)

tree.fit(X_train, y_train)
y_pred = tree.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('DecisionTreeClassifier accuracy score: {}'.format(accuracy))
172/413:
seed = 10
tree = DecisionTreeClassifier(criterion='gini',
                              min_samples_leaf=5,
                              min_samples_split=5,
                              max_depth=None,
                              random_state=seed)

tree.fit(X_train, y_train)
y_pred = tree.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('DecisionTreeClassifier accuracy score: {}'.format(accuracy))
172/414:
seed = 10
tree = DecisionTreeClassifier(criterion='gini',
                              min_samples_leaf=5,
                              min_samples_split=5,
                              max_depth=None,
                              random_state=seed)

tree.fit(X_train, y_train)
y_pred = tree.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('DecisionTreeClassifier accuracy score: {}'.format(accuracy))
172/415:
seed = 10
tree = DecisionTreeClassifier(criterion='gini',
                              min_samples_leaf=5,
                              min_samples_split=5,
                              max_depth=None,
                              random_state=seed)

tree.fit(X_train, y_train)
y_pred = tree.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('DecisionTreeClassifier accuracy score: {}'.format(accuracy))
172/416:
seed = 10
tree = DecisionTreeClassifier(criterion='gini',
                              min_samples_leaf=5,
                              min_samples_split=5,
                              max_depth=None,
                              random_state=seed)

tree.fit(X_train, y_train)
y_pred = tree.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('DecisionTreeClassifier accuracy score: {}'.format(accuracy))
172/417:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
        'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
172/418:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
172/419:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
172/420:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/421: results_1 = plot_model_stats(1, [tree], ['tree'], X_test, y_test, 1)
172/422: results_1 = plot_model_stats(2, [tree], ['tree'], X_test, y_test, 1)
172/423:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
172/424: results_1 = plot_model_stats(2, [tree], ['tree'], X_test, y_test, 1)
172/425: results_1 = plot_model_stats(1, [tree], ['tree'], X_test, y_test, 1)
172/426: results_1 = plot_model_stats(2, [tree], ['tree'], X_test, y_test, 1)
172/427:
seed = 10
tree = DecisionTreeClassifier(criterion='gini',
                              min_samples_leaf=5,
                              min_samples_split=5,
                              max_depth=None,
                              random_state=seed)

tree.fit(X_train, y_train)
172/428: results_1 = plot_model_stats(2, [tree], ['tree'], X_test, y_test, 1)
172/429:
seed = 42
tree_1 = DecisionTreeClassifier(criterion='gini', random_state=seed).fit(X_train, y_train)
tree_1 = DecisionTreeClassifier(criterion='gini', random_state=seed).fit(X_train, y_train)
172/430:
seed = 42
tree_1 = DecisionTreeClassifier(criterion='gini', random_state=seed).fit(X_train, y_train)
tree_2 = DecisionTreeClassifier(criterion='gini', random_state=seed).fit(X_train, y_train)
172/431:
seed = 42
tree_1 = DecisionTreeClassifier(criterion='gini', random_state=seed).fit(X_train, y_train)
tree_2 = DecisionTreeClassifier(criterion='entropy', random_state=seed).fit(X_train, y_train)
tree_3 = DecisionTreeClassifier(criterion='log_loss', random_state=seed).fit(X_train, y_train)
172/432: results_1 = plot_model_stats(3, [tree1, tree_2, tree_3], ['tree_1', 'tree_2', 'tree_3'], X_test, y_test, 1)
172/433: results_1 = plot_model_stats(3, [tree_1, tree_2, tree_3], ['tree_1', 'tree_2', 'tree_3'], X_test, y_test, 1)
172/434:
results_1 = build_dataframe(results_1)
results_1
172/435:
tree_4 = DecisionTreeClassifier(splitter='best', random_state=seed).fit(X_train, y_train)
tree_5 = DecisionTreeClassifier(splitter='random', random_state=seed).fit(X_train, y_train)
172/436: results_2 = plot_model_stats(2, [tree_4, tree_5], ['tree_4', 'tree_5'], X_test, y_test, 2)
172/437:
results_2 = build_dataframe(results_2)
results_2
172/438:
tree_6 = DecisionTreeClassifier(random_state=seed).fit(X_train, y_train)
tree_7 = DecisionTreeClassifier(class_weight='balanced', random_state=seed).fit(X_train, y_train)
172/439: results_3 = plot_model_stats(2, [tree_6, tree_7], ['tree_6', 'tree_7'], X_test, y_test, 3)
172/440:
results_3 = build_dataframe(results_3)
results_3
172/441:
tree_6 = DecisionTreeClassifier(min_samples_split=3, random_state=seed).fit(X_train, y_train)
tree_7 = DecisionTreeClassifier(min_samples_split=4, random_state=seed).fit(X_train, y_train)
172/442: results_3 = plot_model_stats(2, [tree_6, tree_7], ['tree_6', 'tree_7'], X_test, y_test, 3)
172/443:
results_3 = build_dataframe(results_3)
results_3
172/444:
tree_6 = DecisionTreeClassifier(min_samples_split=3, random_state=seed).fit(X_train, y_train)
tree_7 = DecisionTreeClassifier(min_samples_split=5, random_state=seed).fit(X_train, y_train)
172/445: results_3 = plot_model_stats(2, [tree_6, tree_7], ['tree_6', 'tree_7'], X_test, y_test, 3)
172/446:
results_3 = build_dataframe(results_3)
results_3
172/447:
tree_6 = DecisionTreeClassifier(min_samples_split=3, random_state=seed).fit(X_train, y_train)
tree_7 = DecisionTreeClassifier(min_samples_split=7, random_state=seed).fit(X_train, y_train)
172/448: results_3 = plot_model_stats(2, [tree_6, tree_7], ['tree_6', 'tree_7'], X_test, y_test, 3)
172/449:
results_3 = build_dataframe(results_3)
results_3
172/450:
tree_6 = DecisionTreeClassifier(min_samples_split=3, random_state=seed).fit(X_train, y_train)
tree_7 = DecisionTreeClassifier(min_samples_split=10, random_state=seed).fit(X_train, y_train)
172/451: results_3 = plot_model_stats(2, [tree_6, tree_7], ['tree_6', 'tree_7'], X_test, y_test, 3)
172/452:
results_3 = build_dataframe(results_3)
results_3
172/453:
tree_6 = DecisionTreeClassifier(min_samples_split=3, random_state=seed).fit(X_train, y_train)
tree_7 = DecisionTreeClassifier(min_samples_split=5, random_state=seed).fit(X_train, y_train)
172/454: results_3 = plot_model_stats(2, [tree_6, tree_7], ['tree_6', 'tree_7'], X_test, y_test, 3)
172/455:
results_3 = build_dataframe(results_3)
results_3
172/456:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=3, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=7, random_state=seed).fit(X_train, y_train)
172/457: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/458:
results_4 = build_dataframe(results_4)
results_4
172/459:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=3, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=9, random_state=seed).fit(X_train, y_train)
172/460: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/461:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=3, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=7, random_state=seed).fit(X_train, y_train)
172/462: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/463:
results_4 = build_dataframe(results_4)
results_4
172/464:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=3, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=9, random_state=seed).fit(X_train, y_train)
172/465: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/466:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=3, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=7, random_state=seed).fit(X_train, y_train)
172/467: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/468:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=3, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=9, random_state=seed).fit(X_train, y_train)
172/469: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/470:
results_4 = build_dataframe(results_4)
results_4
172/471:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=3, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=11, random_state=seed).fit(X_train, y_train)
172/472: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/473:
results_4 = build_dataframe(results_4)
results_4
172/474:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=3, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=13, random_state=seed).fit(X_train, y_train)
172/475: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/476:
results_4 = build_dataframe(results_4)
results_4
172/477:
results_4 = build_dataframe(results_4)
results_4
172/478:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=3, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=15, random_state=seed).fit(X_train, y_train)
172/479: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/480:
results_4 = build_dataframe(results_4)
results_4
172/481:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=15, random_state=seed).fit(X_train, y_train)
172/482: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/483:
results_4 = build_dataframe(results_4)
results_4
172/484:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=17, random_state=seed).fit(X_train, y_train)
172/485: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/486:
results_4 = build_dataframe(results_4)
results_4
172/487:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=19, random_state=seed).fit(X_train, y_train)
172/488: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/489:
results_4 = build_dataframe(results_4)
results_4
172/490:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=25, random_state=seed).fit(X_train, y_train)
172/491: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/492:
results_4 = build_dataframe(results_4)
results_4
172/493:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, random_state=seed).fit(X_train, y_train)
172/494: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/495:
results_4 = build_dataframe(results_4)
results_4
172/496:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=35, random_state=seed).fit(X_train, y_train)
172/497: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/498:
results_4 = build_dataframe(results_4)
results_4
172/499:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=40, random_state=seed).fit(X_train, y_train)
172/500: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/501:
results_4 = build_dataframe(results_4)
results_4
172/502:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=60, random_state=seed).fit(X_train, y_train)
172/503: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/504:
results_4 = build_dataframe(results_4)
results_4
172/505:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=50, random_state=seed).fit(X_train, y_train)
172/506: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/507:
results_4 = build_dataframe(results_4)
results_4
172/508:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=40, random_state=seed).fit(X_train, y_train)
172/509: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/510:
results_4 = build_dataframe(results_4)
results_4
172/511:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, random_state=seed).fit(X_train, y_train)
172/512: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/513:
results_4 = build_dataframe(results_4)
results_4
172/514:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=25, random_state=seed).fit(X_train, y_train)
172/515: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/516:
results_4 = build_dataframe(results_4)
results_4
172/517:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=20, random_state=seed).fit(X_train, y_train)
172/518: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/519:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=15, random_state=seed).fit(X_train, y_train)
172/520: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/521:
results_4 = build_dataframe(results_4)
results_4
172/522:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=17, random_state=seed).fit(X_train, y_train)
172/523: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/524:
results_4 = build_dataframe(results_4)
results_4
172/525:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=18, random_state=seed).fit(X_train, y_train)
172/526: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/527:
results_4 = build_dataframe(results_4)
results_4
172/528:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=19, random_state=seed).fit(X_train, y_train)
172/529: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/530:
results_4 = build_dataframe(results_4)
results_4
172/531:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=21, random_state=seed).fit(X_train, y_train)
172/532: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/533:
results_4 = build_dataframe(results_4)
results_4
172/534:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=22, random_state=seed).fit(X_train, y_train)
172/535: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/536:
results_4 = build_dataframe(results_4)
results_4
172/537:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=23, random_state=seed).fit(X_train, y_train)
172/538: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/539:
results_4 = build_dataframe(results_4)
results_4
172/540:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=24, random_state=seed).fit(X_train, y_train)
172/541: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/542:
results_4 = build_dataframe(results_4)
results_4
172/543:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=25, random_state=seed).fit(X_train, y_train)
172/544: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/545:
results_4 = build_dataframe(results_4)
results_4
172/546:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=26, random_state=seed).fit(X_train, y_train)
172/547: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/548:
results_4 = build_dataframe(results_4)
results_4
172/549:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=27, random_state=seed).fit(X_train, y_train)
172/550: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/551:
results_4 = build_dataframe(results_4)
results_4
172/552:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=28, random_state=seed).fit(X_train, y_train)
172/553: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/554:
results_4 = build_dataframe(results_4)
results_4
172/555:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=29, random_state=seed).fit(X_train, y_train)
172/556: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/557:
results_4 = build_dataframe(results_4)
results_4
172/558:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, random_state=seed).fit(X_train, y_train)
172/559: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/560:
results_4 = build_dataframe(results_4)
results_4
172/561:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=31, random_state=seed).fit(X_train, y_train)
172/562: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/563:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=32, random_state=seed).fit(X_train, y_train)
172/564: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/565:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=33, random_state=seed).fit(X_train, y_train)
172/566: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/567:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=34, random_state=seed).fit(X_train, y_train)
172/568: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/569:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=35, random_state=seed).fit(X_train, y_train)
172/570: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/571: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/572:
results_4 = build_dataframe(results_4)
results_4
172/573:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, random_state=seed).fit(X_train, y_train)
172/574: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/575:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=20, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, random_state=seed).fit(X_train, y_train)
172/576: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/577:
results_4 = build_dataframe(results_4)
results_4
172/578:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=21, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, random_state=seed).fit(X_train, y_train)
172/579: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/580:
tree_8 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=10, random_state=seed).fit(X_train, y_train)
tree_9 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=20, random_state=seed).fit(X_train, y_train)
tree_10 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, random_state=seed).fit(X_train, y_train)
172/581: results_4 = plot_model_stats(3, [tree_8, tree_9, tree_10], ['tree_8', 'tree_9', 'tree_10'], X_test, y_test, 4)
172/582:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=5, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=10, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=15, random_state=seed).fit(X_train, y_train)
172/583: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/584:
results_5 = build_dataframe(results_5)
results_5
172/585:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=5, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=10, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=25, random_state=seed).fit(X_train, y_train)
172/586: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/587:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=4, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=10, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=25, random_state=seed).fit(X_train, y_train)
172/588: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/589:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=3, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=10, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=25, random_state=seed).fit(X_train, y_train)
172/590: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/591:
results_5 = build_dataframe(results_5)
results_5
172/592:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=2, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=10, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=25, random_state=seed).fit(X_train, y_train)
172/593: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/594:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=1, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=10, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=25, random_state=seed).fit(X_train, y_train)
172/595: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/596:
results_5 = build_dataframe(results_5)
results_5
172/597:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=2, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=10, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=25, random_state=seed).fit(X_train, y_train)
172/598: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/599:
results_5 = build_dataframe(results_5)
results_5
172/600:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=3, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=10, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=25, random_state=seed).fit(X_train, y_train)
172/601: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/602:
results_5 = build_dataframe(results_5)
results_5
172/603:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=4, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=10, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=25, random_state=seed).fit(X_train, y_train)
172/604: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/605:
results_5 = build_dataframe(results_5)
results_5
172/606:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=1, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=2, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_depth=3, random_state=seed).fit(X_train, y_train)
172/607: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/608:
results_5 = build_dataframe(results_5)
results_5
172/609:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=1, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=2, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=3, random_state=seed).fit(X_train, y_train)
172/610:
results_5 = build_dataframe(results_5)
results_5
172/611:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=2, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=3, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=4, random_state=seed).fit(X_train, y_train)
172/612: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/613:
results_5 = build_dataframe(results_5)
results_5
172/614:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=None, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=3, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=4, random_state=seed).fit(X_train, y_train)
172/615: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/616:
results_5 = build_dataframe(results_5)
results_5
172/617:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=5, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=3, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=4, random_state=seed).fit(X_train, y_train)
172/618: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/619:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=10, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=3, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=4, random_state=seed).fit(X_train, y_train)
172/620: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/621:
results_5 = build_dataframe(results_5)
results_5
172/622:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=7, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=3, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=4, random_state=seed).fit(X_train, y_train)
172/623: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/624:
results_5 = build_dataframe(results_5)
results_5
172/625:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=6, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=3, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=4, random_state=seed).fit(X_train, y_train)
172/626: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/627:
results_5 = build_dataframe(results_5)
results_5
172/628:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=5, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=3, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=4, random_state=seed).fit(X_train, y_train)
172/629: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/630:
results_5 = build_dataframe(results_5)
results_5
172/631:
tree_11 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=4, random_state=seed).fit(X_train, y_train)
tree_12 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=5, random_state=seed).fit(X_train, y_train)
tree_13 = DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=30, max_leaf_nodes=6, random_state=seed).fit(X_train, y_train)
172/632: results_5 = plot_model_stats(3, [tree_11, tree_12, tree_13], ['tree_11', 'tree_12', 'tree_13'], X_test, y_test, 5)
172/633:
results_5 = build_dataframe(results_5)
results_5
172/634:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/635:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
params = {'criterion': ['gini', 'entropy', 'log_los'], 'splitter': ['best', 'random'], 'min_samples_leaf': min_samples_leaf_range}
172/636: tree_gscv = GridSearchCV()
172/637:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
params = {'criterion': ['gini', 'entropy', 'log_los'], 'splitter': ['best', 'random'], 'min_samples_leaf': min_samples_leaf_range}
172/638: tree_gscv = GridSearchCV()
172/639:
tree = DecisionTreeClassifier()
tree_gscv = GridSearchCV(tree, params)
172/640: tree_gscv.best_score
172/641: tree_gscv.best_score_
172/642:
tree = DecisionTreeClassifier()
tree_gscv = GridSearchCV(estimator=tree, param_grid=params)
172/643: tree_gscv.best_score_
172/644: tree_gscv
172/645:
tree = DecisionTreeClassifier()
tree_gscv = GridSearchCV(estimator=tree, param_grid=params).fit(X_train, y_train)
172/646:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
params = {'criterion': ['gini', 'entropy', 'log_loss'], 'splitter': ['best', 'random'], 'min_samples_leaf': min_samples_leaf_range}
172/647:
tree = DecisionTreeClassifier()
tree_gscv = GridSearchCV(estimator=tree, param_grid=params).fit(X_train, y_train)
172/648: tree_gscv.best_score_
172/649: tree_gscv.best_estimator_
172/650: best = tree_gscv.best_estimator_
172/651: best
172/652:
yhat = best.predict(X_test)
precision_score(y_test, y_hat)
172/653:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/654: recall_score(y_test, y_hat)
172/655:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range}
172/656:
tree = DecisionTreeClassifier()
tree_gscv = GridSearchCV(estimator=tree, param_grid=params).fit(X_train, y_train)
172/657: tree_gscv.best_score_
172/658: best = tree_gscv.best_estimator_
172/659:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/660: recall_score(y_test, y_hat)
172/661: confusion_matrix(y_test, y_hat)
172/662:
best = tree_gscv.best_estimator_
best
172/663:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range}
172/664:
tree = DecisionTreeClassifier()
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/665: tree_gscv.best_score_
172/666:
best = tree_gscv.best_estimator_
best
172/667:
best = tree_gscv.best_estimator_
best
172/668:
tree = DecisionTreeClassifier()
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/669: tree_gscv.best_score_
172/670:
best = tree_gscv.best_estimator_
best
172/671:
best = tree_gscv.best_estimator_
best.best_params_
172/672:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/673:
tree = DecisionTreeClassifier()
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/674: tree_gscv.best_score_
172/675:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/676:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/677: recall_score(y_test, y_hat)
172/678:
tree = DecisionTreeClassifier()
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/679: tree_gscv.best_score_
172/680:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/681:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/682: recall_score(y_test, y_hat)
172/683:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 5, 6]}
172/684:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/685: tree_gscv.best_score_
172/686:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/687:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/688: recall_score(y_test, y_hat)
172/689: confusion_matrix(y_test, y_hat)
172/690:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 5, 6]}
172/691:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/692: tree_gscv.best_score_
172/693:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/694:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/695: recall_score(y_test, y_hat)
172/696: confusion_matrix(y_test, y_hat)
172/697:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 5, 6],
          'ccp_alpha': [.1, .01, .001]}
172/698:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/699: tree_gscv.best_score_
172/700:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/701:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/702: recall_score(y_test, y_hat)
172/703: confusion_matrix(y_test, y_hat)
172/704:


num_imputer = SimpleImputer(missing_values=np.nan, strategy='median')
cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES])
train_df[CATEGORICAL_FEATURES] = cat_imputer.fit_transform(train_df[CATEGORICAL_FEATURES])
172/705: train_df.isnull().sum()
172/706:
#from google.colab import drive
#drive.mount('/content/drive')
172/707:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/708:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
172/709:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
172/710:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
172/711:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/712:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/713: train_df.sample(5)
172/714: train_df.describe()
172/715: train_df.shape
172/716:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/717:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/718:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/719:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/720:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/721: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/722: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/723: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/724:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/725: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/726: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/727: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/728:
percent = train_df.isnull().mean()
percent
172/729:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/730:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/731: train_df.sample(1)
172/732:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/733:
CATEGORICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in train_df.columns if train_df[col].dtype != 'object']
172/734: #train_df = train_df.fillna(np.NaN)
172/735: train_df.isnull().sum()
172/736:


num_imputer = SimpleImputer(missing_values=np.nan, strategy='median')
cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES])
train_df[CATEGORICAL_FEATURES] = cat_imputer.fit_transform(train_df[CATEGORICAL_FEATURES])
172/737: train_df.isnull().sum()
172/738: train_df.sample(5)
172/739:
oe = OrdinalEncoder()

train_df[CATEGORICAL_FEATURES] = oe.fit_transform(train_df[CATEGORICAL_FEATURES])
172/740: train_df.sample(5)
172/741:
# Matriz de features X e vetor TARGET y
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived

# Separando a base pré-processada em bases treino e teste
# seguindo uma proporção de 25% para a de teste
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
172/742:
# Inicializando scaler para realizar scaling nas
# matrizes de features de treino e teste
scaler = MinMaxScaler()

X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.fit_transform(X_test))
172/743: X_train.shape
172/744:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 5, 6],
          'ccp_alpha': [.1, .01, .001]}
172/745:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/746: tree_gscv.best_score_
172/747:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/748:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/749: recall_score(y_test, y_hat)
172/750: confusion_matrix(y_test, y_hat)
172/751:
min_samples_leaf_range = [5, 7, 10, 12, 15, 17, 20, 22, 25, 27, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 5, 6],
          'ccp_alpha': [.1, .01, .001]}
172/752:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/753: tree_gscv.best_score_
172/754:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/755:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/756: recall_score(y_test, y_hat)
172/757: confusion_matrix(y_test, y_hat)
172/758:
tree = DecisionTreeClassifier(random_state=1024)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/759: tree_gscv.best_score_
172/760:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/761:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/762: recall_score(y_test, y_hat)
172/763: confusion_matrix(y_test, y_hat)
172/764:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 5, 6],
          'max_features': ['auto', 'sqrt', 'log2']}
172/765:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/766:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 5, 6],
          'max_features': ['sqrt', 'log2']}
172/767:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/768: tree_gscv.best_score_
172/769:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/770:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/771: recall_score(y_test, y_hat)
172/772: confusion_matrix(y_test, y_hat)
172/773:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 5, 6]}
172/774:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/775: tree_gscv.best_score_
172/776:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/777:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/778: recall_score(y_test, y_hat)
172/779: confusion_matrix(y_test, y_hat)
172/780:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True, scoring='recall').fit(X_train, y_train)
172/781: tree_gscv.best_score_
172/782:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/783:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/784: recall_score(y_test, y_hat)
172/785: confusion_matrix(y_test, y_hat)
172/786:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/787:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/788: recall_score(y_test, y_hat)
172/789: confusion_matrix(y_test, y_hat)
172/790: tree_gscv.best_score_
172/791:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/792:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/793: recall_score(y_test, y_hat)
172/794:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True, scoring='recall').fit(X_train, y_train)
172/795: tree_gscv.best_score_
172/796:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/797:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/798: recall_score(y_test, y_hat)
172/799: confusion_matrix(y_test, y_hat)
172/800:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True, scoring='roc_auc').fit(X_train, y_train)
172/801: tree_gscv.best_score_
172/802:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/803:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/804: recall_score(y_test, y_hat)
172/805: confusion_matrix(y_test, y_hat)
172/806:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True, scoring='precision').fit(X_train, y_train)
172/807: tree_gscv.best_score_
172/808:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/809:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/810: recall_score(y_test, y_hat)
172/811: confusion_matrix(y_test, y_hat)
172/812:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True, scoring='accuracy').fit(X_train, y_train)
172/813: tree_gscv.best_score_
172/814:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/815:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/816: recall_score(y_test, y_hat)
172/817: confusion_matrix(y_test, y_hat)
172/818: tree_gscv.scorer_
172/819: accuracy_score(y_test, y_hat)
172/820:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
172/821: recall_score(y_test, y_hat)
172/822: accuracy_score(y_test, y_hat)
172/823:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True, scoring='accuracy', refit='accuracy').fit(X_train, y_train)
172/824: tree_gscv.best_score_
172/825: tree_gscv.scorer_
172/826:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/827:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
172/828: recall_score(y_test, y_hat)
172/829: accuracy_score(y_test, y_hat)
172/830: confusion_matrix(y_test, y_hat)
172/831:
min_samples_leaf_range = [5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 6, 8, 10]}
172/832:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True, scoring='accuracy', refit='accuracy').fit(X_train, y_train)
172/833: tree_gscv.best_score_
172/834: tree_gscv.scorer_
172/835:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/836:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
172/837: recall_score(y_test, y_hat)
172/838: accuracy_score(y_test, y_hat)
172/839: confusion_matrix(y_test, y_hat)
172/840:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 6, 8, 10]}
172/841:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True, scoring='accuracy', refit='accuracy').fit(X_train, y_train)
172/842: tree_gscv.best_score_
172/843: tree_gscv.scorer_
172/844:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/845:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
172/846: recall_score(y_test, y_hat)
172/847: accuracy_score(y_test, y_hat)
172/848: confusion_matrix(y_test, y_hat)
172/849:
seed = 42
tree_1 = DecisionTreeClassifier(criterion='gini', random_state=seed).fit(X_train, y_train)
tree_2 = DecisionTreeClassifier(criterion='entropy', random_state=seed).fit(X_train, y_train)
tree_3 = DecisionTreeClassifier(criterion='log_loss', random_state=seed).fit(X_train, y_train)
172/850:
tree = DecisionTreeClassifier(random_state=42)
tree_gscv = GridSearchCV(estimator=tree, param_grid=params, verbose=True).fit(X_train, y_train)
172/851: tree_gscv.best_score_
172/852: tree_gscv.scorer_
172/853:
best = tree_gscv.best_estimator_
tree_gscv.best_params_
172/854:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
172/855: recall_score(y_test, y_hat)
172/856: accuracy_score(y_test, y_hat)
172/857: confusion_matrix(y_test, y_hat)
172/858:
column_trans = ColumnTransformer([
                    ('num_imp', SimpleImputer(missing_values=np.nan, strategy='mean'), NUMERICAL_FEATURES),
                    ('cat_imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), CATEGORICAL_FEATURES)])
172/859:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/860:
column_trans = ColumnTransformer([
                    ('num_imp', SimpleImputer(missing_values=np.nan, strategy='mean'), NUMERICAL_FEATURES),
                    ('cat_imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), CATEGORICAL_FEATURES)])
172/861: column_trans
172/862:
column_trans = ColumnTransformer([
                    ('num_imp', SimpleImputer(missing_values=np.nan, strategy='mean'), NUMERICAL_FEATURES),
                    ('cat_imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), CATEGORICAL_FEATURES),
                    ('ord_enc', OrdinalEncoder(), CATEGORICAL_FEATURES)])
172/863: column_trans
172/864:
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
172/865:
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
172/866:
CATEGORICAL_FEATURES = [col for col in X_.columns if X_[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/867: train_df.isnull().sum()
172/868:
#from google.colab import drive
#drive.mount('/content/drive')
172/869:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/870:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
172/871:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
172/872:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
172/873:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/874:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/875: train_df.sample(5)
172/876: train_df.describe()
172/877: train_df.shape
172/878:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/879:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/880:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/881:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/882:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/883: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/884: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/885: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/886:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/887: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/888: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/889: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/890:
percent = train_df.isnull().mean()
percent
172/891:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/892:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/893: train_df.sample(1)
172/894:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/895:
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
172/896:
CATEGORICAL_FEATURES = [col for col in X_.columns if X_[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/897: #train_df = train_df.fillna(np.NaN)
172/898: train_df.isnull().sum()
172/899:
column_trans = ColumnTransformer([
                    ('num_imp', SimpleImputer(missing_values=np.nan, strategy='mean'), NUMERICAL_FEATURES),
                    ('cat_imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), CATEGORICAL_FEATURES),
                    ('ord_enc', OrdinalEncoder(), CATEGORICAL_FEATURES)])
172/900: column_trans
172/901: X_ = column_trans.fit_transform(X_)
172/902: X_
172/903: X_.shape
172/904: y_.shape
172/905: X_ = pd.DataFrame(column_trans.fit_transform(X_))
172/906: X_.shape
172/907:
#from google.colab import drive
#drive.mount('/content/drive')
172/908:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/909:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
172/910:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
172/911:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
172/912:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/913:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/914: train_df.sample(5)
172/915: train_df.describe()
172/916: train_df.shape
172/917:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/918:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/919:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/920:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/921:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/922: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/923: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/924: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/925:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/926: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/927: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/928: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/929:
percent = train_df.isnull().mean()
percent
172/930:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/931:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/932: train_df.sample(1)
172/933:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/934:
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
172/935:
CATEGORICAL_FEATURES = [col for col in X_.columns if X_[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/936: #train_df = train_df.fillna(np.NaN)
172/937: train_df.isnull().sum()
172/938:


num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES])
train_df[CATEGORICAL_FEATURES] = cat_imputer.fit_transform(train_df[CATEGORICAL_FEATURES])
172/939:
column_trans = ColumnTransformer([
                    ('num_imp', SimpleImputer(missing_values=np.nan, strategy='mean'), NUMERICAL_FEATURES),
                    ('cat_imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), CATEGORICAL_FEATURES),
                    ('ord_enc', OrdinalEncoder(), CATEGORICAL_FEATURES)])
172/940: column_trans
172/941: X_ = pd.DataFrame(column_trans.fit_transform(X_))
172/942: X_.shape
172/943: X_
172/944: X_.isnull().sum()
172/945:
#from google.colab import drive
#drive.mount('/content/drive')
172/946:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/947:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
172/948:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
172/949:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
172/950:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/951:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/952: train_df.sample(5)
172/953: train_df.describe()
172/954: train_df.shape
172/955:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/956:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/957:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/958:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/959:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/960: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/961: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/962: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/963:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/964: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/965: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/966: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/967:
percent = train_df.isnull().mean()
percent
172/968:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/969:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/970: train_df.sample(1)
172/971:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/972:
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
172/973:
CATEGORICAL_FEATURES = [col for col in X_.columns if X_[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/974: #train_df = train_df.fillna(np.NaN)
172/975: train_df.isnull().sum()
172/976:


#num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
#cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

#train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES])
#train_df[CATEGORICAL_FEATURES] = cat_imputer.fit_transform(train_df[CATEGORICAL_FEATURES])
172/977:
column_trans = ColumnTransformer([
                    ('num_imp', SimpleImputer(missing_values=np.nan, strategy='mean'), NUMERICAL_FEATURES),
                    ('cat_imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), CATEGORICAL_FEATURES),
                    ('ord_enc', OrdinalEncoder(), CATEGORICAL_FEATURES)])
172/978: column_trans
172/979: X_ = pd.DataFrame(column_trans.fit_transform(X_))
172/980: X_.isnull().sum()
172/981:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown="ignore"))
    ]
)

col_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/982:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/983:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown="ignore"))
    ]
)

col_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/984:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OrdinalEncoder(handle_unknown="ignore"))
    ]
)

col_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/985:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OrdinalEncoder(handle_unknown="ignore"))
    ]
)

column_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/986: column_transf
172/987: X_ = pd.DataFrame(column_transf.fit_transform(X_))
172/988:
dt = Pipeline(
    steps=[('transformer', column_transf), ('estimator', DecisionTreeClassifier())]
)
172/989: dt.fit(X_, y_)
172/990:
#from google.colab import drive
#drive.mount('/content/drive')
172/991:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/992:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
172/993:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
172/994:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
172/995:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/996:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/997: train_df.sample(5)
172/998: train_df.describe()
172/999: train_df.shape
172/1000:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/1001:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/1002:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/1003:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/1004:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/1005: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/1006: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/1007: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/1008:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/1009: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/1010: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/1011: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/1012:
percent = train_df.isnull().mean()
percent
172/1013:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/1014:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/1015: train_df.sample(1)
172/1016:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/1017:
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
172/1018:
CATEGORICAL_FEATURES = [col for col in X_.columns if X_[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1019: #train_df = train_df.fillna(np.NaN)
172/1020: train_df.isnull().sum()
172/1021:


#num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
#cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

#train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES])
#train_df[CATEGORICAL_FEATURES] = cat_imputer.fit_transform(train_df[CATEGORICAL_FEATURES])
172/1022:
column_trans = ColumnTransformer([
                    ('num_imp', SimpleImputer(missing_values=np.nan, strategy='mean'), NUMERICAL_FEATURES),
                    ('cat_imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), CATEGORICAL_FEATURES),
                    ('ord_enc', OrdinalEncoder(), CATEGORICAL_FEATURES)])
172/1023:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OrdinalEncoder(handle_unknown="ignore"))
    ]
)

column_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/1024: column_transf
172/1025:
dt = Pipeline(
    steps=[('transformer', column_transf), ('estimator', DecisionTreeClassifier())]
)
172/1026: dt.fit(X_, y_)
172/1027:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value'))
    ]
)

column_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/1028: column_transf
172/1029:
dt = Pipeline(
    steps=[('transformer', column_transf), ('estimator', DecisionTreeClassifier())]
)
172/1030: dt.fit(X_, y_)
172/1031:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

column_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/1032:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/1033:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

column_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/1034: column_transf
172/1035:
dt = Pipeline(
    steps=[('transformer', column_transf), ('estimator', DecisionTreeClassifier())]
)
172/1036: dt.fit(X_, y_)
172/1037: X_.isnull().sum()
172/1038:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 6, 8, 10]}
172/1039:
tree = DecisionTreeClassifier()

dt = Pipeline(
    steps=[('transformer', column_transf),
           ('estimator', GridSearchCV(estimator=tree, param_grid=params, verbose=True))
    ]
)
172/1040: dt.fit(X_, y_)
172/1041: dt.best_score_
172/1042: dt.steps
172/1043: dt.steps[1]
172/1044: dt.steps[1][1]
172/1045: gridcv = dt.steps[1][1]
172/1046:
gridcv = dt.steps[1][1]
gridcv.best_score_
172/1047:
gridcv = dt.steps[1][1]
gridcv.best_estimator
172/1048:
gridcv = dt.steps[1][1]
gridcv.best_estimator_
172/1049: best = dt.steps[1][1].best_estimator_
172/1050:
best = dt.steps[1][1].best_estimator_
best
172/1051:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/1052:
#from google.colab import drive
#drive.mount('/content/drive')
172/1053:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/1054:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
172/1055:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
172/1056:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
172/1057:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/1058:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/1059: train_df.sample(5)
172/1060: train_df.describe()
172/1061: train_df.shape
172/1062:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/1063:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/1064:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/1065:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/1066:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/1067: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/1068: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/1069: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/1070:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/1071: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/1072: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/1073: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/1074:
percent = train_df.isnull().mean()
percent
172/1075:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/1076:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/1077: train_df.sample(1)
172/1078:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/1079:
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
172/1080:
CATEGORICAL_FEATURES = [col for col in X_.columns if X_[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1081: #train_df = train_df.fillna(np.NaN)
172/1082: train_df.isnull().sum()
172/1083:


#num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
#cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

#train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES])
#train_df[CATEGORICAL_FEATURES] = cat_imputer.fit_transform(train_df[CATEGORICAL_FEATURES])
172/1084:
column_trans = ColumnTransformer([
                    ('num_imp', SimpleImputer(missing_values=np.nan, strategy='mean'), NUMERICAL_FEATURES),
                    ('cat_imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), CATEGORICAL_FEATURES),
                    ('ord_enc', OrdinalEncoder(), CATEGORICAL_FEATURES)])
172/1085:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

column_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/1086: column_transf
172/1087:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 6, 8, 10]}
172/1088:
tree = DecisionTreeClassifier()

dt = Pipeline(
    steps=[('transformer', column_transf),
           ('estimator', GridSearchCV(estimator=tree, param_grid=params, verbose=True))
    ]
)
172/1089:
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
172/1090: dt.fit(X_train, y_train)
172/1091:
best = dt.steps[1][1].best_estimator_
best
172/1092:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
172/1093:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/1094:
best = dt.steps[1][1].best_estimator_
best
172/1095:
y_hat = best.predict(X_test)
precision_score(y_test, y_hat)
172/1096:
grid = dt.steps[1][1]
grid
172/1097: grid = dt.steps[1][1]
172/1098: dt.steps[1][1]
172/1099: dt.steps[1]
172/1100: dt.steps[1][1]
172/1101: grid = dt.steps[1][1]
172/1102:
y_hat = grid.predict(X_test)
precision_score(y_test, y_hat)
172/1103: X_test.shape
172/1104: X_train.shape
172/1105:
tree = DecisionTreeClassifier()

dt = Pipeline(
    steps=[('transformer', column_transf),
           ('estimator', tree)
    ]
)
172/1106:
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
172/1107: dt.fit(X_train, y_train)
172/1108: dt.steps[1][1]
172/1109: dt.steps[1]
172/1110: dt.steps[1][1]
172/1111: dt.score(X_train)
172/1112: dt.score(X_test, y_test)
172/1113: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1114: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1115: tree_gscv
172/1116: tree_gscv.fit(X_train, y_train)
172/1117:
tree = DecisionTreeClassifier()

dt = Pipeline(
    steps=[('transformer', column_transf),
           ('classifier', tree)
    ]
)
172/1118:
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
172/1119: dt.fit(X_train, y_train)
172/1120: dt.score(X_test, y_test)
172/1121: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1122: tree_gscv
172/1123: tree_gscv.fit(X_train, y_train)
172/1124: tree_gscv = GridSearchCV(dt, param_grid=params, verbose=True)
172/1125: tree_gscv
172/1126: tree_gscv.fit(X_train, y_train)
172/1127:
#from google.colab import drive
#drive.mount('/content/drive')
172/1128:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
172/1129:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
172/1130:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
172/1131:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
172/1132:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
172/1133:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
172/1134: train_df.sample(5)
172/1135: train_df.describe()
172/1136: train_df.shape
172/1137:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
172/1138:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
172/1139:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/1140:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
172/1141:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
172/1142: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
172/1143: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/1144: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
172/1145:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
172/1146: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
172/1147: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
172/1148: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
172/1149:
percent = train_df.isnull().mean()
percent
172/1150:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
172/1151:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
172/1152: train_df.sample(1)
172/1153:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
172/1154:
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
172/1155:
CATEGORICAL_FEATURES = [col for col in X_.columns if X_[col].dtype == 'object']
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1156: #train_df = train_df.fillna(np.NaN)
172/1157: train_df.isnull().sum()
172/1158:


#num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
#cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

#train_df[NUMERICAL_FEATURES] = num_imputer.fit_transform(train_df[NUMERICAL_FEATURES])
#train_df[CATEGORICAL_FEATURES] = cat_imputer.fit_transform(train_df[CATEGORICAL_FEATURES])
172/1159:
column_trans = ColumnTransformer([
                    ('num_imp', SimpleImputer(missing_values=np.nan, strategy='mean'), NUMERICAL_FEATURES),
                    ('cat_imp', SimpleImputer(missing_values=np.nan, strategy='most_frequent'), CATEGORICAL_FEATURES),
                    ('ord_enc', OrdinalEncoder(), CATEGORICAL_FEATURES)])
172/1160:
num_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

cat_transf = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

column_transf = ColumnTransformer(
    transformers=[
        ('num', num_transf, NUMERICAL_FEATURES),
        ('cat', cat_transf, CATEGORICAL_FEATURES)
    ]
)
172/1161: column_transf
172/1162:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 6, 8, 10]}
172/1163:
tree = DecisionTreeClassifier()

dt = Pipeline(
    steps=[('transformer', column_transf),
           ('classifier', tree)
    ]
)
172/1164:
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
172/1165: dt.fit(X_train, y_train)
172/1166: dt.score(X_test, y_test)
172/1167: tree_gscv = GridSearchCV(dt, param_grid=params, verbose=True)
172/1168: tree_gscv
172/1169: tree_gscv.fit(X_train, y_train)
172/1170: tree_gscv = GridSearchCV(estimator=dt.steps[1][1], param_grid=params, verbose=True)
172/1171: tree_gscv
172/1172: tree_gscv.fit(X_train, y_train)
172/1173: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1174: tree_gscv
172/1175: tree_gscv.fit(X_train, y_train)
172/1176:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'dt__criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 6, 8, 10]}
172/1177: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1178: tree_gscv
172/1179: tree_gscv.fit(X_train, y_train)
172/1180:
tree = DecisionTreeClassifier()

dt = Pipeline(
    steps=[('transformer', column_transf),
           ('estimator', tree)
    ]
)
172/1181:
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
172/1182: dt.fit(X_train, y_train)
172/1183: dt.score(X_test, y_test)
172/1184:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 6, 8, 10]}
172/1185: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1186: tree_gscv
172/1187: tree_gscv.fit(X_train, y_train)
172/1188:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'max_leaf_nodes': [4, 6, 8, 10]}
172/1189: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1190: tree_gscv
172/1191: tree_gscv.fit(X_train, y_train)
172/1192:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
172/1193: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1194: tree_gscv
172/1195: tree_gscv.fit(X_train, y_train)
172/1196:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
172/1197: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1198: tree_gscv
172/1199: tree_gscv.fit(X_train, y_train)
172/1200:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
172/1201: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1202: tree_gscv
172/1203: tree_gscv.fit(X_train, y_train)
172/1204:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
172/1205: tree_gscv = GridSearchCV(estimator=dt, param_grid=params, verbose=True)
172/1206: tree_gscv
172/1207: tree_gscv.fit(X_train, y_train)
172/1208: tree_gscv.best_estimator_
172/1209: tree_gscv.best_score_
172/1210:
y_hat = grid.predict(X_test)
precision_score(y_test, y_hat)
172/1211: X_test.sample(5)
172/1212:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
172/1213: recall_score(y_test, y_hat)
172/1214: accuracy_score(y_test, y_hat)
172/1215: confusion_matrix(y_test, y_hat)
172/1216:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1217: CATEGORICAL_FEATURES
172/1218:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns.to_numpy()
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1219: CATEGORICAL_FEATURES
172/1220:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1221: CATEGORICAL_FEATURES
172/1222:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns[0]
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1223: CATEGORICAL_FEATURES
172/1224:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns[]
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1225: CATEGORICAL_FEATURES
172/1226:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1227: CATEGORICAL_FEATURES
172/1228:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns.tolist()
NUMERICAL_FEATURES = [col for col in X_.columns if X_[col].dtype != 'object']
172/1229: CATEGORICAL_FEATURES
172/1230:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns.tolist()
NUMERICAL_FEATURES = X_.select_dtypes(exclude='object').columns.tolist()
172/1231: CATEGORICAL_FEATURES
172/1232: NUMERICAL_FEATURES
172/1233:
# Matriz de features X e vetor TARGET y
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
172/1234:
numerical_transformer = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

column_transf = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, NUMERICAL_FEATURES),
        ('cat', categorical_transformer, CATEGORICAL_FEATURES)
    ]
)
173/1:
#from google.colab import drive
#drive.mount('/content/drive')
173/2:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
173/3:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
173/4:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
173/5:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
173/6:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
173/7:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
173/8: train_df.sample(5)
173/9: train_df.describe()
173/10: train_df.shape
173/11:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
173/12:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
173/13:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
173/14:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
173/15:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
173/16: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
173/17: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
173/18: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
173/19:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
173/20: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
173/21: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
173/22: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
173/23:
percent = train_df.isnull().mean()
percent
173/24:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
173/25:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
173/26: train_df.sample(1)
173/27:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
173/28:
# Matriz de features X e vetor TARGET y
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
173/29:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns.tolist()
NUMERICAL_FEATURES = X_.select_dtypes(exclude='object').columns.tolist()
173/30:
numerical_transformer = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

column_transformer = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, NUMERICAL_FEATURES),
        ('cat', categorical_transformer, CATEGORICAL_FEATURES)
    ]
)

pipe = Pipeline(
    steps=[('transformer', column_transformer),
           ('estimator', DecisionTreeClassifier())
    ]
)
173/31:
# Separando a base pré-processada em bases treino e teste
# seguindo uma proporção de 25% para a de teste
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
173/32: pipe.fit(X_train, y_train)
173/33: pipe.score(X_test, y_test)
173/34:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
173/35: tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True).fit(X_train, y_train)
173/36: tree_gscv
173/37: tree_gscv.best_score_
173/38:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
173/39: recall_score(y_test, y_hat)
173/40: accuracy_score(y_test, y_hat)
173/41: confusion_matrix(y_test, y_hat)
173/42: tree_gscv.best_estimator_
173/43: #sns.countplot(dataset['Embarked'])
174/1:
#from google.colab import drive
#drive.mount('/content/drive')
174/2:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
174/3:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
174/4:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
174/5:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
174/6:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
174/7:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
174/8: train_df.sample(5)
174/9: train_df.describe()
174/10: train_df.shape
174/11: #sns.countplot(dataset['Embarked'])
174/12:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
174/13:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
174/14:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
174/15:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
174/16:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
174/17: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
174/18: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
174/19: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
174/20:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
174/21: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
174/22: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
174/23: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
174/24:
percent = train_df.isnull().mean()
percent
174/25:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
174/26:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket', 'Age']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
174/27: train_df.sample(1)
174/28:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
174/29:
# Matriz de features X e vetor TARGET y
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
174/30:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns.tolist()
NUMERICAL_FEATURES = X_.select_dtypes(exclude='object').columns.tolist()
174/31:
numerical_transformer = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

column_transformer = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, NUMERICAL_FEATURES),
        ('cat', categorical_transformer, CATEGORICAL_FEATURES)
    ]
)

pipe = Pipeline(
    steps=[('transformer', column_transformer),
           ('estimator', DecisionTreeClassifier())
    ]
)
174/32:
# Separando a base pré-processada em bases treino e teste
# seguindo uma proporção de 25% para a de teste
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
174/33: pipe.fit(X_train, y_train)
174/34: pipe.score(X_test, y_test)
174/35:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
174/36: tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True).fit(X_train, y_train)
174/37: tree_gscv
174/38: tree_gscv.best_score_
174/39: tree_gscv.best_estimator_
174/40:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
174/41: recall_score(y_test, y_hat)
174/42: accuracy_score(y_test, y_hat)
174/43: confusion_matrix(y_test, y_hat)
175/1:
#from google.colab import drive
#drive.mount('/content/drive')
175/2:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
175/3:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
175/4:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
175/5:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
175/6:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
175/7:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
175/8: train_df.sample(5)
175/9: train_df.describe()
175/10: train_df.shape
175/11: #sns.countplot(dataset['Embarked'])
175/12:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
175/13:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
175/14:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
175/15:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
175/16:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
175/17: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
175/18: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
175/19: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
175/20:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
175/21: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
175/22: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
175/23: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
175/24:
percent = train_df.isnull().mean()
percent
175/25:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
175/26:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
175/27: train_df.sample(1)
175/28:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
175/29:
# Matriz de features X e vetor TARGET y
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
175/30:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns.tolist()
NUMERICAL_FEATURES = X_.select_dtypes(exclude='object').columns.tolist()
175/31:
numerical_transformer = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

column_transformer = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, NUMERICAL_FEATURES),
        ('cat', categorical_transformer, CATEGORICAL_FEATURES)
    ]
)

pipe = Pipeline(
    steps=[('transformer', column_transformer),
           ('estimator', DecisionTreeClassifier())
    ]
)
175/32:
# Separando a base pré-processada em bases treino e teste
# seguindo uma proporção de 25% para a de teste
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
175/33: pipe.fit(X_train, y_train)
175/34: pipe.score(X_test, y_test)
175/35:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
175/36: tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall']).fit(X_train, y_train)
175/37: tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall'], refit='recall').fit(X_train, y_train)
175/38: tree_gscv
175/39: tree_gscv.best_params_
175/40: tree_gscv.scorer_
175/41: tree_gscv.best_score_
175/42: tree_gscv.best_estimator_
175/43: tree_gscv.best_estimator_
175/44:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
175/45: recall_score(y_test, y_hat)
175/46: recall_score(y_test, y_hat)
175/47: accuracy_score(y_test, y_hat)
175/48: confusion_matrix(y_test, y_hat)
175/49: tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall'], refit='precision').fit(X_train, y_train)
175/50: tree_gscv
175/51: tree_gscv.best_params_
175/52: tree_gscv.scorer_
175/53: tree_gscv.best_estimator_
175/54:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
175/55: recall_score(y_test, y_hat)
175/56: accuracy_score(y_test, y_hat)
175/57: confusion_matrix(y_test, y_hat)
175/58: tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
175/59: tree_gscv
175/60: tree_gscv.best_params_
175/61: tree_gscv.scorer_
175/62: tree_gscv.best_estimator_
175/63:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
175/64: recall_score(y_test, y_hat)
175/65: accuracy_score(y_test, y_hat)
175/66: confusion_matrix(y_test, y_hat)
175/67:
#scoring = ['precision', 'recall', 'roc_auc']
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
175/68: tree_gscv
175/69: tree_gscv.best_params_
175/70: tree_gscv.scorer_
175/71: tree_gscv.best_estimator_
175/72:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
175/73: recall_score(y_test, y_hat)
175/74: accuracy_score(y_test, y_hat)
175/75: confusion_matrix(y_test, y_hat)
175/76:
min_samples_leaf_range = [1, 10, 20, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
175/77:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'] refit='roc_auc'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
175/78: tree_gscv
175/79: tree_gscv.best_params_
175/80: tree_gscv.scorer_
175/81: tree_gscv.best_estimator_
175/82:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
175/83: recall_score(y_test, y_hat)
175/84: accuracy_score(y_test, y_hat)
175/85: confusion_matrix(y_test, y_hat)
175/86:
min_samples_leaf_range = [1, 5, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
175/87:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
175/88: tree_gscv
175/89: tree_gscv.best_params_
175/90: tree_gscv.scorer_
175/91: tree_gscv.best_estimator_
175/92:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
175/93: recall_score(y_test, y_hat)
175/94: accuracy_score(y_test, y_hat)
175/95:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
175/96: tree_gscv
175/97: tree_gscv.best_params_
175/98: tree_gscv.scorer_
175/99: tree_gscv.best_estimator_
175/100:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
175/101: recall_score(y_test, y_hat)
175/102: accuracy_score(y_test, y_hat)
175/103: confusion_matrix(y_test, y_hat)
175/104:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring='roc_auc', refit='roc_auc').fit(X_train, y_train)
175/105: tree_gscv
175/106: tree_gscv.best_params_
175/107: tree_gscv.scorer_
175/108: tree_gscv.best_estimator_
175/109:
y_hat = tree_gscv.predict(X_test)
precision_score(y_test, y_hat)
175/110: recall_score(y_test, y_hat)
175/111: accuracy_score(y_test, y_hat)
175/112: confusion_matrix(y_test, y_hat)
175/113:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,0]
precision_score(y_test, y_hat)
175/114: recall_score(y_test, y_hat)
175/115: accuracy_score(y_test, y_hat)
175/116: roc_auc_score(y_test, y_hat_prob)
175/117: roc_auc_score(y_test, y_hat_prob)
175/118:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/119: recall_score(y_test, y_hat)
175/120: accuracy_score(y_test, y_hat)
175/121: roc_auc_score(y_test, y_hat_prob)
175/122:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, refit='roc_auc').fit(X_train, y_train)
175/123: tree_gscv
175/124: tree_gscv.best_params_
175/125: tree_gscv.scorer_
175/126: tree_gscv.best_estimator_
175/127:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/128: recall_score(y_test, y_hat)
175/129: accuracy_score(y_test, y_hat)
175/130: roc_auc_score(y_test, y_hat_prob)
175/131: confusion_matrix(y_test, y_hat)
175/132:
min_samples_leaf_range = [30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
175/133:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, refit='roc_auc').fit(X_train, y_train)
175/134: tree_gscv
175/135: tree_gscv.best_params_
175/136: tree_gscv.scorer_
175/137: tree_gscv.best_estimator_
175/138:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/139: recall_score(y_test, y_hat)
175/140: accuracy_score(y_test, y_hat)
175/141: roc_auc_score(y_test, y_hat_prob)
175/142: confusion_matrix(y_test, y_hat)
175/143:
min_samples_leaf_range = [30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
175/144:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, refit='roc_auc').fit(X_train, y_train)
175/145: tree_gscv
175/146: tree_gscv.best_params_
175/147: tree_gscv.scorer_
175/148: tree_gscv.best_estimator_
175/149:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/150: recall_score(y_test, y_hat)
175/151: accuracy_score(y_test, y_hat)
175/152: roc_auc_score(y_test, y_hat_prob)
175/153: confusion_matrix(y_test, y_hat)
175/154:
min_samples_leaf_range = [30]
min_samples_split_range = [2, 3, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
175/155:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, refit='roc_auc').fit(X_train, y_train)
175/156: tree_gscv
175/157: tree_gscv.best_params_
175/158: tree_gscv.scorer_
175/159: tree_gscv.best_estimator_
175/160:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/161: recall_score(y_test, y_hat)
175/162: accuracy_score(y_test, y_hat)
175/163: roc_auc_score(y_test, y_hat_prob)
175/164: confusion_matrix(y_test, y_hat)
175/165:
min_samples_leaf_range = [10, 20, 30]
min_samples_split_range = [2, 3, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
175/166:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, refit='roc_auc').fit(X_train, y_train)
175/167: tree_gscv
175/168: tree_gscv.best_params_
175/169: tree_gscv.scorer_
175/170: tree_gscv.best_estimator_
175/171:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/172: recall_score(y_test, y_hat)
175/173: accuracy_score(y_test, y_hat)
175/174: roc_auc_score(y_test, y_hat_prob)
175/175: confusion_matrix(y_test, y_hat)
175/176:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
175/177: tree_gscv
175/178: tree_gscv.best_params_
175/179: tree_gscv.scorer_
175/180: tree_gscv.best_estimator_
175/181:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/182: recall_score(y_test, y_hat)
175/183: accuracy_score(y_test, y_hat)
175/184: roc_auc_score(y_test, y_hat_prob)
175/185: confusion_matrix(y_test, y_hat)
175/186:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
175/187: tree_gscv
175/188: tree_gscv.best_params_
175/189: tree_gscv.scorer_
175/190: tree_gscv.best_estimator_
175/191:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/192: recall_score(y_test, y_hat)
175/193: accuracy_score(y_test, y_hat)
175/194: roc_auc_score(y_test, y_hat_prob)
175/195: confusion_matrix(y_test, y_hat)
175/196:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['precision', 'recall', 'roc_auc'], refit='precision').fit(X_train, y_train)
175/197: tree_gscv
175/198: tree_gscv.best_params_
175/199: tree_gscv.scorer_
175/200: tree_gscv.best_estimator_
175/201:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/202: recall_score(y_test, y_hat)
175/203: accuracy_score(y_test, y_hat)
175/204: roc_auc_score(y_test, y_hat_prob)
175/205: confusion_matrix(y_test, y_hat)
175/206:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
175/207: recall_score(y_test, y_hat_2)
175/208:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
175/209: tree_gscv
175/210: tree_gscv.best_params_
175/211: tree_gscv.scorer_
175/212: tree_gscv.best_estimator_
175/213:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/214: recall_score(y_test, y_hat)
175/215: accuracy_score(y_test, y_hat)
175/216: roc_auc_score(y_test, y_hat_prob)
175/217: confusion_matrix(y_test, y_hat)
175/218:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
175/219: recall_score(y_test, y_hat_2)
175/220:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['precision', 'recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
175/221: tree_gscv
175/222: tree_gscv.best_params_
175/223: tree_gscv.scorer_
175/224: tree_gscv.best_estimator_
175/225:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/226: recall_score(y_test, y_hat)
175/227: accuracy_score(y_test, y_hat)
175/228: roc_auc_score(y_test, y_hat_prob)
175/229: confusion_matrix(y_test, y_hat)
175/230:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
175/231: recall_score(y_test, y_hat_2)
175/232:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['precision', 'recall'], refit='precision').fit(X_train, y_train)
175/233: tree_gscv
175/234: tree_gscv.best_params_
175/235: tree_gscv.scorer_
175/236: tree_gscv.best_estimator_
175/237:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/238: recall_score(y_test, y_hat)
175/239: accuracy_score(y_test, y_hat)
175/240: roc_auc_score(y_test, y_hat_prob)
175/241: confusion_matrix(y_test, y_hat)
175/242:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
175/243: recall_score(y_test, y_hat_2)
175/244:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
175/245:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['precision', 'recall', 'roc_auc']).fit(X_train, y_train)
175/246: tree_gscv
175/247: tree_gscv.best_params_
175/248: tree_gscv.scorer_
175/249: tree_gscv.best_estimator_
175/250:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
175/251: recall_score(y_test, y_hat)
175/252: accuracy_score(y_test, y_hat)
175/253: roc_auc_score(y_test, y_hat_prob)
175/254: confusion_matrix(y_test, y_hat)
175/255: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['precision', 'recall', 'roc_auc'], refit='precision').fit(X_train, y_train)
175/256:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
175/257: recall_score(y_test, y_hat_2)
175/258: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
175/259:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
175/260: recall_score(y_test, y_hat_2)
175/261: roc_auc_score(y_test, y_hat_prob_2)
175/262: confusion_matrix(y_test, y_hat_2)
   1:
#from google.colab import drive
#drive.mount('/content/drive')
   2:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay, auc
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
   3:
# Definindo o DataFrame com os resultados das predições dos modelos de cada leva
def build_dataframe(results):
    df = pd.DataFrame(results)
    df['metric'] = (df.precision + df.recall + df.roc) / 3
    df = df.sort_values(by=['metric','precision', 'roc', 'recall'], ascending=False).reset_index(drop=True)
    return df
   4:
# Calculando o dicionário com os resultados das predições dos modelos por leva
def plot_model_stats(NCOLS, models, model_names, X_test, y_test, leva):
    fig, ax = plt.subplots(ncols=NCOLS, figsize=(20,4))
    fig.tight_layout()
    
    results = []

    for i in range(NCOLS):
        results += [plot_confusion_matrix(models[i], X_test, y_test, ax[i], model_names[i], leva)]
        ax[i].set_title(model_names[i])

    plt.show()
    return results
   5:
# Define o yhat, a matriz de confusão e os scores necessários para um modelo de uma determinada leva
def plot_confusion_matrix(model, X, y, ax, model_name, leva):
    y_pred = model.predict(X)
    y_pred_prob = model.predict_proba(X)[:, 1]
    
    cm = confusion_matrix(y, y_pred, labels=model.classes_)
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    display.plot(ax=ax)
    
    data = {
        'model_name': model_name,
#       'model_shape': model.hidden_layer_sizes,
        'leva': leva,
        'precision': round(precision_score(y, y_pred), 4),
        'recall': round(recall_score(y, y_pred), 4),
        'roc': round(roc_auc_score(y, y_pred_prob), 4)
    }
    
    return data
   6:
#%cd /content/drive/My Drive/titanic_kaggle/dataset
#train_df = pd.read_csv("train.csv")
#test_df = pd.read_csv("test.csv")
   7:
train_df = pd.read_csv("../dataset/train.csv")
test_df = pd.read_csv("../dataset/test.csv")
   8: train_df.sample(5)
   9: train_df.describe()
  10: train_df.shape
  11: #sns.countplot(dataset['Embarked'])
  12:
males = train_df[train_df.Sex == 'male']
females = train_df[train_df.Sex == 'female']
  13:
males_survived = males.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
males_survived['Sex'] = 'male'

females_survived = females.Survived.value_counts().reset_index().rename(columns={'index': 'Survived', 'Survived': 'Count'})
females_survived['Sex'] = 'female'

survived_per_sex = pd.concat([males_survived, females_survived], axis=0)
  14:
ax = sns.barplot(data=survived_per_sex, x='Sex', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
  15:
dead = train_df[train_df.Survived == 0].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
dead['Survived'] = 0

survived = train_df[train_df.Survived == 1].Pclass.value_counts().reset_index().rename(columns={'index': 'Pclass', 'Pclass': 'Count'})
survived['Survived'] = 1

survived_per_pclass = pd.concat([dead, survived], axis=0)
  16:
ax = sns.barplot(data=survived_per_pclass, x='Pclass', y='Count', hue='Survived', width=0.25, edgecolor='black')
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.show()
  17: sns.scatterplot(data=train_df, x='Age', y='Fare', hue='Survived')
  18: sns.histplot(data=train_df, x='Age', hue='Survived', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
  19: sns.histplot(data=train_df, x='Age', hue='Sex', multiple='layer', shrink=0.5, kde=True, bins=20, element='step')
  20:
fig, ax = plt.subplots(figsize=(14,6))
sns.histplot(data=train_df, x='Sex', y='Age', hue='Survived', bins=50, cbar=True, ax=ax)
  21: sns.boxplot(data=train_df, x='Sex', y='Age', width=0.25)
  22: sns.boxplot(data=train_df, x='Survived', y='Age', hue='Sex', width=0.25)
  23: sns.boxplot(data=train_df, x='Survived', y='Fare', hue='Sex', width=0.25)
  24:
percent = train_df.isnull().mean()
percent
  25:
most_null_cols = percent[percent > 0.75].index
train_df = train_df.drop(columns=most_null_cols)
train_df.sample(1)
  26:
train_df = train_df.drop(columns=['PassengerId', 'Name','Ticket']) # useless features
#test_df = test_df.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'])
  27: train_df.sample(1)
  28:
types = train_df.dtypes.reset_index()
types.columns = ['Feature', 'type']
types
  29:
# Matriz de features X e vetor TARGET y
X_ = train_df.loc[:, train_df.columns != 'Survived']
y_ = train_df.Survived
  30:
CATEGORICAL_FEATURES = X_.select_dtypes(include='object').columns.tolist()
NUMERICAL_FEATURES = X_.select_dtypes(exclude='object').columns.tolist()
  31:
numerical_transformer = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='mean')),
           ('scaler', MinMaxScaler())
    ]
)

categorical_transformer = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

column_transformer = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, NUMERICAL_FEATURES),
        ('cat', categorical_transformer, CATEGORICAL_FEATURES)
    ]
)

pipe = Pipeline(
    steps=[('transformer', column_transformer),
           ('estimator', DecisionTreeClassifier())
    ]
)
  32:
# Separando a base pré-processada em bases treino e teste
# seguindo uma proporção de 25% para a de teste
X_train, X_test, y_train, y_test = train_test_split(
X_, y_, test_size=0.25, random_state=42)
  33: pipe.fit(X_train, y_train)
  34: pipe.score(X_test, y_test)
  35:
min_samples_leaf_range = [1, 5, 10, 15, 20, 25, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6, 8, 10]}
  36:
#scoring = ['precision', 'recall', 'roc_auc']
#scoring =['recall', 'roc_auc'], refit='roc_auc'
#scoring=['recall', 'roc_auc'], refit='recall'
tree_gscv = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
  37: tree_gscv
  38: tree_gscv.best_params_
  39: tree_gscv.scorer_
  40: tree_gscv.best_estimator_
  41:
y_hat = tree_gscv.predict(X_test)
y_hat_prob = tree_gscv.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat)
  42: recall_score(y_test, y_hat)
  43: accuracy_score(y_test, y_hat)
  44: roc_auc_score(y_test, y_hat_prob)
  45: confusion_matrix(y_test, y_hat)
  46: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
  47:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
  48: recall_score(y_test, y_hat_2)
  49: roc_auc_score(y_test, y_hat_prob_2)
  50: confusion_matrix(y_test, y_hat_2)
  51:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
  52: recall_score(y_test, y_hat_2)
  53: roc_auc_score(y_test, y_hat_prob_2)
  54: confusion_matrix(y_test, y_hat_2)
  55: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring =['precision', 'roc_auc'], refit='precision').fit(X_train, y_train)
  56:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
  57: recall_score(y_test, y_hat_2)
  58: roc_auc_score(y_test, y_hat_prob_2)
  59: confusion_matrix(y_test, y_hat_2)
  60: tree_gscv_2
  61: tree_gscv_2.best_estimator_
  62: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
  63: tree_gscv_2.best_estimator_
  64:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
  65: recall_score(y_test, y_hat_2)
  66: roc_auc_score(y_test, y_hat_prob_2)
  67: confusion_matrix(y_test, y_hat_2)
  68: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='precision').fit(X_train, y_train)
  69: tree_gscv_2.best_estimator_
  70:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
  71: recall_score(y_test, y_hat_2)
  72: roc_auc_score(y_test, y_hat_prob_2)
  73: confusion_matrix(y_test, y_hat_2)
  74:
min_samples_leaf_range = [1, 10, 20, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
  75: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
  76: tree_gscv_2.best_estimator_
  77:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
  78: recall_score(y_test, y_hat_2)
  79: roc_auc_score(y_test, y_hat_prob_2)
  80: confusion_matrix(y_test, y_hat_2)
  81: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='recall?').fit(X_train, y_train)
  82: roc_auc_score(y_test, y_hat_prob_2)
  83: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='recall').fit(X_train, y_train)
  84: tree_gscv_2.best_estimator_
  85:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
  86: recall_score(y_test, y_hat_2)
  87: roc_auc_score(y_test, y_hat_prob_2)
  88: confusion_matrix(y_test, y_hat_2)
  89: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
  90: tree_gscv_2.best_estimator_
  91:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
  92: recall_score(y_test, y_hat_2)
  93: roc_auc_score(y_test, y_hat_prob_2)
  94: confusion_matrix(y_test, y_hat_2)
  95:
min_samples_leaf_range = [10, 20, 30]
min_samples_split_range = [2, 3, 4, 5, 6]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
  96: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
  97: tree_gscv_2.best_estimator_
  98:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
  99: recall_score(y_test, y_hat_2)
 100: roc_auc_score(y_test, y_hat_prob_2)
 101: confusion_matrix(y_test, y_hat_2)
 102: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='precision').fit(X_train, y_train)
 103: tree_gscv_2.best_estimator_
 104:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
 105: recall_score(y_test, y_hat_2)
 106: roc_auc_score(y_test, y_hat_prob_2)
 107: confusion_matrix(y_test, y_hat_2)
 108:
min_samples_leaf_range = [10, 20, 30]
min_samples_split_range = [2, 3, 4, 5]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
 109: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='precision').fit(X_train, y_train)
 110: tree_gscv_2.best_estimator_
 111:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
 112: recall_score(y_test, y_hat_2)
 113: roc_auc_score(y_test, y_hat_prob_2)
 114: confusion_matrix(y_test, y_hat_2)
 115: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
 116: tree_gscv_2.best_estimator_
 117:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
 118: recall_score(y_test, y_hat_2)
 119: roc_auc_score(y_test, y_hat_prob_2)
 120: confusion_matrix(y_test, y_hat_2)
 121:
min_samples_leaf_range = [10, 20, 30]
min_samples_split_range = [2, 3, 5]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
 122: tree_gscv_2 = GridSearchCV(estimator=pipe, param_grid=params, verbose=True, scoring=['precision', 'recall', 'roc_auc'], refit='roc_auc').fit(X_train, y_train)
 123: tree_gscv_2.best_estimator_
 124:
y_hat_2 = tree_gscv_2.predict(X_test)
y_hat_prob_2 = tree_gscv_2.predict_proba(X_test)[:,1]
precision_score(y_test, y_hat_2)
 125: recall_score(y_test, y_hat_2)
 126: roc_auc_score(y_test, y_hat_prob_2)
 127: confusion_matrix(y_test, y_hat_2)
 128:
min_samples_leaf_range = [10, 20, 30]
min_samples_split_range = [2, 3, 5]
params = {'estimator__criterion': ['gini', 'entropy', 'log_loss'],
          'estimator__splitter': ['best', 'random'],
          'estimator__min_samples_leaf': min_samples_leaf_range,
          'estimator__min_samples_split': min_samples_split_range,
          'estimator__max_depth': [None, 3, 4, 5, 6],
          'estimator__max_leaf_nodes': [4, 6]}
 129: %history
 130: %history -g
 131: %hist -o -g -f ipython_history.md
